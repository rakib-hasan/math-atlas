@ROUT atlas_tprim.h
#ifndef ATLAS_TPRIM_H
   #define ATLAS_TPRIM_H
/*
 * This file defines just a few primitive threading routines necessary
 * for us to perform the parallel runs necessary to test/time ATLAS.
 * Therefore, all of the files prototyped here can be built prior to any 
 * ATLAS tuning, other than the creation of the two files in BLDdir/include:
 * 1. atlas_pthreads.h  : defines ATL_NTHREAD/NTHRPOW2
 * 2. atlas_taffinity.h : defines how to support affinity
 */
#define ATL_MEMBAR_ONLY 1
#include "atlas_cbc.h"
#include "atlas_pthreads.h"
#include "atlas_taffinity.h"
#include "atlas_aux.h"

#if defined(ATL_OS_Win64) || defined(ATL_OS_WinNT)
   #ifdef ATL_USE64BITS
      #define ATL_WIN64THREADS 1
      #define ATL_WINTHREADS 1
   #else
      #define ATL_WIN32THREADS 1
      #define ATL_WINTHREADS 1
   #endif
#endif
#include "atlas_pthreads.h" /* gened file defs ATL_NTHREADS & ATL_NTHRPOW2 */
#ifdef ATL_WINTHREADS
   #include <windows.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      HANDLE thrH;   /* handle to thread */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* affinity id for this core */
   } ATL_thread_t;
   #ifndef CREATE_SUSPENDED
      #define CREATE_SUSPENDED 0x00000004
   #endif
   #ifndef WAIT_FAILED
      #define WAIT_FAILED (~0)
   #endif
#elif defined(ATL_OMP_THREADS)
   #include <omp.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* < 0: not set, affID=1-affID, else affID */
   } ATL_thread_t;
  #ifndef ATL_lock_t
     #define ATL_lock_t omp_lock_t
  #endif
#else
   #define ATL_USE_POSIXTHREADS 1
   #include <pthread.h>
   typedef struct
   {
      pthread_t thrH;/* handle of thread */
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* < 0: not set, affID=1-affID, else affID */
   } ATL_thread_t;
  #ifndef ATL_lock_t
     #define ATL_lock_t pthread_mutex_t
  #endif
#endif

int ATL_thread_start
   (ATL_thread_t *thr, int proc, int JOINABLE, void *(*rout)(void*), void*);
int ATL_thread_join(ATL_thread_t*); /* waits on completion of thread */

int ATL_lock(void *lck);
int ATL_trylock(void *lck);
/*
 * On weakly-ordered systems, unlock must flush store buffs before or at
 * same time as lock is written, and must prevent local reads from moving
 * above the unlock
 */
int ATL_unlock(void *lck);
void ATL_lock_init(void *lck);
void ATL_lock_destroy(void *lck);
#if 0
void *ATL_newLocks(unsigned int nlocks);
void *ATL_freeLocks(unsigned int nlocks, void *vp);
#endif

#define ATL_SAFELS 128L
#define ATL_SAFELS_SH 7
#define ATL_AddBytesPtr(vp_, by_) (void*) ((size_t)(vp_) + (size_t)by_)
#define ATL_SubBytesPtr(vp_, by_) (void*) ((size_t)(vp_) - (size_t)by_)
#define ATL_AlignSafeLS(vp_) (void*) \
   ((((size_t)(vp_))+ATL_SAFELS-1L)&(~((size_t)ATL_SAFELS-1L)))
#define ATL_IncBySafeLS(vp_) ATL_AddBytesPtr(vp_, ATL_SAFELS)
#define ATL_DecBySafeLS(vp_) ATL_AddBytesPtr(vp_, -ATL_SAFELS)
/*
 * Helper function that starts P threads, waits for them to complete,
 * and returns.  Used in initial tuning before we've set up full thread
 * framework.
 */
#ifdef ATL_DEF_RUNTHR
#include <assert.h>
static void ATL_runThreads(int P, void *(*rout)(void*), void *arg)
{
   int i;
   ATL_thread_t tp[ATL_NTHREADS];
#ifdef ATL_OMP_THREADS
   omp_set_num_threads(P);
   #pragma omp parallel
   {
      assert(omp_get_num_threads() == P);
      i = omp_get_thread_num();
      #ifdef ATL_PAFF_SELF
         ATL_setmyaffinity(i);
      #endif
      tp[i].vp = arg;
      tp[i].rank = i;
      tp[i].P = P;
      rout(tp+i);
   }
#else
   for (i=0; i < P; i++)
   {
      tp[i].vp = arg;
      assert(!ATL_thread_start(tp+i, i, 1, rout, tp+i));
   }
   for (i=0; i < P; i++)
      assert(!ATL_thread_join(tp+i));
#endif
}
#endif

#endif
@ROUT atlas_ttypes.h
/*
 * This file defines the types for the new threaded routines, without using
 * any specific thread info so that they can be safely included prior to
 * building & tuning the threading system
 */
#ifdef TYPE
#ifndef ATLAS_TTYPES_H
   #define ATLAS_TTYPES_H
#include "atlas_amm.h"
#include "atlas_bitvec.h"
typedef volatile int VINT;
/*
 * This structure is for a TRSM that uses ATL_trsm as its compute kernel,
 * and only divides N (RHS) matrix.  Works best for tiny TRiangular matrix.
 */
typedef struct ATL_ttrsm_tTR_t ATL_ttrsm_tTR_t;
struct ATL_ttrsm_tTR_t
{
   void *rhsBlkCtr;        /* deals out RHS, [1-nrblks] */
   size_t lda, ldb;        /* leading dims of A & B */
   const TYPE *A;          /* triangular matrix */
   TYPE *B;                /* MxN input/output matrix of right-hand sides */
   #ifdef TREAL
      TYPE alpha;          /* scalar for B */
   #else
      const TYPE *alpha;   /* scalar for B */
   #endif
   enum ATLAS_SIDE side;   /* Whether B on Left or Right of A */
   enum ATLAS_TRANS TA;    /* Whether A is transposed or not */
   enum ATLAS_DIAG uplo;   /* Whether triangle stored Upper of Lower */
   enum ATLAS_DIAG diag;   /* Unit or non-unit diagonal */
   ATL_INT M, N;           /* rows (cols) of array B */
   ATL_INT minrhs;         /* # of rhs all jobs get */
   int rb;                 /* block size for RHS division */
   int neblks;             /* # of extra blocks dealt out */
   int rr;                 /* size of last extra-block, or 0 */
};

typedef struct ATL_ttrsm_amm ATL_ttrsm_amm_t;
struct ATL_ttrsm_amm
{
   ammkern_t amm_b0, amm_b1;
   ablk2cmat_t blk2c;
   cm2am_t a2blk, b2blk;
   void *AblkCtr;      /* deals out copy of A blocks column-wise */
   void *rhsCtr;       /* deals out column panels of Right Hand Side (B/X) */
   ATL_BV_t *AcpyBV;   /* nablks-len BV; 0: A blk not yet copied */
   void *Acpymut;      /* mutex protecting AcpyBV */
   TYPE *wA;           /* ptr to start of shared A workspace */
   TYPE *w;            /* nthr separate blkszB+panszB workspace */
   const TYPE *A;      /* M-order triangular matrix */
   TYPE *X;            /* MxN matrix of right hand sides (RHS) */
   size_t lda, ldx;
   size_t blkszB;      /* mb*nb, use size_t to prevent overflow with mul */
   size_t blkszA;      /* mb*mb */
   size_t panszC;      /* size of col panel of C : (nmblks-1)*blkszB */
   size_t wsL;         /* # of elts of each core's workspace */
   TYPE alpha;         /* scale to apply to X before access */
   ATL_INT M, N;       /* M: size of A, N: NRHS */
   VINT AcpyDone;      /* set to 1 when copy of entire A complete */
   int nmblks;         /* number of row panels B is split into */
   int nnblks;         /* number of AMM blocks in a row panel */
   int nablks;         /* # of non-diagonal blocks in A */
   int nxblks;         /* # of blocks in X, C has nnblks less */
   int nbf;            /* N%nb, if 0, nb */
   int nnuf;           /* (nbf+nu-1)/nu */
   int mb0;            /* size of 1st diag blk: M%mb, if 0, mb */
   int MB0;            /* for k-vector kernels, (mb0+ku-1)/ku*ku, else mb0 */
   int mb;             /* row & col blocking of A (mb & kb of amm) */
   int nb;             /* column blocking of X/B/C (nb of amm) */
   int mu;             /* M unrolling for amm kernel */
   int nu;             /* N unrolling for amm kernel */
   int ku;             /* K unrolling for amm kernel */
   int nnu, nmu, nmu0;  /* I think can kill nmu0 -> test then KILL */
   enum ATLAS_TRANS TA; 
   enum ATLAS_DIAG uplo;
   enum ATLAS_DIAG diag;
};
/*
 * Function prototypes 
 */
int Mjoin(PATL,tGetTrsmInfo)(ATL_ttrsm_amm_t *pd, int P, enum ATLAS_TRANS TA, 
                             ATL_CSZT M, ATL_CSZT N, const SCALAR beta);
#endif
#endif
@rout atlas_threads.h
#ifndef ATLAS_THREADS_H
   #define ATLAS_THREADS_H
#include "atlas_tprim.h"
#include "atlas_ttypes.h"
#include "atlas_bitvec.h"
#include "atlas_atmctr.h"
#define ATL_chkgap 128
#define ATL_chksh  7
/*
 * Unless told otherwise, use the less-intrusive conditional variable-based
 * thread pool, unless we're on the PHI, where that scale and slow OS make
 * that prohibitive.
 * NOTE: right now, always use sleep, as there may be a race condition in
 *       polling thread pool.
 */
#if !defined(ATL_POLLTPOOL) && !defined(ATL_SLEEPTPOOL)
@beginskip
   #ifdef ATL_ARCH_XeonPHI
      #define ATL_POLLTPOOL 1
   #else
      #define ATL_SLEEPTPOOL 1
   #endif
@endskip
   #define ATL_SLEEPTPOOL 1
#endif
/*
 * Need to fix this later by using a probe.  ATL_PHI_SORTED being defined
 * asserts that the first P/4 cores in the main list use context 0, the
 * next context 1, and so on.  This is the default sorting by ATLAS, and
 * we'll presently assume it true as long as the NTHR%4 == 0
 *
 * NOTE: PHI_SORTED probably presently broken.
 */
#if !defined(ATL_PHI_SORTED) && !defined(ATL_PHI_UNSORTED)
   #if defined(ATL_ARCH_XeonPHI) && ATL_NTHREADS%4 == 0
      #define ATL_PHI_SORTED 1
   #else
      #define ATL_PHI_UNSORTED 1
   #endif
#endif
#ifndef ATL_PTMAXMALLOC
   #ifndef ATL_MaxMalloc
      #include "atlas_maxmalloc.h"
   #endif
   #define ATL_PTMAXMALLOC ATL_MaxMalloc
#endif
/*
 * If we don't have thread affinity, then the thread I'm waiting on may share
 * the core with me.  In this case, yield my time slice when polling
 */
#include "atlas_tsumm.h"
#include <limits.h>
#if defined(ATL_TAFFINITY) && ATL_TAFFINITY
   #define ATL_POLL
#else
   #define ATL_POLL ATL_thread_yield()
#endif

typedef struct ATL_LaunchStruct ATL_LAUNCHSTRUCT_t;
struct ATL_LaunchStruct
{
   ATL_thread_t *rank2thr;              /* index by rank to get thread handle */
   void *opstruct;
   int (*OpStructIsInit)(void*);        /* Query to see if struct is valid */
   void (*DoWork)(ATL_LAUNCHSTRUCT_t*, void*);
   void (*DoComb)(void*, const int, const int);  /* combine func */
   void *vp;                            /* misc. extra info ptr */
   volatile int *chkin;                 /* nthr-len checkin array */
   void **acounts;                      /* var-len array of atomic counters */
};

/*
 * Constants for use in chkin array
 */
#define ATL_CHK_DONE_OP   -2048
#define ATL_CHK_DONE_COMB -2049

/*
 * The following info is all for when a thread pool is being used, rather
 * than launch & join paradigm.
 */
@iexp ib 0 1 +
/*
 * Thread pool flag macros
 */
@whiledef nm MICSORTED DIE DYNCOMB ZEROWAKES POLL
#define ATL_TPF_@(nm)(p_) ((p_)->pflag & @(ib))
#define ATL_TPF_SET_@(nm)(p_) ((p_)->pflag |= @(ib))
#define ATL_TPF_UNSET_@(nm)(p_) ((p_)->pflag &= ~@(ib))
   @iexp ib @(ib) 2 *
@endwhile

#define VUINT volatile unsigned int
/*
 * Presently, always-polling only threadpool option designed to work on Windows
 */
#if defined(ATL_WINTHREADS)
   #define ATL_TP_FULL_POLL
#elif defined(ATL_SLEEPTPOOL) && ATL_SLEEPTPOOL
   #ifdef ATL_TP_FULLPOLL
      #undef ATL_TP_FULLPOLL
   #endif
#elif defined(ATL_TP_FULLPOLL) && !ATL_TP_FULLPOLL
   #undef ATL_TP_FULLPOLL
#elif defined(ATL_HAS_AFFINITY) && ATL_HAS_AFFINITY && !defined(ATL_OMP_THREADS)
   #define ATL_TP_FULLPOLL 1
#endif
/*
 * Function pointer taking the pool as an argument that does the work.
 */
typedef void (*ATL_tpjfunc_t)(void *pp, int rank, int vrank);
/*
 * A thread pool takes can combine results at end of run if needed.
 * It takes everything the jobfunc does, + the vranks to combine
 */
typedef void (*ATL_tpjcomb_t)
   (void *pp, int rank, int vrank, int hisvrank);
/*
 * Definition of an ATLAS thread pool
 */
typedef struct ATL_ThreadPool ATL_tpool_t;
#ifdef ATL_TP_FULLPOLL
struct ATL_ThreadPool
{
   VUINT nthr;            /* # of threads total in this thread pool */
   VUINT pflag;           /* bitvector of options */
   VUINT nworkers;        /* # of thr supposed to wake up and work */
   #ifdef ATL_PHI_SORTED
      VUINT ncores, cntxts, nwcores;
   #endif
   volatile char *bchkin0;/* original ptr to free bchkin */
   volatile char *bchkin; /* nthr*ATL_chkgap-len boolean checkin array */
   volatile short *chkin; /* nthr-len chkin array wt meaning:
      /* 0: keep polling, 1: start, 2: have started, 3: finished 4: exited */
      /* 0&1 written by master, 2-4 written by worker */
   volatile int *icomm;   /* nthr-long integer communication array */
   ATL_thread_t *threads; /* array of thread ptrs */
/*
 * Variables below set only when doing a combine, and protected by tpmut
 * (which need only be set when doing a combine)
 */
   void *combmut;         /* mutex for doing optional combine op */
   ATL_BV_t *combReadyBV; /* 1: thr of that rank is ready to do combine */
   ATL_BV_t *combDoneBV;  /* 1: thr of that rank's data already combined */
   void *tpmut;           /* mutex protecting above pool info */
   void *cbcmut;          /* mutex used for cbc ops on weakly-ord caches */
/*
 * variables below here manipulated only when threads known to be polling
 */
   ATL_tpjfunc_t jobf;    /* ptr to job function to execute */
   ATL_tpjcomb_t combf;   /* NULL: don't combine, else combine func */
   void *PD;              /* problem def to give to jobf & combf */
   void *extra;           /* extra info for jobf & combf */
};
#elif defined(ATL_OMP_THREADS)
struct ATL_ThreadPool
{
   VUINT nthr;            /* # of threads total in this thread pool */
   VUINT pflag;           /* bitvector of options */
   VUINT nworkers;        /* # of thr supposed to wake up and work */
   #ifdef ATL_PHI_SORTED
      VUINT ncores, cntxts, nwcores;
   #endif
   volatile char *bchkin0;/* original ptr to free bchkin */
   volatile char *bchkin; /* nthr*128-len boolean checkin array */
   volatile int *icomm;   /* nthr-long integer communication array */
   ATL_thread_t *threads; /* array of thread ptrs */
/*
 * Variables below set only when doing a combine, and protected by tpmut
 * (which need only be set when doing a combine)
 */
   void *combmut;         /* mutex for doing optional combine op */
   ATL_BV_t *combReadyBV; /* 1: thr of that rank is ready to do combine */
   ATL_BV_t *combDoneBV;  /* 1: thr of that rank's data already combined */
   void *tpmut;           /* mutex protecting above pool info */
   void *cbcmut;          /* mutex used for cbc ops on weakly-ord caches */
/*
 * variables below here manipulated only when threads known to be polling
 */
   ATL_tpjfunc_t jobf;    /* ptr to job function to execute */
   ATL_tpjcomb_t combf;   /* NULL: don't combine, else combine func */
   void *PD;              /* problem def to give to jobf & combf */
   void *extra;           /* extra info for jobf & combf */
};
#else
struct ATL_ThreadPool
{
   VUINT jobID;           /* count of jobs, wraps at top of uint range */
   VUINT WORKDONE;        /* zeroed to start job, set by last worker done */
   VUINT NOWORK;          /* optionally set when all work dealt out */
   VUINT nthr;            /* # of threads total in this thread pool */
   VUINT nsleep;          /* # of threads that have gone to sleep at start */
   VUINT nworkers;        /* # of thr supposed to wake up and work */
   VUINT wcnt;            /* count incremented as workers wake up */
   VUINT nwdone;          /* # of workers that have completed the task */
   VUINT pflag;           /* bitvector of options */
   void *wcond;           /* cond var for work pool sleep/wake */
   #ifdef ATL_PHI_SORTED
      void *wcond2;       /* cond vars 2nd context sleeps on */
      void *wcond3;       /* cond vars 3rd context sleeps on */
      void *wcond4;       /* cond vars 4th context sleeps on */
   #elif defined(ATL_PHI_SORTED2)
      int ncntxts;
      void **wconds;
   #endif
   void *mcond;           /* cond for master process sleep/wake */
   ATL_thread_t *threads; /* array of thread ptrs */
   volatile char *bchkin0;/* original ptr to free bchkin */
   volatile char *bchkin; /* nthr*128-len boolean checkin array */
   volatile int *icomm;   /* nthr-long integer communication array */
   void *combmut;         /* mutex for doing optional combine op */
   ATL_BV_t *combReadyBV; /* 1: thr of that rank is ready to do combine */
   ATL_BV_t *combDoneBV;  /* 1: thr of that rank's data already combined */
   void *tpmut;           /* mutex protecting above pool info */
   void *cbcmut;          /* mutex used for cbc ops on weakly-ord caches */
/* 
 * variables below here manipulated only when threads known asleep 
 */
   ATL_tpjfunc_t jobf;    /* ptr to job function to execute */
   ATL_tpjcomb_t combf;   /* NULL: don't combine, else combine func */
   void *PD;              /* problem def to give to jobf & combf */
   void *extra;           /* extra info for jobf & combf */
};
#endif

/*
 * Declare the beautiful global variables used by thread pool
 */
#ifdef ATL_TP_DECL
   double ATL_POLLTIME=0.01;  /* poll for 10 millisecond */
   ATL_tpool_t *ATL_TP_PTR=NULL, *ATL_TP1_PTR=NULL;
#else
   extern double ATL_POLLTIME;
   extern ATL_tpool_t *ATL_TP_PTR, *ATL_TP1_PTR;
#endif
void *ATL_threadpool(void *tp);
void *ATL_threadpool_launch(void *tp);
void ATL_InitThreadPoolStartup(int P, void *pd, void *extra);
void ATL_goParallel (const unsigned int P, void *DoWork, void *DoComb,
                     void *PD, void *extra);
ATL_tpool_t *ATL_NewThreadPool(const int P, int ICOM, void *vp);
void ATL_FreeThreadPool(ATL_tpool_t *pp);
void ATL_oldjobwrap(void *vpp, int rank, int vrank);
void ATL_oldcombwrap(void *vpp, int rank, int vrank, int vhisrank);
int ATL_tpool_dojob(ATL_tpool_t *pp, const int rank, const int CallFrWrk);
void *ATL_threadpool(void *vp);
#undef VUINT
int ATL_setmyaffinity(const int); /* sets affinity of already-running thread */
/* Sets up ATL_LAUNCHSTRUCT_t var and ATL_thread_t array & starts threads*/
void ATL_thread_launch(void *opstruct, int opstructstride, void *OpStructIsInit,
                       void *DoWork, void *CombineOpStructs);
void ATL_goparallel(const unsigned int P, void *DoWork, void *opstruct, void*);
void ATL_goparallel_prank(const unsigned int P, void *DoWork, void *opstruct,
                          void *DoComb);
@beginskip
/*  Starts a thread running, and sets its affinity to proc if possible */
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE, 
                     void *(*rout)(void*), void*);
int ATL_thread_join(ATL_thread_t*); /* waits on completion of thread */
@endskip
void ATL_thread_exit(void*);        /* exits currently executing thread */
void *ATL_log2tlaunch(void *vp);    /* min spanning tree launch */
void *ATL_lin0tlaunch(void *vp);    /* 0 linear launches all threads */
void *ATL_dyntlaunch(void *vp);     /* launch done as workpool */
/*
 * Atomic count functions; may be less overhead than mutex on some systems
 */
@beginskip
void *ATL_SetAtomicCount(int cnt);   /* allocates acnt, sets acnt=cnt */
int   ATL_ResetAtomicCount(void *vp, int cnt);  /* reset vp to cnt */
int   ATL_DecAtomicCount(void *vp);  /* returns acnt-- (not --acnt!) */
int   ATL_GetAtomicCount(void *vp);  /* returns acnt */
void  ATL_FreeAtomicCount(void *vp); /* free acount resources */
@endskip
#define ATL_SetAtomicCount(cnt_) ATL_atmctr_new(cnt_)
#define ATL_ResetAtomicCount(vp_, cnt_) ATL_atmctr_set(vp_, cnt_)
#define ATL_DecAtomicCount(vp_) ATL_atmctr_dec(vp_)
#define ATL_GetAtomicCount(vp_) ATL_atmctr_get(vp_)
#define ATL_FreeAtomicCount(vp_) ATL_atmctr_free(vp_)
/*
 * Global count functions, built out of P local counters for good scaling
 */
void *ATL_SetGlobalAtomicCount(int P, int cnt, int percLoc); 
void  ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc);
int   ATL_DecGlobalAtomicCount(void *vp, int rank); 
int   ATL_GetGlobalAtomicCount(void *vp, int rank);
void  ATL_FreeGlobalAtomicCount(void *vp);
/*
 * Countdown funcs: same as above Glob, but guarantee 1 is last non-zero # ret
 */
void *ATL_SetGlobalAtomicCountDown(int P, int cnt);
int ATL_DecGlobalAtomicCountDown(void *vp, int rank);
void ATL_FreeGlobalAtomicCountDown(void *vp);

/*
 * If using pthreads, just wrapper around pthread mutex funcs, else written
 * in terms of the AtomicCount funcs with init value set to 1
 */
void *ATL_mutex_init(void);       /* returns mutex pointer */
void ATL_mutex_free(void *vp);    /* frees mutex vp */
void ATL_mutex_lock(void *vp);
void ATL_mutex_unlock(void *vp);
int  ATL_mutex_trylock(void *vp); /* opp pthreads: 0 means lock NOT acquired */
void ATL_thread_yield(void);      /* gives up time slice */
/*
 * Condition variables only used for thread pool, not implemented yet on
 * anything but pthreads (Windows & OpenMP missing)
 */
void *ATL_cond_init(void);
void ATL_cond_free(void *vp);
void ATL_cond_signal(void *cond);
void ATL_cond_bcast(void *cond);
void ATL_cond_wait(void *cond, void *mut);

#define MindxT(A_,i_) ((void*)( ((char*)(A_)) + ((size_t)i_) ))
#define ATL_tlaunch ATL_log2tlaunch   /* may want linear launch later */
void ATL_tDistMemTouch(size_t N, void *vp);
unsigned int ATL_GetSchedDelayIdx(float dly);
@whiledef sy mut lac pub mix prv
float ATL_GetSchedTime_@(sy)(unsigned int idx);
@endwhile
int ATL_IsFirstThreadedCall(void);

#endif   /* end of #ifdef protecting include file from redundant inclusion */

@ROUT atlas_tlevel3.h
#ifndef ATLAS_TLEVEL3_H
   #define  ATLAS_TLEVEL3_H
   #define DMM_H 1
   #define SMM_H 1
   #define CMM_H 1
   #define ZMM_H 1
/*
 * ========================================
 * Threaded routines in all four precisions
 * ========================================
 */
@multidef styp double@^*  float@^* double@^ float@^
@multidef typ double float double float
@whiledef pre z c d s
int ATL_@(pre)threadMM(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                  size_t M, size_t N, size_t K);
@whiledef rt gemm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
@endwhile
void ATL_@(pre)tsymm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @whiledef rt trmm trsm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag, 
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    @(typ) *B, ATL_CINT ldb);
   @endwhile
void ATL_@(pre)tsyr2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)tsyrk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
@endwhile

/*
 * =======================================================
 * Threaded routines appearing only for complex precisions
 * =======================================================
 */
@multidef sty2 double@^ float@^
@multidef styp double@^*  float@^*
@multidef typ double float
@whiledef pre z c
void ATL_@(pre)themm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)ther2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)therk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(sty2)alpha, const @(typ) *A, ATL_CINT lda,
    const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
   @undef sty2
@endwhile
#endif
@ROUT atlas_tlvl3.h
#ifndef atlas_tlvl3_H
   #define atlas_tlvl3_H
   #define DMM_H 1
   #define SMM_H 1
   #define CMM_H 1
   #define ZMM_H 1

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl3.h"
   #include "atlas_amm.h"
   #include "atlas_cbc.h"
#endif
#ifndef ATL_XOVER_L3
   #ifdef TREAL
      #define ATL_XOVER_L3 2   /* number of NBxNB blocks */
   #else
      #define ATL_XOVER_L3 1
   #endif
#endif

#ifndef ATL_TGEMM_XOVER
   #define ATL_TGEMM_XOVER ATL_XOVER_L3
#endif
#ifndef ATL_TGEMM_ADDP
   #define ATL_TGEMM_ADDP 1
#endif
/*
 * Number of blocks per proc for GEMM to divide M only
 */
#ifndef ATL_TMMMINMBLKS
   #define ATL_TMMMINMBLKS 4
#endif
#ifndef ATL_TGEMM_THRESH_MF
   #define ATL_TGEMM_THRESH_MF \
  ((((2.0*(ATL_TGEMM_XOVER))*ATL_geAMM_LASTLCMU)*ATL_geAMM_LASTLCMU)*ATL_geAMM_LASTLCMU)
#endif
/*
 * This is the minimal number of flops each thread requires once THRESH
 * is exceeded
 */
#ifndef ATL_TGEMM_PERTHR_MF
   #define ATL_TGEMM_PERTHR_MF \
      ((((2.0*ATL_TGEMM_ADDP)*ATL_geAMM_LASTLCMU)*ATL_geAMM_LASTLCMU)*ATL_geAMM_LASTLCMU)
#endif
/*
 * For debugging, can define ATL_SERIAL_COMBINE, and then it any required
 * workspaces of C will be allocated before beginning parallel operations,
 * and all required combined will happen after parallel operations are
 * done.
 */
// #define ATL_SERIAL_COMBINE
#ifdef ATL_SERIAL_COMBINE
typedef struct ATL_CombNode ATL_combnode_t;
struct ATL_CombNode
{
   ATL_INT M, N, ldw, ldd;
   void *W, *D;                 /* Work and Destination */
   ATL_combnode_t *next;
};
#endif
/*
 * The array Cinfp holds C partitioning information.  This array holds a
 * list of pointers to nodes whose data I have not been able to combine
 * with my native C partition.  The first nCw entries contain the pointers
 * to the MMNODE of allocated C workspaces that I have not been able to
 * combine.  If my node has C in workspace, I am the first entry in this array.
 * Sometimes, a child thread has been combined with me that owned a piece of
 * the original C.  These values do not need to be combined (they were written
 * to the original C), but we need to combine the range of "owned" workspaces
 * so that we know when it is legal for a parent node to add into the space.
 * The final nCp entries of Cinfp entries of Cinfp hold these original pieces
 * that need to be combined to create larger owned partitions (starting from 
 * the end of the array).  If the C ptr is NULL, that means that entry has
 * been subsumed into a new entry.
 */
typedef struct ATL_TMMNode ATL_TMMNODE_t;
struct ATL_TMMNode
{
   ATL_TMMNODE_t *Cinfp[ATL_NTHREADS];
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   const void *A, *B;
   void *C, *Cw;
   void *alpha, *beta;
   void *zero, *one;
   ATL_INT ldcw, M, N, K, lda, ldb, ldc;
   int mb, nb, kb;
   int eltsz, eltsh; /* element size, and shift (eg. log_2(eltsz)) */
   int rank;         /* the rank of my thread ([0,P-1]) */
   int nCw;          /* # of workspace entries in 1st nCw elts of Cinfp array */
   int nCp;          /* # of orig. C pieces last nCp elts of Cinfp */
   int ownC;         /* do I own my piece of C, or only wrkspace? */
};
/*
 * This data structure used for dynamically scheduled rank-K update
 * It is needed only by routines that are typed, and thus define TYPE
 */
#ifdef TYPE
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void **aMcnts;         /* P-len array of counts on row-blks of C */
   void **Mlocks;         /* mutexes protecting init of aMcnts */
   int *Js;               /* current C col for each node */
   int Sync0;             /* 0: no sync at end; else thr 0 waits til all done */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   #ifdef TREAL
      TYPE alpha;          
      TYPE beta;
   #else
      const TYPE *alpha;          
      const TYPE *beta;
   #endif
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_t;
#endif

/*
 * This data structure is used when we split K for SYRK
 */
typedef struct ATL_SyrkK ATL_TSYRK_K_t;
struct ATL_SyrkK
{
   ATL_TSYRK_K_t *Cinfp[ATL_NTHREADS];
@beginskip
   void (*gemmT)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
@endskip
   void (*gemmT)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A;
   void *C, *Cw;
   void *DoComb;
   ATL_LAUNCHSTRUCT_t *lp;
   const void *alpha, *beta;
   const void *zero, *one;
   ATL_INT ldcw, N, K, nb, lda, ldc;
   int eltsh, rank, nCw;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans, TB;
};
#ifdef TYPE
   #define ulong unsigned long
   #define VINT volatile int
   #define VSHORT volatile short
   #define VCHAR volatile char
   #define ushort unsigned short
   #define uint unsigned int
/*
 * This structure used when K <= MAXKB, and M and N are large
 */
typedef struct ATL_tamm_rkK ATL_tamm_rkK_t;
struct ATL_tamm_rkK
{
   ammkern_t amm_b0;
   cm2am_t a2blk;       /* block copy for A */
   cm2am_t b2blk;       /* block copy for B, applies alpha */
   ablk2cmat_t blk2c;   /* copy that applies beta  */
   const TYPE *A;       /* input A matrix */
   const TYPE *B;       /* input B matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* nthr wsz-len thread-local workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *B1cpyAsgCtr;   /* 1: 1st B blk not assgnd, caller copies,0: cpy done */
   void *B1cpyDonCtr;   /* 1: 1st B blk not copied yet, 0: cpy done */
   void *AcpyCtr;       /* when 0, all of A has been copied */
   void **MbCtr;        /* nnblk-len array of Mblk ctrs */
   void *BAssgBV;       /* 1 means being copied, 0: not assigned */
   void *BDoneBV;       /* 1 means already copied, 0: not yet copied */
   void *cpBmut;        /* protects B[assg,done]BV */
   size_t wsz;          /* size of local workspace */
   int BCPYDONE;        /* if 1, all of B has been copied */
   int ACPYDONE;        /* if 1, all of A has been copied */
   int bsz;             /* size of common workspace for B */
   int TA;              /* 0: noTrans; 1: Trans */
   int TB;              /* 0: noTrans; 1: Trans */
   int N;               /* # cols of C; N <= MAXNB */
   int K;               /* common dim A&B; K <= MAXKB */
   int nmblks;          /* # of M blocks, including any partial block */
   int mr;              /* M%mu */
   int nbm;             /* # of M blocks 1 MbCtr gives out (1st col always 1) */
   int nmu;             /* CEIL(mb/mu) */
   int nmuL;            /* # of mus in final block */
   int mb;              /* block size for all blocks but last */
   int mbL;             /* block size for last block */
   int nnu;             /* CEIL(N/nu) */
   int KB0;             /* if kmajor, it is CEIL(K/ku)*ku, else K */
   int lda;             /* leading dim of A */
   int ldb;             /* leading dim of B */
   int ldc;             /* leading dim of C */
};
/*
 * This structure used when M <= MAXMB && N <= MAXNB (K large)
 */
typedef struct ATL_tamm_tMN ATL_tamm_tMN_t;
struct ATL_tamm_tMN
{
   ipinfo_t *ip;          /* inner product structure */
   const TYPE *A, *B;
   TYPE *C;
   TYPE *w;
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   void *PartKctr;        /* K partition counter */
   void *blkUctr;         /* unpartitioned K block counter */
   void *Cmut;            /* mutex protecting output matrix */
   size_t nblksKp;        /* # of blocks in a K partition */
   size_t nKp;            /* # of K partitions to be dealt out */
   size_t nUblks;         /* nUblks = # of blocks not assigned to Kps */
                          /* nKp*nblksKp + nUblks = (ip->nfkblks+1) */
   size_t wsz;            /* gap between threads' works */
   #ifndef TCPLX
      TYPE beta;
   #endif
   uint ndone;             /* [0,P] */
};

typedef struct ATL_tamm_tNK ATL_tamm_tNK_t;
struct ATL_tamm_tNK
{
   rkinfo_t *rp;        /* properly setup rank-K serial data structure */
   const TYPE *A, *B;   /* input (A & B) matrices */
   TYPE *C;             /* output matrix */
   TYPE *wB;            /* workspace for B, shared by all threads */
   TYPE *w;             /* wrkspc for A&C, len=wsz*P */
   void *MbCtr;         /* which M block */
   void *BassgCtr;      /* 1 means must be copied, 0 assigned */
   void *BdoneCtr;      /* 0 means must be copied, 1 not ready */
   size_t wsz;          /* gap between thread's workspaces */
   ushort P;            /* # of threads assigned to this problem */
};
/*
 * This data structure for doing access-major threaded gemm for moderately
 * sized problems where no dimension is <= to the blocking factor, and
 * the problem is not too large to prevent us from copying all of A & B
 * up front.  Copying up front allows us to compute the blocks of C
 * in any order.  For large problems, will have to recur (mainly on K) to
 * get workspace down to reasonable levels.  This case will not work well
 * if the number of C blocks is not quite a bit larger than the nthreads.
 */
typedef struct ATL_tamm_gOOO ATL_tamm_gOOO_t;
struct ATL_tamm_gOOO
{
   ipinfo_t *ip;
   const TYPE *A;    /* input matrix */
   const TYPE *B;    /* input matrix */
   TYPE *C;          /* output matrix */
   TYPE *wC;         /* ATL_NTHREADS thread-local mb*nb workspaces */
   TYPE *wA, *wB;    /* workspaces for storing A & B */
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   ATL_BV_t *cpyAdBV;/* nmblks BV, set means K-panel of A has been copied */
   ATL_BV_t *cpyBdBV;/* nnblks BV, set means K-panel of A has been copied */
   ATL_BV_t *cCblkBV;/* nMNblks BV for dealing C blks while copying */
   ATL_BV_t *cbetaBV;/* nMNblks BV: unset means beta not yet applied */
   ATL_BV_t *lbetaBV;/* ncK BV: unset means beta not yet applied */
   void *cpmut;      /* mutex protecting cpyA/BdBV & cCblkBV */
   void *cbetamut;   /* mutex protecting cbetaBV */
   void *lbetamut;   /* mutex protecting lbetaBV */
   void *ccCtr;      /* nMNblks ctr for dealing out (diagonal) blocks wt copy */
   void *cCtr;       /* ncblks ctr for dealing out blocks w/o copy */
   void **KbegCtr;   /* counters for dealing out K blocks for copying */
   void **KdonCtr;   /* K-counters for copied blocks */
   void **KlastCtr;  /* [1,ncK]: final C blks par on K for load balance */
   void **Cmuts;     /* MNblks mutex locks for copy-blocks of C */
   size_t nmblks;    /* ip->nfmblks + ip->npmblks */
   size_t nnblks;    /* ip->nfmblks + ip->npmblks */
   size_t nCblks;    /* nmblks * nnblks */
   size_t nMNblks;   /* MAX(nmblks,nnblks) */
   #ifndef TCPLX
      TYPE beta;
   #endif
   #ifdef ATL_PHI_SORTED
      VINT *chkin;   /* ncores*ATL_Cachelen check-in array */
      int ncores;    /* ncores, on PHI it is nthr/4 */
      int ncntxts;   /* ncontexts to use per core */
   #endif
   ushort ncK;       /* # of C blks reserved to share K for load balance */
   VSHORT cpyAdone;  /* set when all of A has been copied */
   VSHORT cpyBdone;  /* set when all of B has been copied */
   VSHORT NOCPWORK;  /* set to 1 by 1st thread to find all copy work started */
};
/*
 * This structure used for inner-product (K > MAXKB) -based solutions.
 * It uses the MNK loop order, so it loops over row-panels of C in outer loop.
 * It copies all of A & B up front before starting the computation.
 */
typedef struct ATL_tamm_gMNK ATL_tamm_gMNK_t;
struct ATL_tamm_gMNK
{
   ipinfo_t *ip;
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   const TYPE *A, *B;   /* input (A & B) matrices */
   TYPE *C;             /* output matrix */
   TYPE *wA;            /* workspace for A, shared by all threads */
   TYPE *wB;            /* workspace for B, shared by all threads */
   TYPE *wC;            /* wrkspc for C, len=szC*P, part among threads */
   void *RowCtr;        /* [1,nByRows] */
   void **BlkCtrs;      /* nByBlks [1,nnblks] Atomic Ctrs */
   void *asgActr;       /* [1,nBblks] */
   void *asgBctr;       /* [1,nAblks] */
   #if ATL_CBC_STRONG
      void *donActr;    /* [1,nAblks] */
      void *donBctr;    /* [1,nBblks] */
   #endif
   size_t nByRows;        /* # of rowpans dealt out 1 colpan at time */
   size_t nByBlks;        /* # of last rowpans worked on by block */
   size_t nAblks;         /* nmblks*nkblks, total # of blks in A */
   size_t nBblks;         /* nkblks*nnblks, total # of blks in B */
   #ifndef TCPLX
      TYPE beta;
   #endif
};
/*
 * This structure used for K > MAXKB, but K small enough we can allocate
 * all of A plus a few colpans of B.
 */
typedef struct ATL_tamm_sNK ATL_tamm_sNK_t;
struct ATL_tamm_sNK
{
   ipinfo_t *ip;
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   const TYPE *A, *B;   /* input (A & B) matrices */
   TYPE *C;             /* output matrix */
   TYPE *wB;            /* workspace for B, shared by all threads */
   TYPE *wAb;           /* wrkspc for last nByBlks blks of A */
   TYPE *w;             /* wrkspc for A&C, len=wsz*P */
   void *RowCtr;        /* [1,nByRows] */
   void **BlkCtrs;      /* nByBlks [1,nnblks] Atomic Ctrs */
   void *begBCtr;       /* [1,nnblks], assign B copy */
   void *begABlksCtr;   /* [1,nByBlks] */
   #if ATL_CBC_STRONG
      void *donBCtr;    /* [1,nnblks], indicate B copy done */
      void *donABlksCtr;/* [1,nByBlks] */
      void *begACtr;    /* 1: you should copy common A */
      void *donACtr;    /* 0: common A is ready, you can proceed */
   #endif
   size_t wsz;          /* gap between thread's workspaces */
   size_t szAp;         /* # of elts in one K-panel of A */
   #ifndef TCPLX
      TYPE beta;
   #endif
   uint nByRows;        /* # of colpans dealt out 1 colpan at time */
   uint nByBlks;        /* # of last colpans worked on by block */
};
@whiledef shp tK sMK
   @addkeys shp=@(shp)
@SHP tK
/*
 * This structure used when K <= MAXKB, and nnblks isn't too small
 */
@SHP sMK
/*
 * This structure used for K > MAXKB, but K small enough we can allocate
 * all of A plus a few colpans of B.
 */
@SHP !
typedef struct ATL_tamm_@(shp) ATL_tamm_@(shp)_t;
struct ATL_tamm_@(shp)
{
@SHP tK
   rkinfo_t *rp;        /* properly setup rank-K serial data structure */
@SHP smK
   ipinfo_t *ip;
   #ifdef TCPLX
      const TYPE *beta;
   #endif
@SHP !
   const TYPE *A, *B;   /* input (A & B) matrices */
   TYPE *C;             /* output matrix */
   TYPE *wA;            /* workspace for A, shared by all threads */
   TYPE *wBb;           /* wrkspc for last nByBlks blks of B */
   TYPE *w;             /* wrkspc for B&C, len=wsz*P */
   void *ColCtr;        /* [1,nByCols] */
   void **BlkCtrs;      /* nByBlks [1,nmblks] Atomic Ctrs */
   void *begACtr;       /* [1,nmblks], assign A copy */
   void *begBBlksCtr;   /* [1,nByBlks] */
   #if ATL_CBC_STRONG
      void *donACtr;    /* [1,nmblks], indicate A copy done */
      void *donBBlksCtr;/* [1,nByBlks] */
      void *begBCtr;    /* 1: you should copy common B */
      void *donBCtr;    /* 0: common B is ready, you can proceed */
   #endif
   size_t wsz;          /* gap between thread's workspaces */
@SHP smK
   size_t szBp;         /* # of elts in one K-panel of B */
   #ifndef TCPLX
      TYPE beta;
   #endif
@SHP !
   uint nByCols;        /* # of colpans dealt out 1 colpan at time */
   uint nByBlks;        /* # of last colpans worked on by block */
   ushort P;            /* # of threads assigned to this problem */
};
@endwhile
/*
 * This structure used for building a full GEMM out ot a series of rank-K
 * updates, where K ~ 4*MAXKB.  Assumes N/nb >= P.  Wrkspc ~ 2MK + P(szA+szC)
 * We'll use two copies of this structure to avoid sync between rank-K updates.
 * Entries that are same for both structs marked SHARED (SHA), else SEP
 */
typedef struct ATL_tamm_gK ATL_tamm_gK_t;
struct ATL_tamm_gK
{
   ipinfo_t *ip;        /* SHARED between structs */
   const TYPE *A, *B;   /* input (A & B) matrices; SHARED */
   TYPE *C;             /* output matrix; SHARED */
   TYPE *wA;            /* wrkspc for A, common to threads; SEP */
   TYPE *w;             /* wrkspc for B&C, len=wsz*P, SHARED */
   void *Cmut;          /* mut for C update; SHARED */
   void *betaBV;        /* nnblks*nmblks bitvec for beta; prot by Cmut; SHARD */
   void *done;          /* [1,P] when 0, can reuse struct; SEP */
   void *ColCtr;        /* [1,nnblks] ; SEP */
   void *begACtr;       /* [1,nmblks], assign A copy, SEP */
   #if ATL_CBC_STRONG
      void *donACtr;    /* [1,nmblks], indicate A copy done, SEP */
   #endif
   size_t wsz;          /* gap between thread's workspaces */
   volatile uint RDY;   /* set when struct is available for work assignment */
};
/*
 * This struct use to parallelize case where C consists of only 1 block,
 * so all parallelism must be found along K
 */
typedef struct ATL_tsyrk_tN ATL_tsyrk_tN_t;
struct ATL_tsyrk_tN
{
   ablk2cmat_t blk2c, blk2c_b1;
   cm2am_t a2blk;
   const TYPE *A;
   TYPE *C;
   TYPE *w, *wT;         /* ptrs to global main wrkspc & triang wrkspc */
   #ifdef TCPLX
      const TYPE *alpha, *beta, *ONE;
   #endif
   void *jobCtr;         /* [1,njobs] */
   void *K1ctr;          /* [1,nK1] : counter for individual kb blks */
   void *KB0ctr;         /* if (kb0) 1, else ptr is NULL */
   void *Cmut;           /* mutex protecting write to C/appBeta */
   size_t lda, ldc;      /* stride between rows A/C arrays */
   size_t njobs;         /* number of nKjob chunks to give out */
   size_t nK1;           /* total number of K blks given out individually */
   size_t incAk;         /* A ptr inc after doing kb-sized block */
   size_t szW;           /* stride between threads' part of w */
   #ifndef TCPLX
      TYPE alpha, beta;
   #endif
   uint szA, szC;        /* 1 block storage size for A/C workspace */
   #ifdef Conj_
      ushort NoTrans;    /* = (TA == AtlasNoTrans) */
      ushort Pcnt;       /* set to P & dec during C write-out, prot by Cmut */
   #endif
   ushort N;             /* size of triangle, known small for this case */
   ushort nnu;           /* CEIL(N/NU) */
   ushort kb;            /* size of K for all blks except last */
   ushort kb0, KB0;      /* unpadded & (possibly) padded last-blk size */
   ushort jobshift;      /* power-of-2 of # of kbblks given out as a job */
   ushort appBeta;       /* set to 0 once beta applied; protected by Cmut */
};  /* NOTE: nKjob*njob + nK1 = nkblks; kb0 always in nK1 */

/*
 * This structure used when N <= MIN(MAXNB,MAXMB), so we deal out only
 * K blocks using the global counter KbCtr.  
 */
typedef struct ATL_tsyrk_ammK ATL_tsyrk_ammK_t;
struct ATL_tsyrk_ammK
{
   ammkern_t amm_b0, amm_b1, ammK_b0, ammK_b1;
   cm2am_t a2blk;        /* no-transpose copy */
   cm2am_t b2blk;        /* transpose copy */
   ablk2cmat_t blk2c_b0;/* copy for beta=0 */
   ablk2cmat_t blk2c_b1;/* copy for beta=1 */
   const TYPE *A;       /* input matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* 2*ATL_NTHREADS thread-local mb*nb workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *KbCtr;         /* which k block */
   void *Cmut;          /* mutex lock for block of C */
   size_t wsz;          /* size of local workspace */
   VINT BETA_APPLIED;
   int LOWER;           /* set true if lower triangular C */
   int TA;              /* 0: noTrans; 1: Trans */
   int nkblks;          /* # of k blocks, including any partial block */
   int N;               /* total size of C, known to be <= MAXNB */
   int mb;              /* ((N+mu-1)/mu)*mu */
   int nb;              /* ((N+nu-1)/nu)*nu */
   int mbnb;            /* mb * nb */
   int nmu;             /* CEIL(N/mu) */
   int nnu;             /* CEIL(N/nu) */
   int kb;              /* blocking used for K */
   int kb0;             /* K%kb, if 0, kb */
   int KB0;             /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int lda;             /* leading dim of A */
   int ldc;             /* leading dim of C */
};
/*
 * This structure used by dynamic access-major SYRK
 * In the first phase, we work only on diagonal blocks, while copying both
 * A & A'.  For diag work, we parallelize both N & K dims so that the copy
 * is done as quickly as possible.  Threads coming in first choose differing
 * diag blks; diagonal blocks are dealt out cheaply using the dCtr global
 * counter (which starts at nnblks == ndiag).
 * Once all diagonal blocks are dealt out, new threads will start using
 * the atomic ctr array KbegCtr array to share K work for each diagonal.
 * both KbegCtr & KdonCtr are nnblk-len arrays of atmctrs.  Each
 * counter starts at nkblks.  Once the block pointed to by KbegCtr is
 * completely copied, the copying array increments the KdonCtr.  Only one
 * core per diag will get KdonCtr == 1 after doing his copy, and this
 * core will set the appropraite bit in cpydoneBV, which is a nnblks-len
 * is a nnblks-length mutex-protected bit vector.  If the kth bit is set,
 * that means the ith row of A & jth col of A' has been copied.
 * Once a thread gets KbegCtr for a particular diag of 0, it means there's
 * no more work for this block of C, and so it will seize the appropriate
 * Cdmuts mutex which protects each diagonal block of C, and write its
 * finished contribution out to C.  The first such thread to ever seize
 * the mutex will scope dbetaBV to find this diagonal block needs beta applied;
 * while later threads will use beta=1.
 * Eventually, all diagonal work is finished, and the first processor to
 * get 0 for all dCtr & KbegCtr requests will set NODWORK=1, so later
 * threads don't have to query all the counters to know they should proceed
 * to non-diagonal work.
 *
 * Non-diagonal work work is dealt out using the cblkBV array of tBVs.
 * The gap between column cblkBV is Cstr (Cblk STRide) bytes.
 * The first BV is of lenth nnblks-1, and the last is of length 1, and there
 * are nnblks-1 of them (the last colblk is diag only).  Total number of
 * nondiagonal blocks are ncblks.
 * A unset bit means that particular non-diagonal block has not yet been 
 * assigned to a thread, while a 1 means it has. 
 * So threads performing non-diagonal work will examine cpydonBV elt i & j
 * to see that Cblk(i,j) is available, and will set that bit in the cblkBV
 * array of BVs to reserve it for local computation.
 * When all input operand copying is complete, workers will switch to a mode
 * where they individually work on columns of C, and only at the end will
 * they steal work from each other (i.e. two threads work on same colpan).
 * When all bits are set in cblkBV, then all work has been dealt out.
 */
typedef struct ATL_tsyrk_ammN ATL_tsyrk_ammN_t;
struct ATL_tsyrk_ammN
{
   ipinfo_t *ip;
   ablk2cmat_t syblk2c;
   ablk2cmat_t syblk2c_b1;
   cm2am_t sya2blk;
   const TYPE *A;      /* input matrix */
   TYPE *C;            /* output matrix */
   TYPE *wC;           /* ATL_NTHREADS thread-local nb*nb workspaces */
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   TYPE *wA, *wAt;     /* workspaces for storing A & A' */
   void *cpydonBV;     /* ndiag tBV set means A & A' copy is done */ 
   void *locBVs;       /* P ndiag-len local bitvecs */
   void *cblkBV;       /* arr of tBVs for dealing out work on col 0<=j<nnblks */
   void *cpanDonBV;    /* ndiag tBV: set means this col fully computed */
   void *dbetaBV;      /* nshar BV, prot Cdmuts; unset: beta not yet applied */
   void *diCtr;        /* atmctr deals init diag blks */
   void *dCtr;         /* gatmctr deals out non-init diagonal blocks */
   void *KbegCtr;      /* K-counters for dealing out init diagonal blocks */
   void *KdonCtr;      /* K-counters for completed init diagonal blocks */
   void *Cdmuts;       /* mutex locks for nshar diagonal block of C */
   size_t wrksz;       /* total elts in each threads' wC array */
   size_t LOCgap;      /* gap in bytes between the P locBVs */
   size_t diCgap;      /* gap in bytes between diCtrs */
   size_t Cgap;        /* gap (in bytes) between cblk BVs */
   size_t dCgap;       /* gap (in bytes) between dCtr BVs */
   size_t KBCgap;      /* gap (in bytes) between KbegCtrs */
   size_t KDCgap;      /* gap (in bytes) between KdonCtrs */
   size_t dCinc;       /* gap (in bytes) between Cdmuts */
   size_t lda, ldc;    /* gap between columns (stride within rows) */
   size_t pansz0;      /* size of missing A/At panel */
   #ifndef TCPLX
      TYPE beta;
   #endif
   ulong ndiag;   /* # of diagonal blocks */
   ulong nshar;   /* # of diag blks using max parallism/overhead */
   ulong ncblks;  /* # of non-diagonal blocks left to be assigned */
   ulong nkblks;  /* # of k blocks, including any partial block */
   VINT cpydone;  /* set when all A/A' copying complete */
   uint szCs;     /* size of SYRK's C */
   uint szS;      /* size of SYRK A blk */
   uint flg;      /* bitvec:0: set means C is upper, 1: set TA==AtlasNoTrans */
   uint ncpDiag;  /* # of threads for full-time diagonal copy/compute */
   VCHAR NOINICPY;/* set to 1 by first thread to find all init diag blks cpd */
   VCHAR NODWORK; /* set to 1 by first thread to find all work started */
   VCHAR DONE;    /* set to 1 by first thread to find all work complete */
   VCHAR ngrab;   /* # of off-diag C blks to take at once */
};
@beginskip
/*
 * This structure used by dynamic access-major SYRK
 * In the first phase, we work only on diagonal blocks, while copying both
 * A & A'.  For diag work, we parallelize both N & K dims so that the copy
 * is done as quickly as possible.  Threads coming in first choose differing
 * diag blks; diagonal blocks are dealt out cheaply using the dCtr global
 * counter (which starts at nnblks == ndiag).
 * Once all diagonal blocks are dealt out, new threads will start using
 * the atomic ctr array KbegCtr array to share K work for each diagonal.
 * both KbegCtr & KdonCtr are nnblk-len arrays of atomic counters.  Each
 * counter starts at nkblks.  Once the block pointed to by KbegCtr is
 * completely copied, the copying array increments the KdonCtr.  Only one
 * core per diag will get KdonCtr == 0 after doing his copy, and this
 * core will seize cdmut mutex in order to set the appropriate bit in
 * cpydonBV, which is a nnblks-length bit vector.  If the kth bit is set,
 * that means the ith row of A & jth col of A' has been copied.
 * Once a thread gets KbegCtr for a particular diag of 0, it means there's
 * no more work for this block of C, and so it will seize the appropriate
 * Cdmuts mutex which protects each diagonal block of C, and write its
 * finished contribution out to C.  The first such thread to ever seize
 * the mutex will scope dbetaBV to find this diagonal block needs beta applied;
 * while later threads will use beta=1.
 * Eventually, all diagonal work is finished, and the first processor to
 * get 0 for all dCtr & KbegCtr requests will set NODWORK=1, so later
 * threads don't have to query all the counters to know they should proceed
 * to non-diagonal work.
 *
 * For non-diagonal work, we count the number of non-diagonal blocks of C,
 * which is initially stored in the ncblks variable, which is protected
 * the cwmut mutex, which also protects the cblkBV, which is a ncblk-len BV.
 * A unset bit means that particular non-diagonal block has not yet been 
 * assigned to a thread, while a 1 means it has.  The mutex also protects
 * the cpydone variable, which is set to 1 when cpydonBV has all bits set.
 * So, threads wanting to do non-diagonal work will find the first unset
 * bit in cblkBV, and then translate that to a (i,j) C block coordinate.
 * If the ith & jth bits are both set in cpydonBV (or cpydone is set), then
 * they will take that block as their own to do (in this phase, each thread
 * gets an individual block of C to do) by setting the bit in cblkBV.
 * When all bits are set in cblkBV, then all work has been dealt out, and
 * threads will exit once they say there is no more work to do.
 * The master process joins all created threads, and can delete data structures
 * safely after all joins succeed.
 */
typedef struct ATL_tsyrk_ammN ATL_tsyrk_ammN_t;
struct ATL_tsyrk_ammN
{
   ipinfo_t *ip;
   ablk2cmat_t syblk2c;
   ablk2cmat_t syblk2c_b1;
   cm2am_t sya2blk;
   const TYPE *A;      /* input matrix */
   TYPE *C;            /* output matrix */
   TYPE *wC;           /* ATL_NTHREADS thread-local nb*nb workspaces */
   #ifdef TCPLX
      const TYPE *beta;
   #endif
   TYPE *wA, *wAt;     /* workspaces for storing A & A' */
   ATL_BV_t *cpydonBV; /* nnblks BV set means A & A' copy is done */
   ATL_BV_t *cblkBV;   /* ncblks BV for dealing out cblks */
   ATL_BV_t *dbetaBV;  /* nnblks BV: unset means beta not yet applied */
   ATL_BV_t *cbetaBV;  /* ncblks BV: unset means beta not yet applied */
   void *cdmut;        /* mutex protecting cpydonBV */
   void *cwmut;        /* mutex protecting non-diag work */
   void *dCtr;         /* AtomicCtr for dealing out diagonal blocks */
   void **KbegCtr;     /* K-counters for dealing out diagonal blocks */
   void **KdonCtr;     /* K-counters for completed diagonal blocks */
   void **Cdmuts;      /* mutex locks for each diagonal block of C */
   size_t wrksz;       /* total elts in each threads' wC array */
   size_t lda, ldc;    /* gap between columns (stride within rows) */
   size_t pansz0;      /* size of missing A/At panel */
   #ifndef TCPLX
      TYPE beta;
   #endif
@beginskip
   #if defined(ATL_PHI_SORTED) || defined(ATL_PHI_SORTED2)
      VINT *chkin;   /* ncores*ATL_Cachelen check-in array */
      int ncores;    /* ncores, on PHI it is nthr/4 */
      int ncntxts;   /* ncontexts to use per core */
   #endif
   #ifdef ATL_PHI_SORTED2
      int nnuC;
      int CoffC;
      int BoffC;
   #endif
@endskip
   VINT cpydone;  /* set when all A/A' copying complete */
   uint szCs;     /* size of SYRK's C */
   uint szS;      /* size of SYRK A blk */
   uint flg;      /* bitvec:0: set means C is upper, 1: set TA==AtlasNoTrans */
   int ndiag;     /* # of diagonal blocks */
   int ncblks;    /* # of non-diagonal blocks left to be assigned */
   int nkblks;    /* # of k blocks, including any partial block */
   int NODWORK;   /* set to 1 by first thread to find all work started */
};
@endskip
/*
 * This data structure for doing access-major threaded gemm for moderately
 * sized problems where no dimension is <= to the blocking factor, and
 * the problem is not too large to prevent us from copying all of A & B
 * up front.  Copying up front allows us to compute the blocks of C 
 * in any order.  For large problems, will have to recur (mainly on K) to
 * get workspace down to reasonable levels.  This case will not work well
 * if the number of C blocks is not quite a bit larger than the nthreads.
 */

typedef struct ATL_tgemm_ammG ATL_tgemm_ammG_t;
struct ATL_tgemm_ammG
{
   ammkern_t amm_b0, amm_b1, ammK_b0, ammK_b1;
   cm2am_t a2blk;    /* copy/transpose for A */
   cm2am_t b2blk;    /* copy/tranpose for B */
   ablk2cmat_t blk2c,/* access-major to column-major copy/scale for C */
     blk2c_b1;       /* access-major to column-major copy for C, BETA=1 */
   const TYPE *A;    /* input matrix */
   const TYPE *B;    /* input matrix */
   TYPE *C;          /* output matrix */
   TYPE *wC;         /* ATL_NTHREADS thread-local mb*nb workspaces */
   TYPE *wA, *wB;    /* workspaces for storing A & B */
   SCALAR beta;      /* beta */
   SCALAR alpA;      /* alpha to apply to A */
   SCALAR alpB;      /* alpha to apply to B */
   SCALAR alpC;      /* alpha to apply to C */
   ATL_BV_t *cpyAdBV; /* nmblks BV, set means K-panel of A has been copied */
   ATL_BV_t *cpyBdBV; /* nnblks BV, set means K-panel of A has been copied */
   ATL_BV_t *cCblkBV; /* nMNblks BV for dealing C blks while copying */
   ATL_BV_t *cbetaBV; /* nMNblks BV: unset means beta not yet applied */
   void *cpmut;      /* mutex protecting cpyA/BdBV & cCblkBV */
   void *cbetamut;   /* mutex protecting cbetaBV */
   void *ccCtr;      /* nMNblks ctr for dealing out (diagonal) blocks wt copy */
   void *cCtr;       /* ncblks ctr for dealing out blocks w/o copy */
   void **KbegCtr;   /* counters for dealing out K blocks for copying */
   void **KdonCtr;   /* K-counters for copied blocks */
   void **Cmuts;     /* MNblks mutex locks for copy-blocks of C */
   size_t panszA;    /* nkblks * blkszA */
   size_t panszB;    /* nkblks * blkszB */
   size_t lda;       /* leading dim of A */
   size_t ldb;       /* leading dim of B */
   size_t ldc;       /* leading dim of C */
   #ifdef ATL_PHI_SORTED
      VINT *chkin;   /* ncores*ATL_Cachelen check-in array */
      int ncores;    /* ncores, on PHI it is nthr/4 */
      int ncntxts;   /* ncontexts to use per core */
   #endif
   VINT cpyAdone;    /* set when all of A has been copied */
   VINT cpyBdone;    /* set when all of B has been copied */
   VINT NOCPWORK;    /* set to 1 by 1st thread to find all copy work started */
   int TA;           /* 0: noTrans; 1: Trans */
   int TB;           /* 0: noTrans; 1: Trans */
   int nCblks;       /* nmblks * nnblks */
   int nMNblks;      /* MAX(nmblks,nnblks) */
   int nmblks;       /* CEIL(M/mb) */
   int nnblks;       /* CEIL(N/nb) */
   int nkblks;       /* CEIL(K/kb) */
   int mb;           /* M blocking used for all but remainder block */
   int nb;           /* N blocking used for all but remainder block */
   int nmu;          /* mb/mu */
   int nnu;          /* nb/nu */
   int mbf;          /* M%mb, if 0, mb */
   int nbf;          /* N%nb, if 0, nb */
   int nmuf;         /* CEIL(mbf/mu) */
   int nnuf;         /* CEIL(nbf/nu) */
   int kb;           /* blocking used for K */
   int kb0;          /* K%kb, if 0, kb */
   int KB0;          /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int blkszA;       /* mb*kb */
   int blkszB;       /* nb*kb */
   int blkszC;       /* mb*nb */
};
   #undef uint
   #undef ushort
   #undef VINT
   #undef ulong
#endif
/*
 * This data structure used when we divide N only, and NTHREAD is a power of 2
 */
typedef struct 
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   void *T;             /* Triangular matrix to do SYRK into*/
   void *C;             /* rect matrix to do GEMM into */
   const void *A0;      /* input matrix for syrk, */
   const void *A;       /* 1st input matrix for GEMM */
   const void *B;       /* 2nd input matrix for GEMM */
   const void *alpha, *beta;
   ATL_INT M;           /* size of SYRK and 1st dim of GEMM */
   ATL_INT N;           /* size of 2nd dim of N */
   ATL_INT K;           /* K of original problem */
   ATL_INT lda, ldc;
   int nb, eltsh;       /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_M_t;

@beginskip
/*
 * This structure is used for amm-based SYRK, dealing out row- (Upper)
 * or column (Lower) panels of C.  Assume Lower for rest of discussion.
 * The first N/nb threads copy A while computing the first column panel of C,
 * and then entire column panels of C are given out as work.  Load balance
 * isn't as bad as it sounds, since the size of the colpan goes down by one
 * block at each step.
 * This code uses mutexes to avoid probs on systems with weakly-ordered caches.
 * workspace ~ N*K (A), P * [ K*NB (A^T) + NB^2 (C) + syrkSpc ]
 */
typedef struct 
{
   ipinfo_t *ip,
   ablk2cmat_t syblk2c;
   cm2am_t sya2blk;
   const TYPE *A,
   TYPE *C,
   TYPE *wP, /* workspace for each thread, sep by incWP */
   TYPE *wA,  /* common workspace for gemm's A */
   #ifdef TCPLX
      const TYPE *beta
   #endif
   ATL_BV_t **lcblkBV;   /* each thr gets local C blk BV */
   void *pan0Ctr,   /* Deals out C blks in initial panel (copy A) */
   size_t incWP,
} ATL_TSYRK_CPAN_t
@endskip
@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
typedef struct
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A0;  /* input matrix, split only along N */
   const void *A1;  /* A of 2nd triangular matrix (C), or B of gemm */
   void *T;         /* 1st triangular matrix */
   void *C;         /* if (T), 2nd triangular mat, else rect matrix */
   const void *alpha, *beta;
   ATL_INT M;      /* if (T) order of 1st triang mat, else 1st dim of C */
   ATL_INT N;      /* if (T) order of 2nd triang mat, else 2nd dim of C */
   ATL_INT K;      /* size of K dim (2nd dim of A, first of A^T) */
   ATL_INT lda, ldc;
   int nb, eltsh;  /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_N_t;
/*
 * sets ATL_TSYRK_N_t sy_[i] = sy[j]
 */
#define McpSYN(sy_, i_, j_) \
@skip   memcpy((sy_)+(i_), (sy_)+(j_), sizeof(ATL_TSYRK_N_t));
{ \
   (sy_)[(i_)].gemmK  = (sy_)[(j_)].gemmK; \
   (sy_)[(i_)].tvsyrk = (sy_)[(j_)].tvsyrk; \
   (sy_)[(i_)].numthr = (sy_)[(j_)].numthr; \
   (sy_)[(i_)].A0     = (sy_)[(j_)].A0; \
   (sy_)[(i_)].A1     = (sy_)[(j_)].A1; \
   (sy_)[(i_)].T      = (sy_)[(j_)].T; \
   (sy_)[(i_)].C      = (sy_)[(j_)].C; \
   (sy_)[(i_)].alpha  = (sy_)[(j_)].alpha; \
   (sy_)[(i_)].beta   = (sy_)[(j_)].beta; \
   (sy_)[(i_)].M      = (sy_)[(j_)].M; \
   (sy_)[(i_)].N      = (sy_)[(j_)].N; \
   (sy_)[(i_)].K      = (sy_)[(j_)].K; \
   (sy_)[(i_)].nb     = (sy_)[(j_)].nb; \
   (sy_)[(i_)].lda    = (sy_)[(j_)].lda; \
   (sy_)[(i_)].ldc    = (sy_)[(j_)].ldc; \
   (sy_)[(i_)].eltsh  = (sy_)[(j_)].eltsh; \
   (sy_)[(i_)].Uplo   = (sy_)[(j_)].Uplo; \
   (sy_)[(i_)].TA     = (sy_)[(j_)].TA; \
   (sy_)[(i_)].TB     = (sy_)[(j_)].TB; \
}
#endif
@endskip

typedef struct
{
   const void *A, *alpha;
   void *B;
@skip   void (*trsmK)(ATL_TTRSM_t*);
   ATL_INT M, N, lda, ldb;
@skip   int eltsh;                   /* shift for element size */
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
   enum ATLAS_TRANS TA;
   enum ATLAS_DIAG  diag;
} ATL_TTRSM_t;

typedef struct
{
   const void *A, *B, *alpha, *beta;
   void *C;
   ATL_INT M, N, lda, ldb, ldc, nb;
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
} ATL_TSYMM_t;
typedef struct
{
   const void *alpha, *alpha2, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvApAt)(const enum ATLAS_UPLO, ATL_CINT, const void *, ATL_CINT, 
                  const void *, void *, ATL_CINT);

   ATL_INT K, lda, ldb, ldc;
   int nb, eltsh;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS trans, TA, TB,  /* trans for syr2k, TA,TB are for GEMM */
                    TA2, TB2;       /* transpose of TA,TB */
} ATL_SYR2K_t;

@beginskip
typedef struct
{
   const void *alpha, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A, *B;
   void *C;
   ATL_INT T, M, N, K, nb, ia, ja, ib, jb, ic, jc, eltsh, lda, ldc;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans;
} ATL_TSYRK_t;
@endskip
/*
 * For triangular matrices, diagonal blocks are handled specially, but we
 * get dense square blocks above/below the diagonal.  We consider Upper
 * triangular the transpose of Lower, allowing us to only handle Lower.
 * Our AtomicCtr routines are 1-D counters, not 2-D, so we linearize the
 * blocks beneath the diagonal by counting them column-wise.  So, a 4x4
 * matrix of blocks would look like:
 *    |X X X X|
 *    |1 X X X|
 *    |2 4 X X|
 *    |3 5 7 X|
 */
/*
 * Translates (i,j) coordinate of lower triangular matrix to number between
 * [0,n), where n = number of non-diagonal block.  nm_ is the number of
 * diagonal blocks in the matrix (4 in example above).  
 */
#define Mcoord2tblk(nm_, i_, j_) \
   ((nm_)*(j_) - (((1+(j_))*(j_))>>1) + (i_)-(j_)-1)

/*
 * In a lower matrix of with NB_ diagonal blocks, translate the linearized
 * block number B_ back into rectangular (I,J) coordinates.  The difficulty
 * is finding J.  Would like to do it with an equation, like we do when
 * converting from coordinates to block number.  Tony Castaldo came up with
 *    J = (int)((nm-0.5-0.5*sqrt(4*nm*nm-4*nm+1-8*b)))
 * but sqrt is a function call which does a Newtonian iteration on floats
 * (therefore, has a relatively slow loop).  Finding the J column is indeed
 * the hard part, and in theory we can use binary search to find in 
 * O(log_2(NB_)) time.  However, this algorithm requires multiplication
 * inside the loop, and so it is never competitive for the range we are
 * interested in (NB_ usually < 10, and almost never larger than 2000).
 * So instead do a linear search to find j, but optimize by first
 * constraining j to a 128-column region, then a 8-column region, and
 * then find the actual column.  So worst-case loop counts are
 *  CEIL(NB_/128) + 16 + 8
 *
 * nblkC = # of blocks in a [128,8]-column Chunk
 */
#define Mtblk2coord(NB_, B_, I_, J_) \
{ \
   unsigned int j_=0; \
   unsigned int n_ = (NB_), b_=(B_), nblksC_; \
   KEEP_LOOKING128: /* find 128-col region where J is */ \
      nblksC_ = (n_<<7)-8256; \
      if (b_ < nblksC_ || n_ < 128) \
         goto FOUND128; \
      b_ -= nblksC_; \
      n_ -= 128; \
      j_ += 128; \
   goto KEEP_LOOKING128; \
   FOUND128: \
   KEEP_LOOKING8: /* find 8-col region where J is */ \
      nblksC_ = (n_<<3)-36; \
      if (b_ < nblksC_ || n_ < 8) \
         goto FOUND8; \
      b_ -= nblksC_; \
      n_ -= 8; \
      j_ += 8; \
   goto KEEP_LOOKING8; \
   FOUND8: \
      for (n_--; b_ >= n_; j_++) \
         b_ -= n_--; \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}
@beginskip
/*
 * buffer i, j in case they aren't type int; hopefully compiler will eliminate
 * when they are through copy propogation
 */
#define Mtblk2coord(NM_, B_, I_, J_) \
{ \
   int i_, j_; \ 
   ATL_tblk2coord(NM_, B_,  &(i_), &(j_)); \
   I_ = i_; \
   I_ = i_; \
}
#define Mtblk2coord(NM_, B_, I_, J_) \
{ \
   register int n_ = (NM_)-1, b_=(B_), j_; \
   for (j_=0; b_ >= n_; j_++) \
   { \
      b_ -= n_; \
      n_--; \
   } \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}
@endskip

/*
 * =============================================================================
 * Function prototypes
 * =============================================================================
 */
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P);
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                         size_t M, size_t N, size_t K);
@skip int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
void ATL_tvsyr2k_rec(ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, 
                     const void *A, const void *B, void *C);
#ifdef TYPE
void Mjoin(PATL,ipCompBlk)
   (ipinfo_t *ip, size_t i, size_t j, size_t k, int be,
    TYPE *wA, TYPE *wB, TYPE *wC, TYPE *nA, TYPE *nB, TYPE *nC);
@beginskip
#ifdef TCPLX
   void Mjoin(PATL,therk_tN)
      (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA, ATL_CSZT N, 
       ATL_CSZT K, const TYPE ralpha, const TYPE *A, ATL_CSZT lda,
       const TYPE rbeta, TYPE *C, ATL_CSZT ldc);
#endif
void Mjoin(PATL,tsyrk_tN)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA, ATL_CSZT N, 
    ATL_CSZT K, const SCALAR alpha, const TYPE *A, ATL_CSZT lda,
    const SCALAR beta, TYPE *C, ATL_CSZT ldc);
@endskip
void Mjoin(PATL,tsyrk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,therk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const TYPE alpha, const TYPE *A, ATL_CINT lda,
    const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsymm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,themm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,ther2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif

void Mjoin(PATL,ttrsm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,ttrmm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc);
int Mjoin(PATL,ammm_REC)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    size_t M, size_t N, size_t K, const SCALAR alpha,
    const TYPE *A, size_t lda, const TYPE *B, size_t ldb,
    const SCALAR beta, TYPE *C, size_t ldc, ATL_UINT flg,
    int (*amm)(enum ATLAS_TRANS,enum ATLAS_TRANS, size_t, size_t, size_t,
               const SCALAR, const TYPE*, size_t,  const TYPE*, size_t,
               const SCALAR, TYPE*, size_t));
@multidef rt tammm_G
@whiledef rt tammm_tMN tammm_tNK tammm_tK tammm_sMK tammm_sNK tammm tammm_gMNK
int Mjoin(PATL,@(rt))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    size_t M, size_t N, size_t K, const SCALAR alpha,
    const TYPE *A, size_t lda, const TYPE *B, size_t ldb,
    const SCALAR beta, TYPE *C, size_t ldc);
@endwhile
@whiledef rt
int Mjoin(PATL,@(rt))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
@endwhile

@whiledef ds rec rkK bigMN_Kp
int Mjoin(PATL,tgemm_@(ds))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
@endwhile
@whiledef TA T N C
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb,
    const void *beta, void *C, ATL_CINT ldc);
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
   @endwhile
@endwhile
TYPE *Mjoin(PATL,ipcopyA)
   (ipinfo_t *ip, const TYPE *A, size_t i, size_t k, TYPE *w);
TYPE *Mjoin(PATL,ipcopyB)
   (ipinfo_t *ip, const TYPE *B, size_t k, size_t j, TYPE *w);
#endif  /* end ifdef TYPE */
/*
 * Helper functions we'd like to inline
 */
@beginskip
#ifdef ATL_INDX
const TYPE *IdxA_rkK(rkinfo_t *op, const TYPE *A, size_t i)
{
   size_t m;

   m = op->nfmblks;
   m = Mmin(m, i);
   i -= m;
   A += m * op->incAm + i * op->pincAm;
   return(A);
}
const TYPE *IdxB_rkK(rkinfo_t *op, const TYPE *B, size_t j)
{
   size_t n;

   n = op->nfnblks;
   n = Mmin(n, j);
   j -= n;
   B += op->incBn * n + op->pincBn * j;
   return(B);
}

TYPE *IdxC_rkK(rkinfo_t *op, TYPE *C, size_t i, size_t j)
{
   size_t n;

   n = op->nfmblks;
   n = Mmin(n, i);
   i -= n;
   C += (op->mb * n + op->pmb * i)SHIFT;
   n = op->nfnblks;
   n = Mmin(n, i);
   j -= n;
   C += (op->nb * n + op->pnb * j)*((op->ldc) SHIFT);
   return(C);
}
#endif
@endskip
#ifdef ATL_ESTNCTR
   #ifdef __GNUC__
   static inline int ATL_EstNctr    /* RETURNS: good # of global ctrs */
   #else
   static int ATL_EstNctr           /* RETURNS: good # of global ctrs */
   #endif
   (
      int N,   /* Total count you are going to use */
      int P    /* nthreads to be used */
   )
   {
      int ncnt, ncntP;
/*
 *    Want at least 32 elements per counter
 */
      ncnt = (N > 64) ? (N >> 5) : 1;
/*
 *    allow up to 8-way contention
 */
      if (P >= 16)
         ncntP = (P>>3);
      else 
         ncntP = (P >= 4) ? (P>>1) : 1;
      return(Mmin(ncnt,ncntP));
   }
#endif

#endif
@ROUT ATL_set_ucnt
/*
 * This routines provide the basis of ATLAS's fast and almost-contention-free
 * partitioning algorithm.  Before spawning the threads, the problem is
 * divided into n partitions, and the master thread allocates an unordered
 * counter wt call to ATL_set_ucnt(n).  
 * After threading is complete, the space is freed by ATL_free_ucnt(void*vp);
 * Now, threads call ATL_get_ucnt(void *vp), which returns 0 if all partitions
 * have been handled; a number between 1 & n says that that partition of the
 * problem remains to be done.
 * We can then use an assembly command like XCHG to determine if a given
 * set is available for use.
 * To ease the amount of cache coherence message, each thread gets his
 * own region in each space, which looks like:
 * <p> <n1> <rk1 off> ... <nP> <rkP off> <rk1 region>...<rkP region>
 * beginning of space aligned to CL, all regions start on CL boundary.  Each
 * region is n/p long, with any remainders stuck in the first n%P regions.
 * Each thread will pass his rank in and therefore will do the majority of
 * writing on his own region (avoiding ping-ponging lines thru cache).  
 * Only once all his sets are exhausted will he search other thread's sets
 * (thus starting ping-pong).
 * All we need to implement is something like XCHG, which everybody has:
 * SPARC: LDSTUB: ld/store unsigned byte loads value from memory, rights 0xff
 *                to byte atomically 
 *           http://developers.sun.com/solaris/articles/atomic_sparc/
 * x86  : XCHG
 * PPC  : lwarx/stwcx, see: http://www.ibm.com/developerworks/library/pa-atom/
 * MIPS : ll/sc (load linked store conditional); I think similar to PPC
 *
 * In some systems, we can actually using a simple counter:
 *  SPARC: cas (compare & swap), v9 only
 *   x86 : CMPXCHG
 *   PPC : lwarx/stwcx
 *   MIPS: ll/sc
 *
 * So, can use cntr for all harware we now about, so can implement an assembly
 *    int GetAtomicCount(void *vp)
 * In systems wt cntr support, simply use directly, quitting when count is 0
 * or negative.  For systems that can only handle a boolean (like XCHG/LDSTUB),
 * store the boolean immediately after the counter.  This file will be tested
 * in assembly for compile & use, and we set a Make.inc macro to point at
 * something like: GetAtomicCount_[ppc,sparc,x86,mips,mutex].o, all of
 * which appear in the ATLAS/src/threads directory.
 */
void *ATL_set_ucnt(int nsets, int nranks)
{
}
void ATL_free_ucnt(void *vp)
{
}
int ATL_get_ucnt(void *vp, int rank)
/* 
 * RETURNS: 0 if all sets taken, else a number between 1...nsets
{
}
@ROUT ATL_thread_yield
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"
#ifndef ATL_WINTHREADS
   #include <sched.h>
#endif
void ATL_thread_yield(void)
{
   #ifdef ATL_WINTHREADS
      Sleep(0);
   #elif defined(ATL_ARCH_XeonPHI)
       __asm__ __volatile__ ("delay %0" :: "r"(64) : "0" );
   #else
      sched_yield();
   #endif
}
@ROUT ATL_cond_init ATL_cond_free ATL_cond_signal ATL_cond_bcast ATL_cond_wait
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"
/*
 * condition variable funcs are used in some threadpool implementations, but
 * parallel section sync handles this for OpenMP,
 * On Windows we do not presently support (must use locks or FULLPOLL),
 * so simply issue runtime assert(0) for these implementations so we can
 * find bad calls on Windows or OpenMP.
 */
#if defined(ATL_OMP_THREADS) || defined(ATL_WINTHREADS)
   #define ATL_DIE 1
#endif
@ROUT ATL_cond_init
void *ATL_cond_init(void)
{
   #ifdef ATL_DIE
      ATL_assert(0);
      return(NULL);
   #else
      void *vp;
      vp = malloc(sizeof(pthread_cond_t));
      ATL_assert(vp);
      ATL_assert(!pthread_cond_init(vp, NULL));
      return(vp);
   #endif
}
@ROUT ATL_cond_free
void ATL_cond_free(void *vp)
{
   #ifdef ATL_DIE
      ATL_assert(0);
   #else
      ATL_assert(!pthread_cond_destroy(vp));
      free(vp);
   #endif
}
@ROUT ATL_cond_bcast
void ATL_cond_bcast(void *cond)
{
   #ifdef ATL_DIE
      ATL_assert(0);
   #else
      ATL_assert(!pthread_cond_broadcast(cond));
   #endif
}
@ROUT ATL_cond_signal
void ATL_cond_signal(void *cond)
{
   #ifdef ATL_DIE
      ATL_assert(0);
   #else
      ATL_assert(!pthread_cond_signal(cond));
   #endif
}
@ROUT ATL_cond_wait
void ATL_cond_wait(void *cond, void *mut)
{
   #ifdef ATL_DIE
      ATL_assert(0);
   #else
      ATL_assert(!pthread_cond_wait(cond, mut));
   #endif
}
@ROUT ATL_mutex_init
#include "atlas_misc.h"
#include "atlas_threads.h"
void *ATL_mutex_init(void)
{
/*
 * On Windows, use known-good x86 code.  OS X's mutex have horrible scaling,
 * so use homebrewed code instead
 */
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_SetAtomicCount(1));
#elif defined(ATL_OMP_THREADS)
   void *vp;
   vp = malloc(sizeof(omp_lock_t));
   ATL_assert(vp);
   omp_init_lock(vp);
   return(vp);
#else
   void *vp;
   vp = malloc(sizeof(pthread_mutex_t));
   ATL_assert(vp);
   ATL_assert(!pthread_mutex_init(vp, NULL));
   return(vp);
#endif
}
@ROUT ATL_mutex_free
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_mutex_free(void *vp)
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_FreeAtomicCount(vp);
#elif defined(ATL_OMP_THREADS)
   omp_destroy_lock(vp);
   free(vp);
#else
   ATL_assert(!pthread_mutex_destroy(vp));
   free(vp);
#endif
}
@ROUT ATL_mutex_free
@ROUT ATL_mutex_lock
   @define rt @lock@
@ROUT ATL_mutex_unlock
   @define rt @unlock@
@ROUT ATL_mutex_lock ATL_mutex_unlock
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_mutex_@(rt)(void *vp)
{
@ROUT ATL_mutex_lock
#ifdef ATL_WINTHREADS  /* if not using pthreads, use AtomicCount to sim mut */
   while(!ATL_DecAtomicCount(vp));
#elif defined(ATL_OS_OSX) && defined(ATL_SSE1)
   while(!ATL_DecAtomicCount(vp))
      ATL_thread_yield();
@ROUT ATL_mutex_unlock
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_ResetAtomicCount(vp, 1);
@ROUT ATL_mutex_lock ATL_mutex_unlock
#elif defined(ATL_OMP_THREADS)
@ROUT ATL_mutex_unlock `   omp_unset_lock(vp);`
@ROUT ATL_mutex_lock `   omp_set_lock(vp);`
#else
   ATL_assert(!pthread_mutex_@(rt)(vp));
#endif
}
@ROUT ATL_mutex_trylock
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"

int ATL_mutex_trylock(void *vp)
/*
 * return 0 if lock not required, else return non-zero 
 */
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_DecAtomicCount(vp));
#elif defined(ATL_OMP_THREADS)
   return(omp_test_lock(vp));
#else
   return(!pthread_mutex_trylock(vp));
#endif
}
@ROUT ATL_SetAtomicCount_mut
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"

void *ATL_SetAtomicCount(int cnt)
{
#if defined(ATL_OMP_THREADS)
   char *cp;
   omp_lock_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(omp_lock_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (omp_lock_t*)(cntp+2);
   omp_init_lock(mp);
   *cntp = cnt;
   return((void*)cp);
#else
   char *cp;
   pthread_mutex_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(pthread_mutex_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (pthread_mutex_t*)(cntp+2);
   ATL_assert(!pthread_mutex_init(mp, NULL));
   *cntp = cnt;
   return((void*)cp);
#endif
}

@ROUT ATL_SetGlobalAtomicCountDown ATL_DecGlobalAtomicCountDown @\
      ATL_FreeGlobalAtomicCountDown
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"
/*
 * GlobalAtomicCountDowns are same as GlobalAtomicCount, except that they
 * guarantee that their last non-zero value is 1 (they don't guarantee 
 * a sequential count down, despite the name).  They are used over Counts
 * when it is vital that the someone atomically knows they are the last
 * non-zero return (i.e. the last worker frees resources, etc).
 */
@ROUT ATL_SetGlobalAtomicCountDown 
void *ATL_SetGlobalAtomicCountDown
(
   int P,               /* # of Local counters to use to make global ctr */
   int cnt              /* total count to start global count at */
)
{
   void **va;
   va = malloc(2*sizeof(void*));
   ATL_assert(va);
   if (cnt > 2 && P > 1)
   {
      va[0] = ATL_SetGlobalAtomicCount(P, cnt-1, 0);
      va[1] = ATL_SetAtomicCount(1);
   }
   else
   {
      va[0] = NULL;
      va[1] = ATL_SetAtomicCount(cnt);
   }
   return(va);
}

@ROUT ATL_DecGlobalAtomicCountDown
int ATL_DecGlobalAtomicCountDown(void *vp, int rank)
{
   void **va=vp;
   if (va[0])
   {
      int i;
      i = ATL_DecGlobalAtomicCount(va[0], rank);
      if (i)
         return(i+1);
   }
   return(ATL_DecAtomicCount(va[1]));
}

@ROUT ATL_FreeGlobalAtomicCountDown
void ATL_FreeGlobalAtomicCountDown(void *vp)
{
   void **va=vp;
   if (va[0])
      ATL_FreeGlobalAtomicCount(va[0]);
   ATL_FreeAtomicCount(va[1]);
   free(vp);
}
@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_ResetGlobalAtomicCount
void ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc)
/*
 * This routine resets the global atomic counter vp to cnt
 */
{
   int *ip=vp, *lcnts = ip+4;
   const int P = *ip;
   void **cnts = (void**)(lcnts+(((P+3)>>2)<<2));
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount
void *ATL_SetGlobalAtomicCount
(
   int P,               /* # of Local counters to use to make global ctr */
   int cnt,             /* total count to start global count at */
   int percLoc          /* fraction of local work to reserve for callers */
)                       /* whose rank is exactly equal to the cnt index */
/*
 * This routine counts down from cnt to 0 for in a thread-safe way.
 * For scalability, the count is split up into P different counters
 * (this minimizes contention on atomic counters).  Further, if percLoc
 * is non-zero, then .01*fracLoc*local(cnt) numbers will be reserved exclusively
 * for callers that set their rank to the local counter index.  This allows
 * us to force a particular node to do at least that many columns, and for
 * those column accesses we can do it a non-rentrant read, which means it
 * runs much faster.  However, this means the caller will need to be sure
 * in this case that two processors cannot call with the same rank!
 */
{
   void **cnts;
   int *ip, *lcnts;
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   b = cnt / P;
   extra = cnt - b*P;
   nL = (percLoc > 0) ? percLoc*.01*b : 0;
   nG = b - nL;
@ROUT ATL_SetGlobalAtomicCount
   i = ((P+3)>>2)<<2;
   ip = malloc(P*sizeof(void*)+(i+4)*sizeof(int));
   ATL_assert(ip);
   lcnts = ip+4;
   cnts = (void**)(lcnts + i);
@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   ip[0] = P;
   ip[1] = b;
   ip[2] = extra;
   ip[3] = nL;

   for (i=0; i < P; i++)
   {
      int n = nG;
      if (i < extra) n++;
@ROUT ATL_SetGlobalAtomicCount   `      cnts[i] = ATL_SetAtomicCount(n);`
@ROUT ATL_ResetGlobalAtomicCount `      ATL_ResetAtomicCount(cnts[i], n);`
      lcnts[i] = nL;
   }
@ROUT ATL_SetGlobalAtomicCount `   return((void*)ip);`
}

@ROUT ATL_FreeGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_FreeGlobalAtomicCount(void *vp)
{
   int *ip=vp;
   const int P=ip[0];
   void **acnts = (void**)(ip+4+(((P+3)>>2)<<2));
   int i;
   for (i=0; i < P; i++)
      ATL_FreeAtomicCount(acnts[i]);
   free(vp);
}

@ROUT ATL_DecGlobalAtomicCount
   @define op @Dec@
@ROUT ATL_GetGlobalAtomicCount
   @define op @Get@
@ROUT ATL_DecGlobalAtomicCount ATL_GetGlobalAtomicCount
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_@(op)GlobalAtomicCount(void *vp, int rank)
/*
 * This routine returns a global counter that has been distributed over
 * P local counters
 */
{
   int i, j, P, b, icnt, extra, nL, *ip=vp, *iloc;
   void **acnts;

   P = ip[0];
   b = ip[1];
   extra = ip[2];
   nL = ip[3];
   iloc = ip+4;
/*
 * See if I can get the index from purely local information
 */
   if (rank < P && rank >= 0 && nL)
   {
      j = iloc[rank];
      if (j)
      {
@ROUT ATL_DecGlobalAtomicCount `         iloc[rank]--;`
         j += b * rank + Mmin(rank, extra);
//fprintf(stderr, "%d: j=%d, LRET\n", rank, j);
         return(j);
      }
   }
   acnts = (void**) (ip+4+(((P+3)>>2)<<2));
/*
 * Otherwise, find an atomic counter that still has count
 */
   for (i=0; i < P; i++)
   {
/* 
 *    If I got a counter value, convert it from local to global
 */
      icnt = (rank+i)%P;
      if (j = ATL_@(op)AtomicCount(acnts[icnt]))
      {
         j += nL + b*icnt + Mmin(icnt,extra);
         break;
      }
   }
//fprintf(stderr, "%d: j=%d, icnt=%d, b=%d P=%d, e=%d\n", rank, j, icnt, b, P, extra);
@skip      j = (b) ? ((j-1)/b)*P*b + icnt*b + (j-1)%b + 1 : icnt+1;
   return(j);
}
@ROUT ATL_IsFirstThreadedCall
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_threads.h"
#ifdef ATL_OMP_THREADS
   #include "atlas_misc.h"
#endif
/*
 * A kludge for initializing thread pool related stuff on first call
 * in portable fashion.  
 * OpenMP just uses the OpenMP thread pool, so should not call here.
 * pthreads uses standard initializer.
 * Otherwise, I assume Windows, where DecAtomicCtr should work.
 * Note that this assumption is wrong for Windows on ARM (god help us).
 */
#if ATL_USE_POSIXTHREADS
   static pthread_mutex_t initlock=PTHREAD_MUTEX_INITIALIZER;
   static int atlinit=1;
#else   /* non-pthreads is Windows, where AtomicCtrs work */
   volatile static int ilck[65] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
      1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
      1,1,1,1,1,1,1};
#endif
int ATL_IsFirstThreadedCall(void)
{
#ifdef ATL_OMP_THREADS
   ATL_assert(0);  /* OpenMP should never call this! */
#elif ATL_USE_POSIXTHREADS
   int iret;
   pthread_mutex_lock(&initlock);
   iret = atlinit;
   atlinit = 0;
   pthread_mutex_unlock(&initlock);
   return(iret);
#else   /* non-pthreads is Windows, where AtomicCtrs work */
   return(ATL_DecAtomicCtr((void*)ilck);
#endif
}
/*
 * This function not thread safe, so user must make sure only called once
 */
void ATL_ResetThreadPoolInit(void)
{
#if ATL_USE_POSIXTHREADS
   pthread_mutex_lock(&initlock);
   atlinit = 1;
   pthread_mutex_unlock(&initlock);
#else
   ATL_ResetAtomicCount((void*)ilck, 1);
#endif
}
@ROUT ATL_SetBitAtomic_amd64
/*
 * rax                       rdi                rsi
 * int ATL_SetBitAtomic(void *bv, unsigned char pos)
 */
#define b %rsi
#define bv %rdi
#define one %dx
.global ATL_SetBitAtomic
ATL_SetBitAtomic:
   mov $1, %dx      /* if bit is set in memory */
   xor %rax, %rax   /* assume bit unset in memory */
   lock             /* make following bts atomic */
   bts %si, (bv)    /* CF set to old value */
   cmovC %dx, %ax   /* if (bit set in memory) return(1); */
   ret
/*
 * rax                       rdi                rsi
 * int ATL_SetBitAtomic(void *bv, unsigned char pos)
 */
#define b %rsi
#define bv %rdi
#define one %dx
.global ATL_UnsetBitAtomic
ATL_UnsetBitAtomic:
   mov $1, %dx      /* if bit is set in memory */
   xor %rax, %rax   /* assume bit unset in memory */
   lock             /* make following bts atomic */
   btr %si, (bv)    /* CF set to old value */
   cmovC %dx, %ax   /* if (bit set in memory) return(1); */
   ret
@ROUT ATL_lock_mut
#include "atlas_tprim.h"
int ATL_lock(void *vp)
{
#ifdef ATL_WINTHREADS  /* if not using pthreads, use AtomicCount to sim mut */
   #error "should not compile this file under Windows!"
@skip   while(!ATL_DecAtomicCount(vp));
#elif 0 & defined(ATL_OS_OSX) && defined(ATL_SSE1)  /* assume OSX better? */
   #error "should not compile this file under OS X in x86!"
@skip   while(!ATL_DecAtomicCount(vp))
@skip      ATL_thread_yield();
#elif defined(ATL_OMP_THREADS)
   omp_set_lock(vp);
   return(0);
#else
   return(pthread_mutex_lock(vp));
#endif
}
@ROUT ATL_trylock_mut
#include "atlas_tprim.h"
int ATL_trylock(void *vp)
/*
 * return 0 if lock not required, else return non-zero
 */
{
#if defined(ATL_WINTHREADS) /*  || (defined(ATL_OS_OSX) && defined(ATL_SSE1)) */
   #error "should not compile this file under Windows!"
   return(ATL_DecAtomicCount(vp));
#elif defined(ATL_OMP_THREADS)
   return(omp_test_lock(vp));
#else
   return(!pthread_mutex_trylock(vp));
#endif
}
@ROUT ATL_unlock_mut
#include "atlas_tprim.h"
int ATL_unlock(void *vp)
{
#if defined(ATL_WINTHREADS) /* || (defined(ATL_OS_OSX) && defined(ATL_SSE1)) */
   #error "should not compile this file under Windows!"
@skip   ATL_ResetAtomicCount(vp, 1);
#elif defined(ATL_OMP_THREADS)
   omp_unset_lock(vp);
   return(0);
#else
   return(pthread_mutex_unlock(vp));
#endif
}
@ROUT ATL_lock_init
#include "atlas_threads.h"
void ATL_lock_init(void *vp)
{
/*
 * On Windows, use known-good x86 code.  OS X's mutex have horrible scaling,
 * so use homebrewed code instead
 */
#ifdef ATL_USE_mut
   #if defined(ATL_WINTHREADS) /* || (defined(ATL_OS_OSX)&&defined(ATL_SSE1)) */
   #error "should not compile this file under Windows!"
   @skip   return(ATL_SetAtomicCount(1)); 
   #elif defined(ATL_OMP_THREADS)
      omp_init_lock(vp);
   #else
      ATL_assert(!pthread_mutex_init(vp, NULL));
   #endif
#else
   *((unsigned char)vp) = 0;
#endif
}
@ROUT ATL_lock_destroy
#include "atlas_threads.h"
void ATL_lock_destroy(void *vp)
{
#ifdef ATL_USE_mut
   #if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
      #error "should not compile this file under Windows!"
   @skip   ATL_FreeAtomicCount(vp);
   #elif defined(ATL_OMP_THREADS)
      omp_destroy_lock(vp);
   #else
      ATL_assert(!pthread_mutex_destroy(vp));
   #endif
#else
   *((unsigned char)vp) = 0;
#endif
}

@ROUT ATL_trylock_amd64 ATL_trylock_ia32
#include "atlas_asm.h"
/*
 * 1 at memory means locked, 0 means unlocked
 *  ax                 4/rdi
 * int ATL_trylock(void *lck);
 * RETURNS: 0 if lock was obtained, else 1
 */
.text
.global ATL_asmdecor(ATL_trylock)
ATL_asmdecor(ATL_trylock):
@ROUT ATL_trylock_amd64
   mov $1, %rax
@ROUT ATL_trylock_ia32
   mov $1, %eax
@ROUT ATL_trylock_amd64 ATL_trylock_ia32
   xchg %al, (%rdi)
   ret
@ROUT ATL_lock_amd64 ATL_lock_ia32
#include "atlas_asm.h"
/*
 * 1 at memory means locked, 0 means unlocked
 *                   4/rdi
 * void ATL_lock(void *lck);
 * RETURNS: 0 if lock was obtained, else 1
 */
#define one %cl
.text
.global ATL_asmdecor(ATL_lock)
ATL_asmdecor(ATL_lock):
@ROUT ATL_lock_amd64
   xor %rax, %rax
@ROUT ATL_lock_ia32
   xor %eax, %eax
@ROUT ATL_lock_amd64 ATL_lock_ia32
   mov $1, one
   .local LOOP
   LOOP:
      mov one, %al
      xchg %al, (%rdi)
      cmp one, %al      /* if (already locked) */
      je  WAIT          /*    kill some time & try again */
   ret
   .local FAIL
WAIT:
   pause
   jmp LOOP
@ROUT ATL_lock_arm64
#include "atlas_asm.h"
/*
 * 1 at memory means locked, 0 means unlocked
 * w0                x0
 * int ATL_lock(void *lck);
 * RETURNS: 0 if lock was obtained, else 1
 */
#define one %cl
.text
.global ATL_asmdecor(ATL_lock)
ATL_asmdecor(ATL_lock):
   .local LOOP
   mov x1, x0
   mov w2, 1
   LOOP:
      ldxrb w0, [x1]     /* exclusive read of 0/1 into reg reg */
      stxrb w3, w2, [x1] /* store 1, w3 0 on exclusive success */
   cbnz w3, WAIT         /* if exclusive failed, wait & try again */
   cbnz w0, WAIT         /* if already locked, wait & try again */
   ret
.local WAIT:
WAIT:
   YIELD
   jmp LOOP
@ROUT ATL_unlock_arm64
#include "atlas_asm.h"
/*
 * 1 at memory means locked, 0 means unlocked
 * w0                x0
 * int ATL_lock(void *lck);
 * RETURNS: 0 if lock was obtained, else 1
 */
#define one %cl
.text
.global ATL_asmdecor(ATL_unlock)
ATL_asmdecor(ATL_unlock):
   .local LOOP
   mov x1, x0
   eor w2, w2, w2
   LOOP:
      ldaxrb w0, [x1]     /* exclusive read of 0/1 into reg reg */
      stlxrb w3, w2, [x1] /* store 1, w3 0 on exclusive success */
   cbnz w3, LOOP         /* if exclusive failed, try again */
   sub w0, w0, 1         /* make zero if was locked, else non-zere for error */
   ret
@ROUT ATL_lock
#include "atlas_tprim.h"
void ATL_lock(void *lck)
{
   int fail;
   TRY:
      fail = ATL_trylock(lck);
      if (fail)
      {
        ATL_thread_yield();  /* don't magnify contention */
        goto TRY;
      }
}
@ROUT ATL_unlock_amd64 ATL_unlock_ia32
#include "atlas_asm.h"
/*
 * 1 at memory means locked, 0 means unlocked
 *  ax                 4/rdi
 * int ATL_unlock(void *lck);
 */
.text
.global ATL_asmdecor(ATL_unlock)
ATL_asmdecor(ATL_unlock):
@ROUT ATL_unlock_amd64
   xor %rax, %rax
   xchg %al, (%rdi)
@ROUT ATL_unlock_ia32
   movL 4(%esp), %ecx
   xor %eax, %eax
   xchg %al, (%ecx)
@ROUT ATL_unlock_amd64 ATL_unlock_ia32
   dec %al            /* signal error if it wasn't locked */
   ret
@ROUT ATL_AndAtomicMask_amd64 ATL_AndAtomicMask_ia32
  @define nop @And@
  @define op @and@
@ROUT ATL_OrAtomicMask_amd64 ATL_OrAtomicMask_ia32
  @define nop @Or@
  @define op @or@
@ROUT ATL_AndAtomicMask_arm64
  @define nop @And@
  @define op @and@
@ROUT ATL_OrAtomicMask_arm64
  @define nop @Or@
  @define op @orr@
@ROUT ATL_AndAtomicMask_amd64 ATL_OrAtomicMask_amd64 @\
      ATL_AndAtomicMask_ia32 ATL_OrAtomicMask_ia32 @\
      ATL_AndAtomicMask_arm64 ATL_OrAtomicMask_arm64 
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_asm.h"
/*
 * Applies mask atomically to long stored at vp.
 * RETURNS: 0 if mask did not change value, 1 if it did
 */
@ROUT ATL_AndAtomicMask_arm64 ATL_OrAtomicMask_arm64 
/*
 * w0                          x0                  w1
 * int ATL_@(nop)AtomicMask(void *vp, unsigned char mask)
 */
#define vp  x0
#define msk w1
#define val w2
#define new w3
.global ATL_asmdecor(ATL_@(nop)AtomicMask)
ATL_asmdecor(ATL_@(nop)AtomicMask):
   ATOMIC_LOOP:
      ldxrb val, [vp]       /* exclusive read of val */
      @(op) new, val, msk    /* mask val to produce changed code */
      cmp new, val         /* if (mask didn't change anything) */
      b.eq ZERO_RET        /*    return(0) */
      stxrb w4, new, [vp]   /* store updated val, w4 0 on exclsive success */
   cbnz w4, ATOMIC_LOOP    /* if (w3 != 0) try again */
   mov w0, #1
   ret
ZERO_RET:
   eor w0, w0, w0
   ret
@ROUT ATL_AndAtomicMask_ia32 ATL_OrAtomicMask_ia32
#define vp %esi
#define mask %dl
#define chng %cl
/*
 * eax                          4(esp)              8(%esp)
 * int int ATL_DecAtomicCount(void *vp, unsigned char mask)
 */
#define FSZ 4
.text
.global ATL_asmdecor(ATL_@(nop)AtomicMask)
ATL_asmdecor(ATL_@(nop)AtomicMask):
   sub $FSZ, %esp             /* space to callee-saved regs */
   movl %esi, (%esp)          /* save caller's esi */
   movl FSZ+4(%esp), vp       /* load bitvec address */
   movb FSZ+8(%esp), mask     /* load mask to apply */
   ATOMIC_LOOP:
      movb (%edx), %al        /* read bitvec from memory */
      mov  %al, chng          /* chng = unsigned long from mem */
      @3l@(op) mask, chng          /* apply mask to chng */
      cmp chng, %al           /* if (mask didn't change value) */
      je ZERO_RET             /*   return(0) */
      lock                    /* make cmpxchg atomic */
      cmpxchg chng, (vp)      /* put ecx in mem if mem still == eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */
DONE:
   mov $1, %eax
   ret
ZERO_RET:
   xor %eax, %eax
   ret
@ROUT ATL_AndAtomicMask_amd64 ATL_OrAtomicMask_amd64
#define new %cl
#define vp  %rdi
#define msk %dl
/*
 * rax                          rdi                 rsi
 * int ATL_@(op)AtomicMask(void *vp, unsigned char mask)
 */
.text
.global ATL_asmdecor(ATL_@(nop)AtomicMask)
ATL_asmdecor(ATL_@(nop)AtomicMask):
   mov %si, %dx
   ATOMIC_LOOP:
      movb (%rdi), %al        /* read ul from memory */
      mov  %al, new           /* rcx = unsigned long from mem */
      @3l@(op) msk, new            /* apply mask to rcx */
      cmp new, %al            /* if (mask didn't change value) */
      je ZERO_RET             /*   return(0) */
      lock                    /* make cmpxchg atomic */
      cmpxchg new, (%rdi)     /* put new in mem if mem still == al */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */
DONE:
   mov $1, %rax
   ret
ZERO_RET:
   xor %rax, %rax
   ret
@ROUT ATL_FreeAtomicCount_mut
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_tprim.h"
void ATL_FreeAtomicCount(void *vp)
{
   ATL_lock_destroy(ATL_AddBytesPtr(vp, 2*sizeof(int)+128));
}
@beginskip
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include <stdlib.h>
#ifdef ATL_OMP_THREADS
   #include <omp.h>
#else
   #include <pthread.h>
#endif
#include "atlas_misc.h"
void ATL_FreeAtomicCount(void *vp)
{
   char *cp=vp;

#ifdef ATL_OMP_THREADS
   omp_destroy_lock((omp_lock_t*)(cp+2*sizeof(int)+128));
#else
   ATL_assert(!pthread_mutex_destroy((pthread_mutex_t*)(cp+2*sizeof(int)+128)));
#endif
   free(vp);
}
@endskip
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_tprim.h"
@ROUT ATL_DecAtomicCount_mut
int ATL_DecAtomicCount(void *vp)
@ROUT ATL_ResetAtomicCount_mut
int ATL_ResetAtomicCount(void *vp, int cnt)
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
{
   char *cp=vp;
   void *mp;
   int *cntp;
   int iret;

   cntp = ATL_AddBytesPtr(vp, 128);
   mp = cntp+2;
   ATL_assert(!ATL_lock(mp));
   iret = *cntp;
@ROUT ATL_DecAtomicCount_mut   `   if (iret) (*cntp)--;`
@ROUT ATL_ResetAtomicCount_mut `   *cntp = cnt;`
   ATL_assert(!ATL_unlock(mp));
   return(iret);
}
@ROUT ATL_SetAtomicCount_arch
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_misc.h"

void *ATL_SetAtomicCount(int cnt)
{
   int *ip;

   ip = malloc(260); /* make false sharing unlikely by */
   ATL_assert(ip);   /* putting a 128 byte guard on */
   ip[32] = cnt;     /* both sides of counter */
   return((void*)ip);
}
@ROUT ATL_FreeAtomicCount_arch
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include <stdlib.h>
void ATL_FreeAtomicCount(void *vp)
{
   free(vp);   /* could just do #define ATL_FreeAtomicCount free */
}              /* but do this so compiles same as _mut version */
@ROUT ATL_GetAtomicCount
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
int ATL_GetAtomicCount(void *vp)
{
   volatile int *ip = vp;
   return(ip[32]);
}
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwdate 2014
#include "atlas_asm.h"
/*
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64
 * rax                       rdi         rsi
@ROUT ATL_ResetAtomicCount_win64
 * rax                       rcx         rdx
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
 * int ATL_ResetAtomicCount(void *vp, int cnt)
 * Sets vp's acnt=cnt.
 * RETURNS: acnt before the reset
 */
.text
.global ATL_asmdecor(ATL_ResetAtomicCount)
ATL_asmdecor(ATL_ResetAtomicCount):
@ROUT ATL_ResetAtomicCount_ia32
   @define mm @%edx@
   @define ct @%ecx@
   movl 4(%esp), @(mm)
   movl 8(%esp), @(ct)
@ROUT ATL_ResetAtomicCount_amd64
   @define mm @%rdi@
   @define ct @%esi@
@ROUT ATL_ResetAtomicCount_win64
   @define mm @%rcx@
   @define ct @%edx@
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read acnt from memory */
      lock                    /* make cmpxchg atomic */
      cmpxchg @(ct), (@(mm))   /* put cnt in mem if mem still == acnt in eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */
DONE:
   ret
@ROUT ATL_DecAtomicCount_arm64
#include "atlas_asm.h"
@extract -b @(topd)/cw.inc lang=C -def cwdate 2015 -def contrib "Dave Nuechterlein" -def author "R. Clint Whaley"
/* w0                           x0  */
/* int ATL_DecAtomicCount(void *vp) */
.global ATL_asmdecor(ATL_DecAtomicCount)
ATL_asmdecor(ATL_DecAtomicCount):
/*
 * x0 is aptr to a cnt structure. 128 bytes in, there is an int (acnt)
 * that we want to decrement.
 * IN LOOP: attempt to get exclusive access to acnt, dec it, and return 0
 * if acnt <= 1, and acnt-1 otherwise 
 */
   add  x1,x0,128        /* move to storage loc in memory */
   ATOMIC_LOOP:
      ldxr w0, [x1]      /* exclusive read of cnt into return reg (w0) */
      subs w2, w0, 1     /* dec cnt, set cond codes */
      b.lt   ZERO_RET    /* return 0 if count already below 1 */
      stxr w3, w2, [x1]  /* store decremented val, w3 0 on exclusive success */
   cbnz w3, ATOMIC_LOOP  /* if (w3 != 0) try again */

@skip   sxtw x0,w1
   ret

ZERO_RET:
@skip   eor x0,x0,x0
   eor w0, w0, w0
   ret

@ROUT ATL_ResetAtomicCount_arm64
#include "atlas_asm.h"
@extract -b @(topd)/cw.inc lang=C -def cwdate 2015 -def contrib "Dave Nuechterlein" -def author "R. Clint Whaley"
/*
 * w0                        x1          w2 
 * int ATL_ResetAtomicCount(void *vp, int cnt)
 * Sets vp's acnt=cnt.
 * RETURNS: acnt before the reset
 */
ATL_asmdecor(ATL_ResetAtomicCount):
   add x1, x0, 128
   ATOMIC_LOOP:
      ldxr w0, [x1]         /* load exlusive into ret reg */
      stxr w2, w0, [x1]     /* w2 == 0 on exclusive success */
   cbnz w2, ATOMIC_LOOP     /* if (w2 != 0) failed; try again */
@skip   mov w0, w1
@skip   sxtw x0,w1
   ret
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
#include "atlas_asm.h"
/* rax                  %rdi/rcx/4  */
/* int ATL_DecAtomicCount(void *vp) */
.text
.global ATL_asmdecor(ATL_DecAtomicCount)
ATL_asmdecor(ATL_DecAtomicCount):
@ROUT ATL_DecAtomicCount_ia32
   movl 4(%esp), %edx
   @define mm @%edx@
@ROUT ATL_DecAtomicCount_amd64 
   @define mm @%rdi@
@ROUT ATL_DecAtomicCount_win64 
   @define mm @%rdx@
   movq %rcx, %rdx
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read cnt from memory */
      movl %eax, %ecx         /* ecx = cnt */
      subl $1, %ecx           /* ecx = cnt-1 */
      jl ZERO_RET             /* return 0 if count already below 1 */
      lock                    /* make cmpxchg atomic */
      cmpxchg %ecx, (@(mm))    /* put cnt-1 in mem if mem still == cnt in eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */

ZERO_RET:
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_win64 `   xor %rax, %rax`
@ROUT ATL_DecAtomicCount_ia32  `   xor %eax, %eax`
@skip   movl %eax, (@(mm))  /* safety to ensure no roll from neg back to pos */
DONE:
   ret
@ROUT ATL_ResetAtomicCount_ppc
   @define op @Reset@
@ROUT ATL_DecAtomicCount_ppc
   @define op @Dec@
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
#include "atlas_asm.h"
.text
#ifdef ATL_AS_OSX_PPC
   .globl _ATL_@(op)AtomicCount
   _ATL_@(op)AtomicCount:
#else
   #if defined(ATL_USE64BITS) &&  _CALL_ELF != 2
/*
 *      Official Program Descripter section, seg fault w/o it on Linux/PPC64
 */
        .section        ".opd","aw"
        .align 2
	.globl  ATL_USERMM
        .align  3
ATL_@(op)AtomicCount:
        .quad   Mjoin(.,ATL_@(op)AtomicCount),.TOC.@tocbase,0
        .previous
        .type   Mjoin(.,ATL_@(op)AtomicCount),@function
        .text
	.globl  Mjoin(.,ATL_@(op)AtomicCount)
.ATL_@(op)AtomicCount:
   #else
	.globl  ATL_@(op)AtomicCount
ATL_@(op)AtomicCount:
   #endif
#endif
@ROUT ATL_ResetAtomicCount_ppc
/* r3                                 r3       r4 */
/* int int ATL_ResetAtomicCount(void *vp, int cnt) */
@ROUT ATL_DecAtomicCount_ppc
#error "Code is not reliable on PPC, don't know why"
/* r3                           r3  */
/* int ATL_DecAtomicCount(void *vp) */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
RETRY:
   lwarx r5, 0, r3    /* Read int from mem, place reservation */
@ROUT ATL_DecAtomicCount_ppc
   addi  r5, r5, -1   /* decrement value */
   stwcx. r5, 0, r3   /* attempt to store decremented value back to mem */
@ROUT ATL_ResetAtomicCount_ppc
   stwcx. r4, 0, r3   /* attempt to store new value back to mem */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
   bne-  RETRY        /* If store failed, retry */
   mr r3, r5
   blr
@ROUT ATL_DecAtomicCount_sparc ATL_ResetAtomicCount_sparc @\
      ATL_DecAtomicCount_mips ATL_ResetAtomicCount_mips
#error "not implemented"
@ROUT ATL_thread_launch
#include "atlas_misc.h"
#include "atlas_threads.h"

/*
 * These redefinitions allow us to try various launch structures
 */
#ifdef ATL_TUNE_LIN
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_lin0tlaunch_noaff
      #define ATL_thread_launch ATL_tllin_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_lin0tlaunch
      #define ATL_thread_launch ATL_tllin
   #endif
#elif defined(ATL_TUNE_LG2)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_log2tlaunch_noaff
      #define ATL_thread_launch ATL_tllg2_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_log2tlaunch
      #define ATL_thread_launch ATL_tllg2
   #endif
#elif defined(ATL_TUNE_DYN)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_dyntlaunch_noaff
      #define ATL_thread_launch ATL_tldyn_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_dyntlaunch
      #define ATL_thread_launch ATL_tldyn
   #endif
#endif
void *ATL_tlaunch(void *vp); /* _noaff versions not protoed in threads.h */

/*
 * This routine can be called by any threaded routine to autoset all needed
 * data structures for ATLAS to launch the threads for a parallel operation
 */
void ATL_thread_launch
(
   void *opstruct,              /* P-len struct given to each launched thread */
   int opstructstride,          /* sizeof(opstruct) */
   void *OpStructIsInit,        /* NULL, or test to see if thread is spawned */
   void *DoWork,                /* computation to call (rout launched) */
   void *CombineOpStructs       /* NULL, or combine func */
)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   void *vp;
   int i;

   ls.opstruct = opstruct;
   ls.opstructstride = opstructstride;
   ls.CombineOpStructs = CombineOpStructs;
   ls.OpStructIsInit = OpStructIsInit;
   ls.DoWork = DoWork;
   ls.rank2thr = tp;
   #ifdef ATL_TUNE_DYN
      ls.acounts = &vp;
      ls.acounts[0] = ATL_SetGlobalAtomicCount(ATL_NTHREADS>>1, 
                                               ATL_NTHREADS-1, 0);
      ls.chkin = calloc(ATL_NTHREADS, sizeof(int));
      ATL_assert(ls.chkin);
   #endif

   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   #ifdef ATL_TUNE_DYN
       ATL_FreeGlobalAtomicCount(ls.acounts[0]);
       free((void*)ls.chkin);
   #endif
}
@ROUT ATL_launch_threads
#include "atlas_misc.h"
#include "atlas_threads.h"

void *ATL_tDoStart(void *vp)
{
   ATL_thread_t *tp = vp;
   void (*DoWork)(void *);
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
      {
          tp->affID = 1 - tp->affID;
          ATL_assert(!ATL_setmyaffinity(tp->affID));
      }
   #endif
   DoWork = tp->vp;
   DoWork(tp);
}

void *ATL_tDoSpawn0(void *vp)
{
   ATL_thread_t *tp = vp;
   static int P = tp->P;
   int i;

   for (i=1; i < P; i++)
   {
      ATL_assert(!ATL_thread_start(tp+i, i, 0, ATL_tDoStart, tp+i);
   }
}
ATL_thread_t *ATL_launch_threads(int P, void *vp)
/*
 * Start P threads (with affinity if called for).  vp should be a void-return
 * function pointer that takes the ATL_thread_t of the starting thread
 * as its only argument (as a void*).  This means the function takes only
 * the thread info as arguments, including P, rank, affID.
 * RETURNS: P-length array of started threads
 */
{
   ATL_thread_t *tp;
   int i;

   tp = malloc(p*sizeof(ATL_thread_t));
   ATL_assert(tp);
   for (i=0; i < P; i++)
   {
      tp[i].rank = i;
      tp[i].P = P;
      tp[i].affID = -1;
      tp[i].vp = vp;
   }
/*
 * If the master process has already been tied to core 0, just spawn threads
 * to other cores, then call thread work function myself
 */
   #ifdef ATL_TP_ACTIVE_MASTER
      for (i-1; i < P; i++)
      {
         ATL_assert(!ATL_thread_start(tp+i, i, 0, ATL_tDoStart, tp+i);
      }
      ATL_tDoStart(tp);
   #else
      ATL_assert(!ATL_thread_start(tp, i, 0, ATL_tDoSpawn0, tp);
   #endif
   return(tp);
}
@ROUT ATL_thread_start
#define Mstr2(m) # m
#define Mstr(m) Mstr2(m)
void ATL_xerbla(int p, char *rout, char *form, ...);
#define ATL_assert(n_) \
{ \
   if (!(n_)) \
   { \
      ATL_xerbla(0, __FILE__, "assertion %s failed, line %d of file %s\n", \
                 Mstr(n_), __LINE__, __FILE__); \
   } \
}
#include <stdlib.h>
#ifndef ATL_NOAFFINITY
#include "atlas_taffinity.h"
#endif
#ifdef ATL_WINTHREADS
   #include <windows.h>
#endif
#if defined(ATL_NOAFFINITY)
   #ifndef ATL_WINTHREADS
      #include <pthread.h>
   #endif
   #ifdef ATL_TUNING
      #define ATL_thread_start ATL_thread_start_noaff
   #endif
#elif defined(ATL_PAFF_SETAFFNP)
   #define _GNU_SOURCE 1 /* what manpage says you need to get CPU_SET */
   #define __USE_GNU   1 /* what actually works on linuxes I've seen */
   #include <sched.h>    /* must be inced wt above defs before pthread.h */
   #include <pthread.h>
#elif defined(ATL_PAFF_SETPROCNP)
   #include <pthread.h>
#else
   #define ATL_PAFF_SELF
#endif
@skip #include "atlas_misc.h"
@skip #include "atlas_threads.h"
#include "atlas_tprim.h"
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE,
                     void *(*rout)(void*), void *arg)
/*
 * Creates a thread that will run only on processor proc.
 * RETURNS: 0 on success, non-zero on error
 * NOTE: present implementation dies on error, so 0 is always returned.
 */
{
#ifdef ATL_WINTHREADS
   #ifdef ATL_WIN32THREADS
      DWORD thrID;
   #else
      unsigned thrID;
   #endif

   #ifdef ATL_NOAFFINITY
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, 0, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 0, &thrID);
      #endif
      ATL_assert(thr->thrH);
   #else
      thr->rank = proc;
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, CREATE_SUSPENDED, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg,
                                            CREATE_SUSPENDED, &thrID);
      #endif
      ATL_assert(thr->thrH);
      #ifdef ATL_RANK_IS_PROCESSORID
         ATL_assert(SetThreadAffinityMask(thr->thrH, ((long long)1)<<proc)));
         thr->affID = proc;
      #else
         thr->affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
         ATL_assert(SetThreadAffinityMask(thr->thrH,
                    (((long long)1)<<(thr->affID))));
      #endif
      ATL_assert(ResumeThread(thr->thrH) == 1);
   #endif
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Should not call thread_start when using OpenMP!");
   ATL_assert(0);
#elif 0 && defined(ATL_OS_OSX)  /* unchecked special OSX code */
/* http://developer.apple.com/library/mac/#releasenotes/Performance/RN-AffinityAPI/_index.html */
   pthread_attr_t attr;
   #define ATL_OSX_AFF_SETS 2       /* should be probed for */
   thread_affinity_policy ap;

   ap.affinity_tag = proc % ATL_OSX_AFF_SETS;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE));
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */

   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   ATL_assert(!thread_policy_set(thr->thrH, THREAD_AFFINITY_POLICY,
                                 (integer_t*)&ap,
                                 THREAD_AFFINITY_POLICY_COUNT));
   ATL_assert(!pthread_attr_destroy(&attr));
#else
   pthread_attr_t attr;
   #ifndef ATL_NOAFFINITY
      #if defined(ATL_PAFF_SETAFFNP)
         cpu_set_t *cpuset=NULL;
      #elif defined(ATL_PAFF_PLPA)
         plpa_cpu_set_t cpuset;
      #elif defined(ATL_PAFF_CPUSET) /* untried FreeBSD code */
         cpuset_t mycpuset;
      #endif
      #ifdef ATL_RANK_IS_PROCESSORID
         const int affID = proc;
      #else
         const int affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
      #endif
      #ifdef ATL_PAFF_SELF
         thr->affID = -affID-1; /* affinity must be set by created thread */
      #endif
   #endif
   thr->rank = proc;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
   {
      #ifdef IBM_PT_ERROR
         ATL_assert(!pthread_attr_setdetachstate(&attr,
                                                 PTHREAD_CREATE_UNDETACHED));
      #else
         ATL_assert(!pthread_attr_setdetachstate(&attr,
                                                 PTHREAD_CREATE_JOINABLE));
      #endif
   }
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   #ifdef ATL_PAFF_SETAFFNP
/*
 *    On PowerPC/Linux, pthread_attr_setaffinity_np sometimes reallocs() the
 *    cpuset variable, thus it must be malloced and not taken from stack!
 */
      cpuset = malloc(sizeof(cpu_set_t));
      CPU_ZERO(cpuset);
      CPU_SET(affID, cpuset);
      ATL_assert(!pthread_attr_setaffinity_np(&attr, sizeof(cpu_set_t),cpuset));
      free(cpuset);
   #elif defined(ATL_PAFF_SETPROCNP)
      ATL_assert(!pthread_attr_setprocessor_np(&attr, (pthread_spu_t)affID,
                                               PTHREAD_BIND_FORCED_NP));
   #endif
   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   ATL_assert(!pthread_attr_destroy(&attr));
#endif
   return(0);
}
@ROUT ATL_thread_start00
#ifndef ATL_NOAFFINITY
   #include "atlas_taffinity.h"  /* include this file first! */
#elif defined(ATL_TUNING)
   #define ATL_thread_start ATL_thread_start_noaff
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE,
                     void *(*rout)(void*), void *arg)
/*
 * Creates a thread that will run only on processor proc.
 * RETURNS: 0 on success, non-zero on error
 * NOTE: present implementation dies on error, so 0 is always returned.
 */
{
#ifdef ATL_WINTHREADS
   #ifdef ATL_WIN32THREADS
      DWORD thrID;
   #else
      unsigned thrID;
   #endif
      
   #ifdef ATL_NOAFFINITY
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, 0, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 0, &thrID);
      #endif
      ATL_assert(thr->thrH);
   #else
      thr->rank = proc;
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, CREATE_SUSPENDED, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 
                                            CREATE_SUSPENDED, &thrID);
      #endif
      ATL_assert(thr->thrH);
      #ifdef ATL_RANK_IS_PROCESSORID
         ATL_assert(SetThreadAffinityMask(thr->thrH, ((long long)1)<<proc)));
         thr->affID = proc;
      #else
         thr->affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
         ATL_assert(SetThreadAffinityMask(thr->thrH, 
                    (((long long)1)<<(thr->affID))));
      #endif
      ATL_assert(ResumeThread(thr->thrH) == 1);
   #endif
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Should not call thread_start when using OpenMP!");
   ATL_assert(0);
#elif 0 && defined(ATL_OS_OSX)  /* unchecked special OSX code */
/* http://developer.apple.com/library/mac/#releasenotes/Performance/RN-AffinityAPI/_index.html */
   pthread_attr_t attr;
   #define ATL_OSX_AFF_SETS 2       /* should be probed for */
   thread_affinity_policy ap;

   ap.affinity_tag = proc % ATL_OSX_AFF_SETS;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE));
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */

   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   ATL_assert(!thread_policy_set(thr->thrH, THREAD_AFFINITY_POLICY, 
                                 (integer_t*)&ap, 
                                 THREAD_AFFINITY_POLICY_COUNT));
   ATL_assert(!pthread_attr_destroy(&attr));
#else
   pthread_attr_t attr;
   #ifndef ATL_NOAFFINITY
      #if defined(ATL_PAFF_SETAFFNP) || defined(ATL_PAFF_SCHED)
         cpu_set_t cpuset;
      #elif defined(ATL_PAFF_PLPA)
         plpa_cpu_set_t cpuset;
      #elif defined(ATL_PAFF_CPUSET) /* untried FreeBSD code */
         cpuset_t mycpuset;
      #endif
      #ifdef ATL_RANK_IS_PROCESSORID
         const int affID = proc;
      #else
         const int affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
      #endif
      #ifdef ATL_PAFF_SELF
         thr->affID = -affID-1; /* affinity must be set by created thread */
      #endif
   #endif
   thr->rank = proc;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
   {
      #ifdef IBM_PT_ERROR
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_UNDETACHED));
      #else
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_JOINABLE));
      #endif
   }
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   #ifdef ATL_PAFF_SETAFFNP
      CPU_ZERO(&cpuset);
      CPU_SET(affID, &cpuset);
      ATL_assert(!pthread_attr_setaffinity_np(&attr, sizeof(cpuset), &cpuset));
   #elif defined(ATL_PAFF_SETPROCNP)
      ATL_assert(!pthread_attr_setprocessor_np(&attr, (pthread_spu_t)affID, 
                                               PTHREAD_BIND_FORCED_NP)); 
   #endif
   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   #if defined(ATL_PAFF_PBIND)
      ATL_assert(!processor_bind(P_LWPID, thr->thrH, affID, NULL));
      thr->affID = affID;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_BINDP) && !defined(ATL_OS_AIX)
      ATL_assert(!bindprocessor(BINDTHREAD, thr->thrH, affID));
      thr->affID = affID;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_CPUSET)  /* untried FreeBSD code */
      CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
      CPU_SET(affID, &mycpuset);
      if (!cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, thr->thrH,
                              sizeof(mycpuset), &mycpuset));
         thr->affID = affID;  /* affinity set by spawner */
   #endif
   ATL_assert(!pthread_attr_destroy(&attr));
#endif
   return(0);
}
@ROUT ATL_thread_join
@skip #include "atlas_misc.h"
@skip #include "atlas_threads.h"
#include "atlas_tprim.h"
int ATL_thread_join(ATL_thread_t *thr)   /* waits on completion of thread */
{
#ifdef ATL_WINTHREADS
   ATL_assert(WaitForSingleObject(thr->thrH, INFINITE) != WAIT_FAILED);
   ATL_assert(CloseHandle(thr->thrH));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_join using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   ATL_assert(!pthread_join(thr->thrH, NULL));
#endif
   return(0);
}
@ROUT ATL_thread_exit
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_thread_exit(void *retval)
{
#ifdef ATL_WINTHREADS
   ExitThread((DWORD)(retval));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_exit using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   pthread_exit(retval);
#endif
}
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_setmyaffinity
#include "atlas_taffinity.h"  /* include this file first! */
/*
 * Cases handled before thread creation / start running:
 *    ATL_WINTHREADS, ATL_PAFF_SETAFFNP, ATL_PAFF_SETPROCNP
 * Cases for post-creation binding done by spawner:
 *    ATL_PAFF_PBIND, ATL_PAFF_BINDP, ATL_PAFF_CPUSET
 * all of these cases provide self-setting as well.
 * Cases that work only for self-setting:
 *   ATL_PAFF_SCHED, ATL_PAFF_PLPA
 */

#ifdef ATL_WINTHREADS
   #include <windows.h>
#elif defined(ATL_NOAFFINITY)
   #ifndef ATL_OMP_THREADS
      #include <pthread.h>
   #endif
#elif defined(ATL_SPAFF_PLPA)
   #include <pthread.h>
   #include <plpa.h>
#elif defined(ATL_SPAFF_PBIND)
   #include <pthread.h>
   #include <sys/types.h>
   #include <sys/processor.h>
   #include <sys/procset.h>
#elif defined(ATL_SPAFF_SCHED)
   #define _GNU_SOURCE 1 /* what manpage says you need to get CPU_SET */
   #define __USE_GNU   1 /* what works on linuxes that I've seen */
   #include <sched.h>    /* must include this before pthreads */
   #include <pthread.h>
#elif defined(ATL_SPAFF_RUNON)
   #include <pthread.h>
#elif defined(ATL_SPAFF_BINDP)
   #include <pthread.h>
   #include <sys/thread.h>      /* thread_self header */
   #include <sys/processor.h>   /* bindprocessor header */
#elif defined(ATL_SPAFF_CPUSET)
   #include <pthread.h>
   #include <sys/param.h>
   #include <sys/cpuset.h>
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_setmyaffinity(const int rank)
/*
 * Attempts to sets the affinity of an already-running thread.  The
 * aff_set flag is set to true whether we succeed or not (no point in
 * trying multiple times).
 * RETURNS: 0 on success, non-zero error code on error
 */
{
   #if !defined(ATL_AFF_NUMID)
      const int bindID=rank;
   #elif defined(ATL_RANK_IS_PROCESSORID)
      const int bindID = rank % ATL_AFF_NUMID;
   #else
      const int bindID = ATL_affinityIDs[rank%ATL_AFF_NUMID];
   #endif
#ifdef ATL_WINTHREADS  /* Windows */
      ATL_assert(SetThreadAffinityMask(GetCurrentThreadId(),
                                       (((long long)1)<<bindID)));
#elif defined(ATL_SPAFF_PLPA)  /* affinity wrapper package */
   plpa_cpu_set_t cpuset;
   PLPA_CPU_ZERO(&cpuset);
   PLPA_CPU_SET(bindID, &cpuset);
   return(plpa_sched_setaffinity((pid_t)0, sizeof(cpuset), &cpuset));
#elif defined(ATL_SPAFF_PBIND)
   return(processor_bind(P_LWPID, P_MYID, bindID, NULL));
#elif defined(ATL_SPAFF_SCHED)  /* linux */
   cpu_set_t cpuset;
   CPU_ZERO(&cpuset);
   CPU_SET(bindID, &cpuset);
   return(sched_setaffinity(0, sizeof(cpuset), &cpuset));
#elif defined (ATL_SPAFF_RUNON)  /* IRIX: this will not work for process! */
   return(pthread_setrunon_np(bindID));
#elif defined(ATL_SPAFF_BINDP)  /* AIX */
   return(bindprocessor(BINDTHREAD, thread_self(), bindID));
#elif defined(ATL_SPAFF_CPUSET)  /* untried FreeBSD code */
   cpuset_t mycpuset;
   CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
   CPU_SET(bindID, &mycpuset);
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, -1,
                             sizeof(mycpuset), &mycpuset));
#elif defined(ATL_NOAFFINITY)
   return(0);   /* no-affinity override says not setting is not error */
#else
   return(1);  /* Don't know how to set affinity, return error */
#endif
   return(0);
}
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch
void *ATL_tDoWorkWrap(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp = tp->vp;
   lp->DoWork(lp, tp);
   return(NULL);
}

#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   #define ATL_lin0tlaunch ATL_lin0tlaunch_noaff
   #define ATL_thread_start ATL_thread_start_noaff
#endif
void *ATL_lin0tlaunch(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int P = tp->P;
   int i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
      {
         tp->affID = 1 - tp->affID;
         ATL_setmyaffinity(tp->affID);
      }
@skip      if (tp->affID < 0)
@skip          ATL_setmyaffinity(tp);
   #endif
/*
 * Spawn DoWork to all nodes
 */
   lp = tp->vp;
   for (i=1; i < P; i++)
   {
      ATL_thread_start(tp+i, i, 1, ATL_tDoWorkWrap, tp+i);
   }
/*
 * Thread 0 must also do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Await completion of each task, and do combine (linear!) if requested
 */
   for (i=1; i < P; i++)
   {
      ATL_thread_join(tp+i);
      if (lp->DoComb)  /* do combine if necessary */
         lp->DoComb(lp->opstruct, 0, i);
   }
   return(NULL);
}
@ROUT ATL_dyntlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_dyntlaunch_noaff(void *vp)
#else
void *ATL_dyntlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int iam = tp->rank, P = tp->P;
   int i, src, dest, nthrP2, mask, abit;
   void *acnt;

   lp = tp->vp;
   acnt = lp->acounts[0];
   btp = tp - iam;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
      {
         tp->affID = 1 - tp->affID;
         ATL_setmyaffinity(tp->affID);
      }
@skip          ATL_setmyaffinity(tp);
   #endif
   dest = ATL_DecGlobalAtomicCount(acnt, iam);
   while(dest)
   {
      dest = tp->P - dest;
      ATL_thread_start(btp+dest, dest, 0, ATL_dyntlaunch, btp+dest);
      dest = ATL_DecGlobalAtomicCount(acnt, iam);
   }
/*
 * Do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Do combine in minimum spanning tree, combining results as required
 */
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               while (lp->chkin[src] != ATL_CHK_DONE_OP)
                  ATL_POLL;
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
         {
            lp->chkin[iam] = ATL_CHK_DONE_OP;
            ATL_thread_exit(NULL);
         }
      }
      mask |= abit;
   }
@beginskip
/* 
 * This code does not work, because combine routs assume combine is always
 * right-to-left
 */
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 * Later on, replace this code with pair-wise summup using AtomicCtrs=2
 */
   if (iam == 0)
   {
      int ndone=1;
      while (ndone < P)
      {
/*
 *       Find an uncombined completed result
 */
         for (i=1; i < P && lp->chkin[i] != ATL_CHK_DONE_OP; i++);
         if (i == P)
         {
            ATL_POLL;
            continue;
         }
/*
 *       Do the combine if necessary, and mark that thread as completely done
 */
         if (lp->DoComb)  /* do combine if necessary */
            lp->DoComb(lp->opstruct, iam, i);
         ndone++;
         lp->chkin[i] = ATL_CHK_DONE_COMB;
      }
   }
@endskip
   return(NULL);
}
@ROUT ATL_ranktlaunch_noaff
static void *ATL_GoToWork(ATL_thread_t *tp, ATL_LAUNCHSTRUCT_t *lp, int iam)
{
/*
 * Do the operation
 */
   lp->DoWork(lp, lp->opstruct+lp->opstructstride*iam);
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 */
   if (iam == 0)
   {
      for (i=1; i < ATL_NTHREADS; i++)
      {
         if (!lp->OpStructIsInit || 
             lp->OpStructIsInit(lp->opstruct+i*lp->opstructstride))
         {
            ATL_thread_join(tp+i);
            if (lp->CombineOpStructs)  /* do combine if necessary */
               lp->CombineOpStructs(lp->opstruct,
                                    lp->opstruct+lp->opstructstride*i);
         }
      }
   }
   return(NULL);
}
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
    :"%rax", "%rbx"
    );
  
  return(myRetn);
} // end *** ATL_coreId ***
static void ATL_CreateThread(void *vp)
{
   pthread_attr_t attr;
   pthread_t pt;
   void *ATL_ranktlaunch_noaff(void *vp);

   ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   ATL_assert(!pthread_create(&pt, &attr, ATL_ranktlaunch_noaff, vp));
}

static void *ATL_KeepLaunching(ATL_thread_t tp, ATL_LAUNCHSTRUCT_t *lp)
{
   void *ranklock = lp->acounts[0];
   int i;
   do
   {
/*
 *    If all cores have gotten a thread, or if any thread has finished the
 *    problem, then stop launching and exit
 */
      if (lp->acounts[1])
         return(NULL);
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (lp->impdone[i] == -1)
            return(NULL);
      }
      iam = ATL_coreID();      /* may have changed if I've slept */
      if (!lp->rank2thr[iam])
      {
         int SUCCESS=0;
         ATL_mutex_lock(ranklock);
         if (!lp->rank2thr[iam])
         {
            int n;
            tp->rank = iam;
            lp->rank2thr[iam].thrH = pthread_self();
            for (i=0; i < ATL_NTHREADS && !lp->rank2thr[i].thrH; i++);
            if (i == ATL_NTHREADS)         /* if threads on all cores */
               lp->acounts[1] = (void*)1;  /* set flag saying launch complete */
            SUCCESS = 1;
         }
         ATL_mutex_unlock(ranklock);
         if (SUCCESS)
            return(ATL_GoToWork(tp, lp, iam));
      }
/*
 *    If there are still threads that need to run, spawn a new one, and go
 *    to sleep to yield to computation
 */
      ATL_CreateThread(tp);
      sched_yield();  /* go to sleep if no success */
   }
   while(1);
   return(NULL);
}
/*
 * This routine is for launching on platforms w/o affinity that have some
 * way for a running process to establish what core they are on.  Threads
 * are continually launched until there is one on each core, or the first
 * core runs out of work, whichever comes first.
 * Any process that is launched with this technique must not barrier,
 * since the number of cores is unknown, and the will certainly enter
 * the program at very different times.
 * NOTE: This routine calls pthreads directly, because ATLAS supports only
 *       Windows threads and pthreads, and Windows threads have affinity.
 */
void *ATL_ranktlaunch_noaff(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp=tp->vp;

   return(ATL_KeepLaunching(vp, lp));

@ROUT ATL_log2tlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_log2tlaunch_noaff(void *vp)
#else
void *ATL_log2tlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   int i, iam, abit, mask, src, dest, nthrP2;
   const int P=tp->P;

   iam = tp->rank;
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
      {
         tp->affID = 1 - tp->affID;
         ATL_setmyaffinity(tp->affID);
      }
   #endif
   btp = tp - iam;
   lp = tp->vp;
   mask = (1<<nthrP2) - 1;   /* no threads are in at beginning */
/*
 * Take log_2(NTHR) steps to do log_2 launch 
 */
   for (i=nthrP2-1; i >= 0; i--)
   {
      abit = (1<<i);
      mask ^= abit;   /* double the # of threads participating */
      if (!(iam & mask))
      {
         if (!(iam & abit))
         {
            dest = iam ^ abit;
            if ( dest < P)
               ATL_thread_start(btp+dest, dest, 1, ATL_log2tlaunch, btp+dest);
         }
      }
   }
   lp->DoWork(lp, tp);   /* do the operation */
/*
 * Join tree back up, combining results as required
 */
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               ATL_thread_join(btp+src);
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
            ATL_thread_exit(NULL);
      }
      mask |= abit;
   }
   return(NULL);
}
@ROUT ATL_goparallel
@beginskip
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   void *ATL_log2tlaunch_noaff(void *vp);
   #define ATL_goparallel ATL_goparallel_noaff
   #define ATL_dyntlaunch ATL_log2tlaunch_noaff
   #define ATL_USE_DYNAMIC 0
#elif defined(ATL_TUNING)
   #if defined(ATL_LAUNCH_LINEAR)
      #define ATL_goparallel ATL_goparallel_lin
      #define ATL_dyntlaunch ATL_lin0tlaunch
      #define ATL_USE_DYNAMIC 0
   #elif defined(ATL_LAUNCH_DYNAMIC)
      #define ATL_goparallel ATL_goparallel_dyn
      #define ATL_USE_DYNAMIC 1
   #else
      #ifdef ATL_LAUNCH_LOG2
         #define ATL_goparallel ATL_goparallel_log2
      #endif
      #define ATL_dyntlaunch ATL_log2tlaunch
      #define ATL_USE_DYNAMIC 0
   #endif
#else
   #define ATL_USE_DYNAMIC 1
#endif
@endskip
void ATL_goparallel
/*
 * This function is used when you pass a single opstruct to all threads;  
 * In this case, we stash opstruct in launchstruct's vp, and then use the
 * rank array as opstruct during the spawn.  Therefore, these routines
 * should expect to get their problem def from ls.vp, and their rank from
 * the second argument.  The DoWork function is the function that should
 * be called from each thread to do the parallel work.  This function should
 * look like:
 * void DoWork_example(ATL_LAUNCHSTRUCT_t *lp, void *vp)
 * {
 *    ATL_thread_t *tp = vp;
 *    const int myrank = tp->rank;
 *    my_prob_def_t *pd = lp->opstruct;
 *    ... do work based on info in struct pointed to by lp->opstruct ...
 * }
 * Your DoWork should perform any needed combine before finishing execution,
 * and any return values can be passed in the problem definition structure
 * that you define.
 */
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* structure giving tasks to threads */
   void *DoComb          /* function to combine two opstructs */
)
{
   ATL_thread_t *tp;
   int *chkin;
   void *vp, *lc;
   int i;
   ATL_LAUNCHSTRUCT_t ls;

   ls.OpStructIsInit = NULL;
   ls.DoWork = DoWork;
   ls.DoComb = DoComb;
   ls.opstruct = opstruct;
   if (DoComb)
      ATL_goParallel(P, ATL_oldjobwrap, ATL_oldcombwrap, &ls, NULL);
   else
      ATL_goParallel(P, ATL_oldjobwrap, NULL, &ls, NULL);
}
@ROUT ATL_goparallel_prank
#include "atlas_misc.h"
#include "atlas_threads.h"
#if defined(ATL_GAS_x8664) || defined(ATL_GAS_x8632)
   #define ATL_HAS_COREID
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
#if defined(ATL_GAS_x8632)
    :"%eax", "%ebx", "%edx", "%ecx"
#elif defined(ATL_GAS_x8664)
    :"%rax", "%rbx", "%rdx", "%rcx"
#endif
    );
  return(myRetn);
}
#else
   #define ATL_HAS_COREID
   #define _GNU_SOURCE 1
   #define __USE_GNU   1
   #include <sched.h>
   #define ATL_coreID sched_getcpu
#endif
#ifndef ATL_HAS_COREID
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   fprintf(stderr, "Hey chief, you are screwed:\n");
   fprintf(stderr, 
      "  someone called goparallel_ptrank wt no way to determine prank!\n");
   exit(-1);
}
#else

typedef struct
{
   volatile int *coreIDs;   /* NTHR-len array providing non-unique coreIDs */
   volatile int *thrrnks;   /* NP-len array of chosen thread ranks */
   void *Tcnt;              /* atomic counter of # of threads launched */
   void *Trankcnt;          /* atomic ctr providing thread rank */
   int NT, NP;              /* # of threads & processors */
   int NLC;                 /* # of local cntrs in global acnts Tcnt/Trankcnt */
   pthread_attr_t attr;     /* attribute for pthread_create */
   ATL_LAUNCHSTRUCT_t *lp;  /* to pretend we've been launched normally */
} ATL_ranklaunch_t;


/*
 * Selects an ID from list A which does not appear in C
 * RETURNS: first unique ID, or -1 if no such ID found
 */
static int GetUniqueID
(
   int Na,   /* number of accepted unique IDs in U */
   int *A,   /* list of accepted unique IDs found so far */
   int Nc,   /* number of candidates left */
   int *C    /* non-unique candidates */
)
{
   int ic, ia;
   for (ic=0; ic < Nc; ic++)
   {
      for (ia=0; ia < Na && A[ia] != C[ic]; ia++);
      if (ia == Na)  /* found unique one */
         return(ic);
   }
   return(-1);
}

void *ATL_DoRankLaunch(void *vp)
{
   ATL_ranklaunch_t *rl = vp;
   ATL_LAUNCHSTRUCT_t *lp = rl->lp;
   const int T = rl->NT, P = rl->NP;
   int trank;   /* thread rank between 0 and NT-1 */
   int prank;   /* processor rank between 0 and NP-1 */
   int i, coreID;
   pthread_t pt;
/*
 * Cooperate with master to launch NT threads
 */
   coreID = ATL_coreID();
   trank = coreID % rl->NLC;
   #ifdef ATL_GLOBAL
   while(ATL_DecGlobalAtomicCount(rl->Tcnt, trank))
   #else
   while(ATL_DecAtomicCount(rl->Tcnt))
   #endif
      pthread_create(&pt, &rl->attr, ATL_DoRankLaunch, rl);
/*
 * Get my coreID, and tell master I'm alive by writing it to coreID array
 */
   #ifdef ATL_GLOBAL
      trank = T - ATL_DecGlobalAtomicCount(rl->Trankcnt, trank);
   #else
      trank = T - ATL_DecAtomicCount(rl->Trankcnt);
   #endif
   rl->coreIDs[trank] = coreID;
/*
 * Wait on master to signal ranking have been established
 */
   while(rl->coreIDs[0] == -1)
      ATL_thread_yield();
/* 
 * See if my core has been selected for survival
 */
   for (i=0; i < P; i++)
      if (rl->thrrnks[i] == trank)
         break;
/*
 * If I'm not in worker list, signal completion by writing -2 to coreID array,
 * and quit
 */
   if (i == P)
   {
      rl->coreIDs[trank] = -2;  /* signal thread has completed */
      pthread_exit(NULL);
   }
/* 
 * i is actually now my processor rank, fill my thread info in
 */
   prank = i;
/*   lp->rank2thr[prank].thrH = pthread_self(); */ /* don't need this */
   lp->rank2thr[prank].rank = prank;

   lp->DoWork(lp, lp->rank2thr+prank);  /* do work */

   rl->coreIDs[trank] = -2 ;    /* signal thread completion for master */
}

/*
 * This function is used when we don't have affinity, but do have some way
 * to determine the coreID, which must be a unique non-negative int.
 * It will launch 4*P threads in a detached state; all those threads that
 * start on unique cores will work on the problem, as will some on non-unique
 * cores that are necessary to get P threads out of the 4P created
 */
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   ATL_LAUNCHSTRUCT_t ls;
   ATL_ranklaunch_t rl;
   int T, t, i, j, prank, nunique, coreID;
   int *uids;    /* unique coreIDs */
   volatile int *coreIDs;
   pthread_t pt;
   pthread_attr_t *attr;
   void *vp;

   attr = &rl.attr;
   ls.DoWork = DoWork;
   ls.vp = opstruct;
   ls.DoComb = NULL;
@skip   ls.CombineOpStructs = NULL;
@skip   ls.opstructstride = 0;
   ls.chkin = NULL;
   ls.acounts = NULL;
   T = (P >= 8) ? P<<2 : P+P;
   rl.NT = T;
   rl.NP = P;
   #ifdef ATL_GLOBAL
      rl.NLC = P;
      rl.Tcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
      rl.Trankcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
   #else
      rl.NLC = T >> 2;
      rl.Tcnt = ATL_SetAtomicCount(T-1);
      rl.Trankcnt = ATL_SetAtomicCount(T-1);
   #endif
   rl.lp = &ls;
   uids = malloc(ATL_Cachelen+sizeof(int)*(T+P+P)+sizeof(ATL_thread_t)*P);
   ATL_assert(uids);
   coreIDs = rl.coreIDs = (volatile int*) (uids + P);
   rl.thrrnks = (volatile int*) (rl.coreIDs + T);
   vp = (void*) (rl.thrrnks + P);
   ls.rank2thr = ATL_AlignPtr(vp);
/*
 * Initialize attribute: detached with system scope 
 */
   ATL_assert(!pthread_attr_init(attr));
   ATL_assert(!pthread_attr_setdetachstate(attr,PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
/*
 * Initialize rank arrays with -1; negative #s are codes, -1 means not init,
 * -2: started and then died.
 */
   for (i=0; i < P; i++)
      coreIDs[i] = rl.thrrnks[i] = -1;
   for (i=P; i < T; i++)
      coreIDs[i] = -1;
/*
 * Cooperate with worker threads to spawn all T threads
 */
   #ifdef ATL_GLOBAL
      while(ATL_DecGlobalAtomicCount(rl.Tcnt, 0))
   #else
      while(ATL_DecAtomicCount(rl.Tcnt))
   #endif
         pthread_create(&pt, attr, ATL_DoRankLaunch, &rl);
/*
 * Wait for all created threads to checkin; worker threads will all write
 * their coreID to their entry in the coreIDs array.  Their index in this array
 * is therefore their thread rank, which everyone agrees on due to using
 * the atomic counter.
 */
   coreID = ATL_coreID();  /* get my core ID */
   for (i=1; i < T; i++)
      while(coreIDs[i] == -1)
         ATL_thread_yield();
/*
 * All workers are spinning on coreIDs[0] awaiting my OK, so it is safe
 * to build all processor/thread ranking arrays 
 */
   uids[0] = coreID;
   rl.thrrnks[0] = 0;   /* master is always first worker */
   for (i=1; i < P; i++)
   {
      j = GetUniqueID(i, uids, T-1, (int*)coreIDs+1) + 1;
      if (!j)
        break;
      rl.thrrnks[i] = j;
      uids[i] = coreIDs[j];
   }
   nunique = i;
/*
 * We didn't get P unique coreIDs, so choose some coreIDs to get extra threads
 * Try to map all extra threads to same cores as much as possible, to make
 * it more likely OS gets off its ass and reschedules; also, since we are
 * using dynamically scheduled ops, only a few processors will be running
 * at low speeds.
 */
   while (i < P)
   {
      int k;
      for (k=0; k < i; k++)
      {
         for (j=0; j < T; j++)
            if (coreIDs[j] == uids[k])
               break;
         if (j < T)
         {
            rl.thrrnks[i] = j;
            uids[i] = coreIDs[j];
            i++;
            break;
         }
      }
   }
/*
 * Signal to workers that thread mapping is complete, then do my portion of work
 */
   coreIDs[0] = coreID;
   ls.rank2thr[0].rank = 0;
   ls.DoWork(&ls, ls.rank2thr);  /* do work */
/*
 * I could have freed these resources after first checkin, but have delayed
 * until now to avoid possible context switch due to system call.  Free some
 * resources I'm no longer using
 */
   ATL_assert(!pthread_attr_destroy(attr));  /* spawning complete, release */
   #ifdef ATL_GLOBAL
      ATL_FreeGlobalAtomicCount(rl.Tcnt);
      ATL_FreeGlobalAtomicCount(rl.Trankcnt);
   #else
      ATL_FreeAtomicCount(rl.Tcnt);
      ATL_FreeAtomicCount(rl.Trankcnt);
   #endif
/*
 * Wait for all threads to complete before returning
 */
   if (nunique < P)
      printf("Node 0 awaits completion on %d unique cores; P=%d\n", nunique, P);
   for (i=1; i < T; i++)
      while(coreIDs[i] != -2)
        ATL_thread_yield();
   free(uids);
}
#endif
@ROUT ATL_Xtgemm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/* 
 * =========================================================================
 * This file contains support routines for TGEMM that are not type-dependent
 * =========================================================================
 */
@ROUT ATL_thrdecompMM ATL_Xtgemm
int ATL_thrdecompMM_rMN
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine recursively splits the M & N dimensions over P processors
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

/*
 * Choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting (or be out of M blocks)
 */
   if (P > 1 && Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N, split M if possible
 */
   if (P > 1 && Mblks > 1)
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   ptmms[indx].ldcw = ptmms[indx].nCw = 0;
   ptmms[indx].nCp = ptmms[indx].ownC = 1;
   ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);
}

int ATL_thrdecompMM_rMNK
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine decomposes the GEMM over P processors by splitting any of
 * the dimensions.  We only call this routine when K is very large or
 * M and N are very small (and thus splitting K, with its associated
 * extra workspace and flops, makes sense).
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

   eltsh = ptmms[indx].eltsh;
#ifdef DEBUG
   ATL_assert(P > 0);
#endif
   if (P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 1))
      goto STOP_REC;
   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
/*
 * Do not consider cutting K unless we have some K blocks, and we have either
 * already done so, or if we are sure that we are within our workspace limit
 */
   if (Kblks > 1 && (COPYC || 
       ((Mblks*ptmms[indx].mb+mr) * ((Nblks*ptmms[indx].nb+nr)<<eltsh)
        < ATL_PTMAXMALLOC)))
   {
/*
 *    Before splitting K, ask that we are out of M and N blocks, or that
 *    our K is 4 times M and twice N
 */
      if ( (Mblks < 2 && Nblks < 2) ||
           (Kblks > (Mblks<<2) && Kblks > (Nblks+Nblks)) )
      {
         #ifdef DEBUG
            fprintf(stderr, "Cut K\n");
         #endif
         nblksL = (d * Kblks);
         nblksR = Kblks - nblksL;
         if (nblksR < nblksL)
         {
            rL = 0;
            rR = kr;
         }
         else
         {
            rL = kr;
            rR = 0;
         }
         i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
         np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                   nblksL, rL, A, lda, B, ldb, C, ldc, pL, 
                                   indx, COPYC);
         np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                    nblksR, rR, (TA==AtlasNoTrans)?
                                    MindxT(A,lda*i):MindxT(A,i), lda, 
                                    (TB == AtlasNoTrans)?MindxT(B,i):
                                    MindxT(B,i*ldb), ldb, C, ldc, pR, 
                                    indx+pL, 1);
         return(np);
      }
   }
/*
 * Now choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting
 */
   if (Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksR, rR, 
                                 Kblks, kr, A,  lda, (TB == AtlasNoTrans) ? 
                                 MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                 MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N or K, split M if possible
 */
   if (Mblks > 1)
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksR, rR, Nblks, nr, 
                                 Kblks, kr,
                                 (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda),
                                 lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                 COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
STOP_REC:
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   if (COPYC)
   {
      ptmms[indx].nCw = 1;
      ptmms[indx].nCp = ptmms[indx].ownC = 0;
      ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *    Make ldcw a multiple of 4 that is not a power of 2
 */
      i = ((ptmms[indx].M + 3)>>2)<<2;
      if (!(i & (i-1)))
         i += 4;
      ptmms[indx].ldcw = i;
   }
   else
   {
      ptmms[indx].ldcw = ptmms[indx].nCw = 0;
      ptmms[indx].nCp = ptmms[indx].ownC = 1;
      ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   }
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);

}
@beginskip
int ATL_thrdecompMM_rec
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

#ifdef DEBUG
   ATL_assert(P > 0);
#endif
/*
 * End recursion if we are down to 1 processor, or if we are out of blocks
 */
   if ( P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 2) )
   {
      ptmms[indx].A = A;
      ptmms[indx].B = B;
      ptmms[indx].C = (void*)C;
      ptmms[indx].lda = lda;
      ptmms[indx].ldb = ldb;
      ptmms[indx].ldc = ldc;
      ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
      ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
      ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
      if (COPYC)
      {
         ptmms[indx].nCw = 1;
         ptmms[indx].nCp = ptmms[indx].ownC = 0;
         ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *       Make ldcw a multiple of 4 that is not a power of 2
 */
         i = ((ptmms[indx].M + 3)>>2)<<2;
         if (!(i & (i-1)))
            i += 4;
@beginskip
         for (j=0; j < sizeof(ATL_INT)*8-1; j++)
         {
            if (!((1<<j)^i))
            {
               i += 4;
               break;
            }
         }
@endskip
         ptmms[indx].ldcw = i;
      }
      else
      {
         ptmms[indx].ldcw = ptmms[indx].nCw = 0;
         ptmms[indx].nCp = ptmms[indx].ownC = 1;
         ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
      }
      ptmms[indx].Cw = NULL;
#ifdef DEBUG
fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
        indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
        ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
#endif
      return(1);
   }

   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */

/*
 * Only cut K if it dominates M & N (here we say K must be 4 time larger)
 * and M&N are small enough that we can afford to malloc C 
 * (here we say C workspace must be 16MB or less) 
 */
   if ( ( ((Mblks < 2) && Nblks < 2) ||
          (((Kblks>>2) > Mblks) && ((Kblks>>2) > Nblks)) )
       && (Mblks*((Nblks*ptmms[indx].mb*(ptmms[indx].nb<<ptmms[indx].eltsh)
           +1023)>>10) <= 16*1024))
   {
#ifdef DEBUG
   fprintf(stderr, "Cut K\n");
#endif
      nblksL = (d * Kblks);
      nblksR = Kblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = kr;
      }
      else
      {
         rL = kr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksL, rL,
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksR, rR,
                                (TA==AtlasNoTrans)?MindxT(A,lda*i):MindxT(A,i), 
                                lda, 
                               (TB == AtlasNoTrans)?MindxT(B,i):MindxT(B,i*ldb),
                               ldb, C, ldc, pR, indx+pL, 1);
      return(np);
   }
   else if (Mblks >= Nblks)  /* split M */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut M\n");
#endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
   else /* split N */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut N\n");
#endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
}
@endskip

int ATL_thrdecompMM_M
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, m, p;
   const char *a=A, *c=C;
   const int eltsh = ptmms[0].eltsh, mb = ptmms[0].mb, n = ptmms[0].nb*Nblks+nr,
             k = ptmms[0].kb*Kblks+kr, minblks = Mblks / P, 
             extrablks = Mblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      m = minblks * mb;
      if (i < extrablks)
         m = (minblks + 1)*mb;
      else if (i == extrablks)
         m = minblks*mb + mr;
      else
         m = minblks*mb;
     if (m)
        p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = B;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (m) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      m <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,m) : MindxT(a,m*lda);
      c = MindxT(c,m);
   }
   return(p);
}

int ATL_thrdecompMM_N
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, n, p;
   const char *b=B, *c=C;
   const int eltsh = ptmms[0].eltsh, nb = ptmms[0].nb, m = ptmms[0].mb*Mblks+mr,
             k = ptmms[0].kb*Kblks+kr, minblks = Nblks / P, 
             extrablks = Nblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      n = minblks * nb;
      if (i < extrablks)
         n = (minblks + 1)*nb;
      else if (i == extrablks)
         n = minblks*nb + nr;
      else
         n = minblks*nb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = A;
      ptmms[i].B = b;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (n) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      n <<= eltsh;
      b = (TB == AtlasNoTrans) ? MindxT(b,n*ldb) : MindxT(b,n);
      c = MindxT(c,n*ldc);
   }
   return(p);
}
int ATL_thrdecompMM_K
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, k, p, ldw;
   const char *a=A, *b=B;
   const int eltsh = ptmms[0].eltsh, kb = ptmms[0].kb, m = ptmms[0].mb*Mblks+mr,
             n = ptmms[0].nb*Nblks+nr, minblks = Kblks / P, 
             extrablks = Kblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      k = minblks * kb;
      if (i < extrablks)
         k = (minblks + 1)*kb;
      else if (i == extrablks)
         k = minblks*kb + kr;
      else
         k = minblks*kb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = b;
      ptmms[i].C = (void*)C;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = k;
      if (i)
      {
         ptmms[i].nCw = 1;
         ptmms[i].nCp = ptmms[i].ownC = 0;
         ldw = ((m + 3)>>2)<<2;  /* make ldw mul of 4 */
         if (!(i & (i-1)))
            ldw += 4;            /* make sure ldw not power of 2 */
         ptmms[i].ldcw = ldw;
         ptmms[i].Cinfp[0] = ptmms+i;
      }
      else
      {
         ptmms[i].ldcw = 0;
         ptmms[i].nCp = ptmms[i].ownC = 1;
         ptmms[i].nCw = 0;
         ptmms[i].Cinfp[ATL_NTHREADS-1] = ptmms+i;
      }
      ptmms[i].Cw = NULL;
      k <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,lda*k) : MindxT(a,k);
      b = (TB == AtlasNoTrans) ? MindxT(b,k) : MindxT(b,k*ldb);
   }
   return(p);
}

#include <string.h>
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P)
/*
 * Takes P intialized entries in ptmms, and makes them contiguous
 * starting from 0 if they aren't already
 */
{
   int i;
   for (i=P-1; i >= 0; i--)
   {
      if (!ptmms[i].K)  /* found empty slot */
      {
         int j;
         for (j=P; !ptmms[j].K; j++);
         memcpy(ptmms+i, ptmms+j, sizeof(ATL_TMMNODE_t));
         if (ptmms[i].nCw || ptmms[i].nCp)
         {
            int k, n;
            n = ptmms[i].nCw;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[k] == ptmms+j)
                  ptmms[i].Cinfp[k] = ptmms+i;
            n = ptmms[i].nCp;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[ATL_NTHREADS-1-k] == ptmms+j)
                  ptmms[i].Cinfp[ATL_NTHREADS-1-k] = ptmms+i;
         }
         ptmms[j].K = 0;
      }
   }
}
@beginskip
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P)
/*
 * If threads aren't a power of 2, then the recursive decomposition will
 * fill in the ptmms array in a different order than the log2 spawn.
 * As long as P >= NTHREADS, all entries are filled in, so the worst that
 * happens is that thread 3 does the work that we expect to be done by 4.
 * However, if P < NTHREADS, then threads that launch starts won't have
 * work, so we must make sure that the 1st P elts in launchorder have
 * work to do.
 */
{
   int i, j, k, kk, h;

   if (P >= ATL_NTHREADS)
      return;
   for (i=0; i < P; i++)
   {
      j = ATL_launchorder[i];
      if (!ptmms[j].K)  /* we have found an empty required entry! */
      {
/*
 *      Search for a filled in entry that will not be used in launch
 */
        for (kk=ATL_NTHREADS-1; kk >= P; kk--)
        {
           k = ATL_launchorder[kk];
           if (ptmms[k].K)  /* found a non-empty entry */
           {
              ptmms[j].A = ptmms[k].A;
              ptmms[j].B = ptmms[k].B;
              ptmms[j].C = ptmms[k].C;
              ptmms[j].lda = ptmms[k].lda;
              ptmms[j].ldb = ptmms[k].ldb;
              ptmms[j].ldc = ptmms[k].ldc;
              ptmms[j].M = ptmms[k].M;
              ptmms[j].N = ptmms[k].N;
              ptmms[j].K = ptmms[k].K;
              ptmms[j].ownC = ptmms[k].ownC;
              ptmms[j].nCp = ptmms[k].nCp;
              ptmms[j].nCw = ptmms[k].nCw;
              ptmms[j].Cw = ptmms[k].Cw;
              ptmms[j].ldcw = ptmms[k].ldcw;
              for (h=0; h < ptmms[j].nCw; h++)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              for (h=ATL_NTHREADS-1; h >= ATL_NTHREADS-ptmms[j].nCp; h--)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              ptmms[k].K = 0;                /* entry k now empty */
              break;
           }
        }
        ATL_assert(kk >= P);
      }
   }
}
@endskip

int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK)
{
   int np, i;
   ATL_CINT Mblks = M/ptmms[0].mb, mr = M-Mblks*ptmms[0].mb;
   ATL_CINT Nblks = N/ptmms[0].nb, nr = N-Nblks*ptmms[0].nb;
   ATL_CINT Kblks = K/ptmms[0].kb, kr = K-Kblks*ptmms[0].kb;
   ATL_CINT mnblks = ((Nblks) ? Nblks : 1) * ((Mblks) ? Mblks : 1);

  *DivideK = 0;
/*
 * First, consider cutting K, which we only do if the number of Kblks
 * dominates the number of blocks we can find in cutting both M & N,
 */
@skip   if (mnblks < P || Kblks > P*mnblks)
   if ((mnblks < P && Kblks > mnblks && Kblks >= 8) || Kblks > P*mnblks)
   {
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, P, 0, 0);
      for (i=0; i < np; i++)
      {
         if (ptmms[i].K > 0 && ptmms[i].K < K)
         {
            *DivideK = 1;
            break;
         }
      }
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
@skip         ATL_EnforceNonPwr2LO(ptmms, np);
      return(np);
   }
/*
 * Divide only the M-dimension to cut down on JIK workspace & improve CE
 * efficiency if we have enough M blocks to make it worthwhile;
 * We ask that we can give each thread at least 4 blocks, and that
 * the N diminsion doesn't dominate
 */
   if ((Mblks >= (P<<2) && Nblks < P*Mblks))
   {
      np = ATL_thrdecompMM_M(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                             A, lda, B, ldb, C, ldc, P, 0, 0);
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
      return(np);
   }
/*
 * If none of these special cases are triggered, recursively divide up C
 */
   np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr, 
                            A, lda, B, ldb, C, ldc, P, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(ptmms, np);
@skip      ATL_EnforceNonPwr2LO(ptmms, np);
   return(np);
}

@ROUT ATL_StructIsInitMM ATL_Xtgemm
int ATL_StructIsInitMM(void *vp)
{
   return(((ATL_TMMNODE_t*)vp)->K);
}

@ROUT ATL_DoWorkMM ATL_Xtgemm
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * Current implementation doesn't need lp, but if we had an error queue or
 * something similar we would need it, so keep it around
 */
{
   ATL_thread_t *tp = vp;
   const int myrank = tp->rank;
   ATL_TMMNODE_t *mmp = ((ATL_TMMNODE_t*)lp->opstruct)+myrank;
/*
 * Allocate space if needed, do operation
 */
   if (mmp->nCw)
   {
/*
 *    If malloc fails, we'll do the operation during the combine
 */
      #ifdef ATL_SERIAL_COMBINE
         ATL_assert(mmp->Cw);
      #else
         mmp->Cw = malloc(((mmp->ldcw)<<mmp->eltsh)*mmp->N+ATL_Cachelen);
      #endif
      if (mmp->Cw)
      {
         mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                    mmp->B, mmp->ldb, mmp->zero, 
                    ATL_AlignPtr(mmp->Cw), mmp->ldcw);
      }
#ifdef DEBUG
      else
         fprintf(stderr, "%d: unable to allocate C(%dx%d)!!\n", 
                 mmp->rank, mmp->M, mmp->N);
#endif
   }
   else  /* do GEMM directly into original C; no possibility of failure! */
      mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                 mmp->B, mmp->ldb, mmp->beta, mmp->C, mmp->ldc);
}
@ROUT ATL_tNumGemmThreads
#include "atlas_misc.h"
#include "atlas_tlvl3.h"
#include Mstr(Mjoin(ATLAS_PRE,geamm_perf.h))

#if 1
/*
 * This function provides an estimate on max number of threads to use to
 * perform a access-major GEMM.
 */
size_t Mjoin(PATL,GetAmmmNthr)(ATL_CSZT M, ATL_CSZT N, ATL_CSZT K)
{
   size_t nnblks, nmblks, nkblks, p;

/*
 * On the XeonPHI, threads take a huge time to start up, so don't try
 * threading until we have a big problem.  We can hopefully fix this
 * later by changing to a thread pool model on the PHI.
 */
   #ifdef ATL_ARCH_XeonPHI
      if ((M*1e-6)*N*K < 8.0)
         return(1);
   #endif

   nmblks = (M >= ATL_geAMM_66MB) ? M/ATL_geAMM_66MB : 1;
   nnblks = (N >= ATL_geAMM_66NB) ? N/ATL_geAMM_66NB : 1;
   nkblks = (K >= ATL_geAMM_66KB) ? K/ATL_geAMM_66KB : 1;
/*
 * Any shape with two degenerate dimensions causes a lot of bus traffic,
 * with very little computation to overcome threading overheads,
 * so demand at least 32 blocks before parallelizing
 */
   if ((nmblks==1 && nnblks==1) || (nmblks==1 && nkblks==1) || 
       (nnblks==1 && nkblks==1))
      return((nnblks*nmblks*nkblks)>>5);
/*
 * If it is a rank-K update, ask to have 4 big blocks of C
 */
   if (K <= ATL_rkAMM_LASTKB)
   {
      nnblks=N/ATL_rkAMM_LASTNB, nmblks=M/ATL_rkAMM_LASTMB;
      return((nnblks*nmblks)>>2);
   }
   
/*
 * By default, give everyone 32 blocks to compute; for square problems,
 * the number of blocks is cubic, so this should not meaningfully restrict
 * parallelism.
 */
   return((nmblks*nnblks*nkblks)>>5);
}
#endif

/* 
 * ====================================================================
 * This function will eventually be generated, but for now just written
 * ====================================================================
 */
int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * RETURNS : estimate of how many threads will be used to run the problem,
 *           assuming we will actually do threading (i.e. THRESH is exceeded)
 *           0 is returned if this number is 1 or less.
 */
{
@beginskip
   int np;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ONE[2] = {1.0, 0.0};
   #else
      TYPE ONE=ATL_rone;
   #endif
   void Mjoin(PATL,InitTMMNodes)
      (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha,
       const TYPE *beta, const TYPE *one, const TYPE *zero, 
       ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);

   np = Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(AtlasNoTrans, AtlasNoTrans, SADD ONE, SADD ONE, 
                               SADD ONE, SADD ONE, NULL, mms);
      if (np == 1)  /* use recursive distribution */
         np = ATL_thrdecompMM_rec(mms, AtlasNoTrans, AtlasNoTrans, 
                                  M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                  NULL, M, NULL, K, NULL, M, 
                                  ATL_NTHREADS, 0, 0);
      else
         np = ATL_thrdecompMM_M(mms, AtlasNoTrans, AtlasNoTrans, 
                                M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                NULL, M, NULL, K, NULL, M, ATL_NTHREADS, 0, 0);
      np = (np < 2) ? 0 : np;
   }
   return(np);
@endskip
@skip   #if ATL_USE_AMM
   size_t np;
   np = Mjoin(PATL,GetAmmmNthr)(M, N, K);
   return(np >= ATL_NTHREADS ? ATL_NTHREADS : np);
@beginskip
   #else
      double flops;
      int np;
      if (M < 4 || N < 4 || K < 4)
         return(0);
      flops = ((2.0*M)*N)*K;
      np = flops / ATL_TGEMM_PERTHR_MF;
      np = (np <= 1) ? 0 : np;
      return(np >= ATL_NTHREADS ? ATL_NTHREADS : np);
   #endif
@endskip
}

int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * Returns: 0 if threshold FLOPS not achieved, rough # of threads used else
 */
{
@skip   #if ATL_USE_AMM
   size_t np;
   np = Mjoin(PATL,GetAmmmNthr)(M, N, K);
   if (np < 2)
      return(0);
   return(Mmin(np, ATL_NTHREADS));
@beginskip
   #else
      if (((2.0*M)*N)*K < ATL_TGEMM_THRESH_MF)
         return(0);
      return(Mjoin(PATL,tNumGemmThreads)(M,N,K));
   #endif
@endskip
}
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/*
 * prototype the typeless tGEMM helper routines
 */
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_StructIsInitMM(void *vp);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P);
@whiledef rt  ATL_thrdecompMM_M ATL_thrdecompMM_N ATL_thrdecompMM_K ATL_thrdecompMM_rMNK
int @(rt)
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC);
@endwhile
@ROUT ATL_tgemm ATL_tgemm_p
int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK);

@ROUT ATL_tgemm
@multidef tta AtlasTrans AtlasNoTrans AtlasConjTrans
@whiledef TA T N C
   @multidef ttb AtlasTrans AtlasNoTrans AtlasConjTrans
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb, 
    const void *beta, void *C, ATL_CINT ldc)
{
#ifdef FindingCE
void Mjoin(PATL,FindCE_mm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                           const int M, const int N, const int K, 
                           const SCALAR alpha, const TYPE *A, const int lda, 
                           const TYPE *B, const int ldb, const SCALAR beta, 
                           TYPE *C, const int ldc);
   Mjoin(PATL,FindCE_mm)(@(tta), @(ttb), M, N, K, 
                         SVVAL((TYPE*)alpha), A, lda, B, ldb,
                         SVVAL((TYPE*)beta), C, ldc);
#else
   Mjoin(PATL,ammm)(@(tta), @(ttb), M, N, K, SVVAL((TYPE*)alpha), 
                    A, lda, B, ldb, SVVAL((TYPE*)beta), C, ldc);
@skip   Mjoin(PATL,tgemm@(TA)@(TB))(M, N, K, SVVAL((TYPE*)alpha), A, lda, B, ldb,
@skip                       SVVAL((TYPE*)beta), C, ldc);
#endif
}
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
      @undef ttb
   @endwhile
   @undef tta
@endwhile
@ROUT ATL_tgemm_p
   @define ds @p@
@ROUT ATL_tgemm_M
   @define ds @M@
@ROUT ATL_tgemm_N
   @define ds @rMN@
@ROUT ATL_tgemm_K
   @define ds @K@
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);
@ROUT ATL_tgemm

#ifdef ATL_SERIAL_COMBINE
static ATL_combnode_t *ATL_NewCombnode
   (ATL_INT M, ATL_INT N, TYPE *W, ATL_INT ldw, TYPE *D, ATL_INT ldd,
    ATL_combnode_t *next)
{
   ATL_combnode_t *np;
   np = malloc(sizeof(ATL_combnode_t));
   ATL_assert(np);
   np->M = M;
   np->N = N;
   np->W = W;
   np->ldw = ldw;
   np->D = D;
   np->ldd = ldd;
   np->next = next;
   return(np);
}
#endif

void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms)
{
   int i;
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);

   if (TA == AtlasNoTrans)
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmNC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmNN):Mjoin(PATL,tsvgemmNT);
   }
#ifdef TCPLX
   else if (TA == AtlasConjTrans)
   {
      if (TB == AtlasNoTrans)
         gemmK = Mjoin(PATL,tsvgemmCN);
      else if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmCC);
      else
         gemmK = Mjoin(PATL,tsvgemmCT);
   }
#endif
   else
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmTC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmTN):Mjoin(PATL,tsvgemmTT);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ptmms[i].mb = Mmin(ATL_geAMM_LASTLCMU, ATL_geAMM_LASTMB);
      ptmms[i].nb = Mmin(ATL_geAMM_LASTLCMU, ATL_geAMM_LASTNB);
      ptmms[i].kb = ATL_geAMM_LASTKB;
      ptmms[i].gemmK = gemmK;
      ptmms[i].eltsz = ATL_sizeof;
      ptmms[i].eltsh = Mjoin(PATL,shift);
      ptmms[i].K = 0;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].ownC = 0;
      ptmms[i].rank = i;
      ptmms[i].alpha = (void*) alpha;
      ptmms[i].beta  = (void*) beta;
      ptmms[i].one = (void*) one;
      ptmms[i].zero  = (void*) zero;
      ptmms[i].Cinfp[0] = ptmms+i;
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
@ROUT ATL_tgemm
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw (or my->C), if poss.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * NOTE: This routine assumes him is *not* an owner of C (i.e. he wrote to
 *       workspace, not to the original C)!
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   ATL_TMMNODE_t *myCp;
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif
   const int eltsh = me->eltsh;

   ATL_assert(!him->ownC);
/*
 * Find starting/ending points of our C partitions
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   meB  = (size_t) me->C; 
   meE  = meB + ((me->N*(me->ldc) + me->M)<<eltsh);
/*
 * If I own my piece of the original C, then I can combine any C that is
 * a proper subset of mine; 
 */
   if (me->ownC)
   {
      ATL_assert(!him->ownC);  /* should never be true in this routine */
/*
 *    If his wrkspc is not a subset of mine, I can't combine it into the
 *    piece of C originally owned by me
 */
      if (himB < meB || himE > meE)
         return(1);       /* can't combine non subset of my C */
      else if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>eltsh;                    /* gap in elements */
      J = I / him->ldc;                           /* column coord */
      I -= J*him->ldc;                            /* row coord */
      if (I+him->M >= me->M || J+him->N >= me->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, w, me->ldcw);
         free(him->Cw);
      }
      else          /* must do GEMM since he didn't */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>eltsh;                    /* gap in elements */
      J = I / me->ldc;                            /* col coordinate */
      I -= J*me->ldc;                             /* row coordinate */
      if (I+me->M >= him->M || J+me->N >= him->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,geadd)(me->M, me->N, ONE, ATL_AlignPtr(me->Cw), 
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my GEMM into his workspace since I couldn't */
         him->gemmK(me->M, me->N, me->K, me->alpha, me->A, me->lda,
                    me->B, me->ldb, SADD ONE, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->M = him->M;
      me->N = him->N;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * Handles joining a Cp to my list of C partitions
 */
{
  size_t himB, himE, meB, meE, B, E, I, J;
  ATL_TMMNODE_t *tp;
  ATL_INT ldc;
  int i, j;
  const int eltsh = me->eltsh;
/*
 * Find the extent of his C partition
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   ldc = him->ldc;
/*
 * First, see if this partition can be joined to one I already own
 */
   for (i=0; i < me->nCp; i++)
   {
      tp = me->Cinfp[ATL_NTHREADS-1-i];
      if (tp)
      {
         meB  = (size_t) tp->C; 
         meE  = meB + ((tp->N*(tp->ldc) + tp->M)<<eltsh);
         if (meB <= himB)  /* my partition has the base pointer */
         {
            I = (himB - meB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from meB */
            I -= J*ldc;                 /* row coord from meB */
/*
 *          If we have same row (col) coord and he starts at col (row) that 
 *          I stop at, join column (row) panel
 */
            if (!I && J == tp->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            break;   /* partitions joined, quit */
         }
         else              /* his partition has the base pointer */
         {
            I = (meB - himB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from himB */
            I -= J*ldc;                 /* row coord from himB */
/*
 *          If we have same row (col) coord and I start at col (row) that 
 *          he stops at, join column (row) space
 */
            if (!I && J == him->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            tp->C = him->C;
            break;   /* partitions joined, quit */
         }
      }
   }
/*
 * If I can't join his partition to any of mine, add his to list
 */
   if (i == me->nCp)
   {
      (me->nCp)++;
      me->Cinfp[ATL_NTHREADS-(me->nCp)] = him;
      tp = him;
   }
/*
 * Either new partition is in tp, or an expanded partition is.  In either
 * case, see if any of my workspaces can be combined into this new 
 * (or newly expanded) area I own.
 */
   if (i < me->nCp)
   {
      for (i=0; i < me->nCw; i++)
      {
         if (!Mjoin(PATL,CombineCw)(tp, me->Cinfp[i]))
         {
            for (j=i+1; j < me->nCw; j++)
               me->Cinfp[j-1] = me->Cinfp[j];
            (me->nCw)--;
         }
      }
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,CombineStructsMM)(void *vp, const int myrank, const int herank);
@ROUT ATL_tgemm
void Mjoin(PATL,CombineStructsMM)
(
   void *vp,          /* void ptr to P MMNODE_t structs give tasks to threads */
   const int myrank,  /* my entry in MMNODE_t array */
   const int hisrank  /* array entry to be combined into mine */
)
{
   ATL_TMMNODE_t *mme = ((ATL_TMMNODE_t*)vp)+myrank; 
   ATL_TMMNODE_t *mhim = ((ATL_TMMNODE_t*)vp)+hisrank;
   int i, j;
   #ifdef ATL_SERIAL_COMBINE  /* do nothing if combining serially */
      return;
   #endif

/*
 * First, for all of the partitions of the original C that he owns, either
 * join them with mine, or add them to my list of owned partitions
 */
   for (i=0; i < mhim->nCp; i++)
      Mjoin(PATL,HandleNewCp)(mme, mhim->Cinfp[ATL_NTHREADS-1-i]);
/*
 * For all of his workspaces, find out where to combine them into
 */
   for (i=0; i < mhim->nCw; i++)
   {
/*
 *    Look through my partitions of original C for combine partner
 */
      for (j=0; j < mme->nCp; j++)
         if (!Mjoin(PATL,CombineCw)(mme->Cinfp[ATL_NTHREADS-1-j], 
                                    mhim->Cinfp[i]))
            break;
/*
 *    If I can't combine his data directly into C, see if it can be
 *    combined with any of my workspaces
 */
      if (j == mme->nCp)
      {
         for (j=0; j < mme->nCw; j++)
            if (!Mjoin(PATL,CombineCw)(mme->Cinfp[j], mhim->Cinfp[i]))
               break;
/*
 *       If I can't combine his data into any partition or workspace, add his
 *       node to my list of workspaces to be combined later
 */
         if (j == mme->nCw)
         {
            mme->Cinfp[j] = mhim->Cinfp[i];
            mme->nCw = j + 1;
         }
      }
   }
}

@ROUT ATL_tgemm
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_rec
int Mjoin(PATL,tgemm_rec)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
int Mjoin(PATL,tgemm_@(ds))(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha, 
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   #ifdef ATL_SERIAL_COMBINE
      ATL_combnode_t *combb=NULL, *combp;
   #endif
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   int i, np, DividedK=0;
   #ifdef TREAL
      TYPE ONE=ATL_rone, ZERO=ATL_rzero;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero}, ZERO[2] = {ATL_rzero, ATL_rzero};
   #endif
@ROUT ATL_tgemm_p `   extern int ATL_FINDP;`

   if (M < 1 || N < 1)
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   if (K < 1 || SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   }
@ROUT ATL_tgemm
   #if 0
   if (K > ATL_geAMM_LASTKB)
      if (!Mjoin(PATL,tammm_gMNK)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                  beta, C, ldc))
         return;
   #endif
   if (!Mjoin(PATL,tammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                          beta, C, ldc))
      return;
/*
 * See how many processors are optimal for this problem
 */
   np = Mjoin(PATL,threadMM)(TA, TB, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                               SADD ZERO, NULL, mms);
      np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                           np, &DividedK);
   }
@ROUT ATL_tgemm_p
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                        ATL_FINDP, &DividedK);
   if (np < ATL_FINDP)
      return(0);
@ROUT ATL_tgemm_rec
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_rMNK(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                             A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@beginskip
   #if ATL_NTHREADS != (1<<ATL_NTHRPOW2)
      if (np < ATL_NTHREADS)
         ATL_EnforceNonPwr2LO(mms, np);
   #endif
@endskip
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_@(ds)(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                          A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#ifdef DEBUG
fprintf(stderr, "np=%d\n\n", np);
#endif
   if (np < 2)
   {
      Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `      return(1);`
@ROUT ATL_tgemm `      return;`
   }
/*
 * If we are debugging, set up serial combine queue
 */
   #ifdef ATL_SERIAL_COMBINE
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (mms[i].K)   /* if this struct being used */
         {
            if (!mms[i].ownC)   /* I need a workspace for C */
            {
               mms[i].Cw = calloc(mms[i].ldcw * mms[i].N, ATL_sizeof);
               ATL_assert(mms[i].Cw);
               combb = ATL_NewCombnode(mms[i].M, mms[i].N, mms[i].Cw, 
                                       mms[i].ldcw, mms[i].C, mms[i].ldc,
                                       combb);
            }
         }
      }
   #endif

@beginskip
   ls.opstruct = (char*) mms;
   ls.opstructstride = (int) ( ((char*)(mms+1)) - (char*)mms );
   ls.OpStructIsInit = ATL_StructIsInitMM;
@ROUT ATL_tgemm_M ATL_tgemm_N
   ls.CombineOpStructs = NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
   ls.CombineOpStructs = DividedK ? Mjoin(PATL,CombineStructsMM) : NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   ls.DoWork = ATL_DoWorkMM;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_log2tlaunch, tp);
   ATL_thread_join(tp);
@endskip
@ROUT ATL_tgemm ATL_tgemm_K ATL_tgemm_p ATL_tgemm_rec
   ATL_goparallel(np, ATL_DoWorkMM, mms, 
                  DividedK ? Mjoin(PATL,CombineStructsMM) : NULL);
@ROUT ATL_tgemm_M ATL_tgemm_N
   ATL_goparallel(np, ATL_DoWorkMM, mms, NULL);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
/*
 * If we are debugging, serially combine all workspaces back to original C
 */
   #ifdef ATL_SERIAL_COMBINE
      while(combb)
      {
         Mjoin(PATL,geadd)(combb->M, combb->N, ONE, combb->W, combb->ldw,
                           ONE, combb->D, combb->ldd);
         free(combb->W);
         combp = combb;
         combb = combb->next;
         free(combp);
      }
   #endif
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `   return(np);`
}
@ROUT ATL_tgemm

void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc)
/* 
 * This void wrapper for tgemm is used in some typeless structures
 */
{
   Mjoin(PATL,tgemm)(TA, TB, M, N, K, SVVAL((const TYPE*)alpha), A, lda,
                     B, ldb, SVVAL((const TYPE*)beta), C, ldc);
}
@ROUT ATL_tsymm
   @define rt @symm@
   @define trans @AtlasTrans@
@ROUT ATL_themm
   @define rt @hemm@
   @define trans @AtlasConjTrans@
@ROUT ATL_themm ATL_tsymm
#include "atlas_misc.h"
@skip #include Mstr(Mjoin(AMM_UPR,_sum.h))
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TSYMM_t*)vp)->M);
}

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_TSYMM_t *sp=((ATL_TSYMM_t*)lp->opstruct)+tp->rank;
   Mjoin(PATL,@(rt))(sp->side, sp->uplo, sp->M, sp->N, SVVAL((TYPE*)sp->alpha),
                     sp->A, sp->lda, sp->B, sp->ldb, SVVAL((TYPE*)sp->beta),
                     sp->C, sp->ldc);
}

static void ATL_@(rt)L_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B10;
   TYPE *C10;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Mblks>>1;
   nbL = Mblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Nblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            nbR*nb, Nblks*nb+nr, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, Mblks*nb+mr, syp->N, 
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? mr : 0;
   rR = mr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B10 = B + (nL SHIFT);
   C10 = C + (nL SHIFT);
   ATL_@(rt)L_rec(syp, nbL, rL, Nblks, nr, A, B, C);
   ATL_@(rt)L_rec(syp, nbR, rR, Nblks, nr, A+(syp->lda+1)*(nL SHIFT), B10, C10);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
   else
   {
      A01 = A + nL*(syp->lda SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
}
static void ATL_@(rt)R_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B01;
   TYPE *C01;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Nblks>>1;
   nbL = Nblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Mblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            Mblks*nb+mr, nbR*nb, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, syp->M, Nblks*nb+nr,
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B01 = B + (nL*syp->ldb SHIFT);
   C01 = C + (nL*syp->ldc SHIFT);
   ATL_@(rt)R_rec(syp, Mblks, mr, nbL, rL, A, B, C);
   ATL_@(rt)R_rec(syp, Mblks, mr, nbR, rR, A+(syp->lda+1)*(nL SHIFT), B01, C01);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A10, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A10, syp->lda, 
                        ONE, C01, syp->ldc);
   }
   else
   {
      A01 = A + (syp->lda * (nL SHIFT));
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A01, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A01, syp->lda, 
                        ONE, C01, syp->ldc);
   }
}

static void ATL_t@(rt)_SYsplit
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc,
    ATL_CINT nb)
/*
 * This routine is specialized for the case where we cannnot split the
 * B matrix, and must instead split the symmetric matrix (A).  It calls
 * a recursive GEMM-based BLAS, that gets its parallel performance from
 * calling threaded GEMM.
 */
{
   ATL_TSYMM_t ss;
   ss.side = Side;
   ss.uplo = Uplo;
   ss.M = M;
   ss.N = N;
   ss.nb = nb;
   ss.alpha = SADD alpha;
   ss.beta  = SADD beta;
   ss.lda = lda;
   ss.ldb = ldb;
   ss.ldc = ldc;
   if (Side == AtlasLeft)
      ATL_@(rt)L_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);
   else
      ATL_@(rt)R_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);

}

/*
 * The XOVER is the min # of blocks required to do parallel operation
 */
#ifndef ATL_TSYMM_XOVER
   #ifdef ATL_TGEMM_XOVER
      #define ATL_TSYMM_XOVER ATL_TGEMM_XOVER
   #else
      #define ATL_TSYMM_XOVER 4  /* want 4 blocks for parallel execution */
   #endif
#endif
/*
 * Once you have achieved enough blocks to do computation, minimum number
 * of blocks to give each processor
 */
#ifndef ATL_TSYMM_ADDP
   #ifdef ATL_TGEMM_ADDP 
      #define ATL_TSYMM_ADDP  ATL_TGEMM_ADDP 
   #else
      #define ATL_TSYMM_ADDP  1  /* want 1 blocks to add proc to workers */
   #endif
#endif
void Mjoin(PATL,t@(rt))
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT n, nblks, tblks, nr, minblks, extrablks, p, i, j;
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TSYMM_t symms[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   const TYPE *b;
   TYPE *c;
   static int nb=0;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      return;
   }
   if (!nb) nb = ATL_sqAMM_66KB;
   if (Side == AtlasLeft)
   {
      nblks = N / nb;
      nr = N - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* N not big enough to give blk to each proc */
      {
/*
 *       If I can't split N, and M is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (M > (N<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute N over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = M;
         symms[i].N = n;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)ldb)*n);
         c = MindxT(c, ATL_MulBySize((size_t)ldc)*n);
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   else  /* Side == AtlasRight */
   {
      nblks = M / nb;
      nr = M - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* M not big enough to give blk to each proc */
      {
/*
 *       If I can't split M, and N is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (N > (M<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute M over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = n;
         symms[i].N = N;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)n));
         c = MindxT(c, ATL_MulBySize((size_t)n));
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   if (p < 2)
   {
SERIAL:
      Mjoin(PATL,@(rt))(Side, Uplo, M, N, alpha, A, lda, B, ldb, beta, C, ldc);
      return;
   }
   ATL_goparallel(p, Mjoin(PATL,DoWork@up@(rt)), symms, NULL);
@beginskip
   ls.opstruct = (char*) symms;
   ls.opstructstride = (int) ( ((char*)(symms+1)) - (char*)(symms) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInit@up@(rt));
   ls.DoWork = Mjoin(PATL,DoWork@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
}

@ROUT ATL_ttrmm
   @define rt @trmm@
@ROUT ATL_ttrsm
   @define rt @trsm@
@ROUT ATL_ttrsm ATL_ttrmm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_ttypes.h"

@beginskip
/*
 * For very small problems, use static scheduling to avoid dynamic overheads
 * Since we currently wait for all the threads to go to sleep before returning,
 * static scheduling essentially cannot be slower for tiny problems.
 */
void Mjoin(PATL,tDo@up@(rt)_tTRs)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_tTR_t *pd = pp->PD;
   const enum ATLAS_SIDE side = pd->side;
   const size_t ldb=pd->ldb;
   size_t inc;
   const int rb=pd->rb, neblks=pd->neblks, rr = pd->rr;
   int nrhs=pd->minrhs;
   TYPE *X = pd->B;
   if (vrank < neblks-1)
   {
      nrhs += rb;
      inc = nrhs*vrank;
   }
   else if (vrank == neblks-1)
   {
      inc = (nrhs+rb)*vrank;
      nrhs += (rr) ? rr : rb;
   }
   else if (rr)  /* vrank >= neblks, last block partial */
      inc = (neblks-1)*(nrhs+rb) + nrhs+rr + (vrank-neblks)*nrhs;
   else /* vrank > neblks, last block full */
      inc = neblks*(nrhs+rb) + (vrank-neblks)*nrhs;
   if (side == AtlasRight)
   {
      X += inc SHIFT;
      Mjoin(PATL,@(rt))(side, pd->uplo, pd->TA, pd->diag, nrhs, pd->N, 
                       pd->alpha, pd->A, pd->lda, X, ldb);
   }
   else
   {
      X += (inc*ldb)SHIFT;
      Mjoin(PATL,@(rt))(side, pd->uplo, pd->TA, pd->diag, pd->M, nrhs, 
                       pd->alpha, pd->A, pd->lda, X, ldb);
   }
}

void Mjoin(PATL,tDo@up@(rt)_tTR)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_tTR_t *pd = pp->PD;
   const unsigned int rb=pd->rb, rbF=pd->rbF, nrblks=pd->nrblks;
   const enum ATLAS_SIDE side = pd->side;
   enum ATLAS_TRANS TA=pd->TA;
   enum ATLAS_DIAG uplo=pd->uplo;
   enum ATLAS_DIAG diag=pd->diag;
   ATL_CINT M=pd->M, N=pd->N;
   const size_t lda=pd->lda, ldb=pd->ldb;
   int rcnt, rblk;
   TYPE *X = pd->B;
   const TYPE *A = pd->A;
   void *rhsBlkCtr = pd->rhsBlkCtr;
   const SCALAR alpha = pd->alpha;

   while ((rcnt = ATL_DecGlobalAtomicCount(rhsBlkCtr, vrank)))
   {
      const int rblk = nrblks - rcnt;
      const int nrhs = (rcnt != 1) ? rb : rbF;
      if (side == AtlasRight)
         Mjoin(PATL,@(rt))(side, uplo, TA, diag, nrhs, N, alpha, 
                          A, lda, X+rblk*(rb SHIFT), ldb);
      else
         Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, nrhs, alpha,
                          A, lda, X+rblk*rb*(ldb SHIFT), ldb);
   }
}

@endskip
@ROUT ATL_ttrsm
@beginskip
#if 0
/*
 * This routine specialized for small triangular matrix
 */
void Mjoin(PATL,ttrsm_tTR)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_tTR_t pd;
   ATL_INT nrhs = (side == AtlasLeft) ? N : M;
   ATL_INT nrblks;
   int rb, rbF, P;
   ATL_tpjfunc_t DoWork = Mjoin(PATL,tDoTRSM_tTR);
   size_t tblks;

   tblks = (((size_t)M)*N)>>10;
   tblks = (tblks+3)>>1;
   P = Mmin(ATL_NTHREADS,tblks);
   rb = nrhs>>1;
   P = Mmin(P, rb);
   if (P < 2)
   {
      Mjoin(PATL,trsm)(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   if (nrhs < (P<<6))
   {
      nrblks = P;
      if (nrhs >= (P<<2))
         rb = (nrhs / (P<<2))<<2;
      else
         rb = nrhs / P;
      rbF = rb + nrhs - nrblks*rb;
      DoWork = Mjoin(PATL,tDoTRSM_tTRs);
   }
   else
   {

/*
 *    Try block being a multple of 60 (LCM(1,2,3,4,5), so good for NU of amm)
 */
      rb = nrhs/60;

      if (rb >= (P<<4))                  /* we've got enough to give everyone */
         rb = ((rb+P+P-1)/(P<<1))*60;    /* 2 full jobs for load balancing */
      else                               /* we've got enough to give everyone */
         rb = ((rb+P-1)/P)*60;           /* 1 job: screw load balancing */
      nrblks = nrhs / rb;
      rbF = nrhs - nrblks*rb;
      if (!rbF)
         rbF = rb;
      else
         nrblks++;
   }
//printf("M=%d, N=%d, P=%d, nrblks=%d, rb=%d, rbF=%d, R=%d \n", 
//       M, N, P, nrblks, rb, rbF, rb*(nrblks-1)+rbF);
   pd.side = side;
   pd.uplo = uplo;
   pd.TA = TA;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.nrblks = nrblks;
   pd.rb = rb;
   pd.rbF = rbF;
   pd.rhsBlkCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nrblks, P), nrblks, 0);
   ATL_goParallel(P, DoWork, NULL, &pd, NULL);
   ATL_FreeGlobalAtomicCount(pd.rhsBlkCtr);
}
#endif
@endskip
@ROUT ATL_ttrsm ATL_ttrmm
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TTRSM_t*)vp)->B != NULL);
}

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *thp=vp;
   ATL_TTRSM_t *tp=((ATL_TTRSM_t*)lp->opstruct) + thp->rank;
   Mjoin(PATL,@(rt))(tp->side, tp->uplo, tp->TA, tp->diag, tp->M, tp->N,
                    SVVAL((TYPE*)tp->alpha), tp->A, tp->lda, tp->B, tp->ldb);
}

@ROUT ATL_ttrsm ATL_ttrmm
#ifndef ATL_TTRSM_XOVER
   #define ATL_TTRSM_XOVER 4   /* want 4 total blocks before adding proc */
#endif
void Mjoin(PATL,t@(rt))(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_TTRSM_t trsms[ATL_NTHREADS];
   TYPE *b;
   ATL_INT n, nblks, minblks;
   double tblks;
   int nr, p, i, j, extrablks;
   static int nb=0;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      Mjoin(PATL,gezero)(M, N, B, ldb);
      return;
   }
@ROUT ATL_ttrsm
   #if 0 && defined(ATL_ARCH_XeonPHI) && defined(TREAL)
   {
      int Mjoin(PATL,ttrsm_amm)
         (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
          const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
          ATL_CINT M, ATL_CINT N, const SCALAR alpha,
          const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
     if (!Mjoin(PATL,ttrsm_amm)(side, uplo, TA, diag, M, N, alpha, 
                                A, lda, B, ldb))
        return;
   }
   #endif
@ROUT ATL_ttrsm ATL_ttrmm
/*
 * Distribute RHS over the processors
 */
   if (!nb) nb = ATL_sqAMM_66KB;
   if (side == AtlasLeft)
   {
      nblks = N/nb;
      nr = N - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (tblks+ATL_TTRSM_XOVER-1)/ATL_TTRSM_XOVER;
      p = Mmin(p, ATL_NTHREADS);
      p = p ? p : 1;

      b = B;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
         trsms[i].A = A;
         trsms[i].M = M;
         trsms[i].N = n;
         trsms[i].lda = lda;
         trsms[i].ldb = ldb;
         trsms[i].B = b;
         trsms[i].alpha = SADD alpha;
         trsms[i].side = side;
         trsms[i].uplo = uplo;
         trsms[i].TA   = TA;
         trsms[i].diag = diag;
         n *= (ldb << Mjoin(PATL,shift));
         b = MindxT(b, n);
      }
   }
   else /* Side == AtlasRight */
   {
      nblks = M/nb;
      nr = M - nblks*nb;
      tblks = (N/nb)*nblks;
      p = (tblks+ATL_TTRSM_XOVER-1)/ATL_TTRSM_XOVER;
      p = Mmin(p, ATL_NTHREADS);
      p = p ? p : 1;

      b = B;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
         trsms[i].A = A;
         trsms[i].M = n;
         trsms[i].N = N;
         trsms[i].lda = lda;
         trsms[i].ldb = ldb;
         trsms[i].B = b;
         trsms[i].alpha = SADD alpha;
         trsms[i].side = side;
         trsms[i].uplo = uplo;
         trsms[i].TA   = TA;
         trsms[i].diag = diag;
         n <<= Mjoin(PATL,shift);
         b = MindxT(b, n);
      }
   }
   if (p < 2)
   {
      Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
      trsms[i].B = NULL;
   ATL_goparallel(p, Mjoin(PATL,DoWork@up@(rt)), trsms, NULL);
}
@beginskip
{
   ATL_ttrsm_tTR_t pd;
   ATL_INT nrblks, nrhs = (side == AtlasLeft) ? N : M;
   int nblksP, neblks, rb, rbF, P, rr;
   size_t tblks;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      Mjoin(PATL,gezero)(M, N, B, ldb);
      return;
   }
@ROUT ATL_ttrsm_bad
   #if 0
   if (0)
   {
      Mjoin(PATL,ttrsm_tTR)(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   #endif
   #if defined(ATL_ARCH_XeonPHI) && defined(TREAL)
   {
      int Mjoin(PATL,ttrsm_amm)
         (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
          const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
          ATL_CINT M, ATL_CINT N, const SCALAR alpha,
          const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
     if (!Mjoin(PATL,ttrsm_amm)(side, uplo, TA, diag, M, N, alpha, 
                                A, lda, B, ldb))
        return;
   }
   #endif
@ROUT ATL_ttrsm_bad ATL_ttrmm_bad
   tblks = (((size_t)M)*N)>>10;
   tblks = (tblks+3)>>2;
   P = Mmin(ATL_NTHREADS,tblks);
   rb = nrhs>>2;
   P = Mmin(P, rb);
/*
 * We keep the block sizes a multiple of 4 because we have special rank-4 update
 * based code for some TRSM kernels that gets used for the solve part
 */
   if (nrhs >= P*ATL_sqAMM_LASTNB)
      rb = (ATL_sqAMM_LASTNB>>2)<<2;
   #ifdef ATL_sqAMM_98NB
      else if (nrhs >= P*ATL_sqAMM_98NB)
         rb = (ATL_sqAMM_98NB>>2)<<2;
   #endif
   #ifdef ATL_sqAMM_66NB
      else if (nrhs >= P*ATL_sqAMM_66NB)
         rb = (ATL_sqAMM_66NB>>2)<<2;
   #endif
   else
      rb = 4;
   nrblks = nrhs / rb;
   P = Mmin(P, nrblks);
   if (P < 2)
   {
      Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   rr = nrhs - nrblks*rb;

   nblksP = nrblks / P;
   neblks = nrblks - nblksP*P;
   if (rr)
      neblks++;

   pd.neblks = neblks;
   pd.rr = rr;
   pd.rb = rb;
   pd.minrhs = nblksP*rb;
   pd.side = side;
   pd.uplo = uplo;
   pd.TA = TA;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.rhsBlkCtr = NULL;
   ATL_goParallel(P, Mjoin(PATL,tDo@up@(rt)_tTRs), NULL, &pd, NULL);
}
@ROUT ATL_Xtsyr2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
                  
static int ATL_tsyr2kK(ATL_SYR2K_t *syp, ATL_CINT N, ATL_CINT K, 
                       const void *A, const void *B, void *C)
/*
 * Attempts to allocate workspace W, then do:
 *   (1) W = alpha*A*B' or alpha*A'B (GEMM)
 *   (2) C <- beta*C + W + W'
 * RETURNS: 0 on success, nonzero if unable to allocate memory
 */
{
   void *v=NULL, *W;
   ATL_INT ldw;
   int i;
   size_t sz;
   const int eltsh = syp->eltsh;

/*
 * Make ldw a multiple of 4 that is not a power of 2
 */
   ldw = ((N+3)>>2)<<2;
   if (!(ldw&(ldw-1)))
      ldw += 4;
@beginskip
   for (i=0; i <= sizeof(ldw)*8; i++)
   {
      if (!(ldw^(1<<i)))
      {
         ldw += 4;
         break;
      }
   }
@endskip
   sz = (ldw*N)<<eltsh;
   if (sz <= ATL_PTMAXMALLOC)
      v = malloc(sz + ATL_Cachelen);
   if (!v)
      return(1);  /* signal we can't get memory */

   W = ATL_AlignPtr(v);
   syp->tvgemm(syp->TA, syp->TB, N, N, K, syp->alpha, A, syp->lda, B, syp->ldb,
               syp->zero, W, ldw);
   syp->tvApAt(syp->Uplo, N, W, ldw, syp->beta, C, syp->ldc);
   free(v);
   return(0);
}

void ATL_tvsyr2k_rec
   (ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, const void *A, 
    const void *B, void *C)
/*
 * Do SYR2K/HER2K, either by mallocing space and calling GEMM, or recurring
 * until C is small enough that space can be allocated.  Gets its parallelism
 * from the calls to parallel GEMM
 */
{
   const int nb = syp->nb, eltsh = syp->eltsh;
   ATL_INT nL, nR, nbL, nbR, rL, rR;
   void *gc, *sc;   /* ptr to C to update with gemm & 2nd syr2k call, resp */
   void *A1, *B1;   /* ptr to 2nd block of a/b resp */
/*
 * Attempt to halt recursion by allocating workspace, and calling GEMM
 */
   if (!ATL_tsyr2kK(syp, Nblks*nb+nr, syp->K, A, B, C))
      return;
   ATL_assert(Nblks>1 || (Nblks==1 && nr));  /* must have something to split */
/*
 * Must recur in order to make problem small enough to allocate C workspace
 */
   nbR = Nblks>>1;
   nbL = Nblks - nbR;
   rL = (nL == nR) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   sc = MindxT(C, (((size_t)nL*(syp->ldc+1))<<eltsh));
   if (syp->trans == AtlasNoTrans)
   {
      A1 = MindxT(A, ((size_t)nL<<eltsh));
      B1 = MindxT(B, ((size_t)nL<<eltsh));
   }
   else  /* index like transpose */
   {
      A1 = MindxT(A, (((size_t)nL*syp->lda)<<eltsh));
      B1 = MindxT(B, (((size_t)nL*syp->ldb)<<eltsh));
   }

   ATL_tvsyr2k_rec(syp, nbL, rL, A, B, C);
   if (syp->Uplo == AtlasUpper)
   {
      gc = MindxT(C, (((size_t)nL*syp->ldc)<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha, A, syp->lda, 
                  B1, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha2, B, syp->ldb, 
                  A1, syp->lda, syp->one, gc, syp->ldc);
   }
   else
   {
      gc = MindxT(C, ((size_t)nL<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha, A1, syp->lda, 
                  B, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha2, B1, syp->ldb, 
                  A, syp->lda, syp->one, gc, syp->ldc);
   }
   ATL_tvsyr2k_rec(syp, nbR, rR, A1, B1, sc);

}

@ROUT ATL_tsyr2k
   @define rt @syr2k@
   @define ApA @syApAt@
   @define trans @AtlasTrans@
@ROUT ATL_ther2k
   @define rt @her2k@
   @define ApA @heApAc@
   @define trans @AtlasConjTrans@
@ROUT ATL_tsyr2k ATL_ther2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "atlas_lvl3.h"

void Mjoin(PATL,tv@(ApA))(const enum ATLAS_UPLO Uplo, ATL_CINT N, const void *A,
                          ATL_CINT lda, const void *beta, void *C, ATL_CINT ldc)
{
   Mjoin(PATL,@(ApA))(Uplo, N, A, lda, SVVAL((const TYPE*)beta), C, ldc);
}

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
@ROUT ATL_tsyr2k
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_ther2k
    const TYPE *B, ATL_CINT ldb, const TYPE beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyr2k ATL_ther2k
{
   ATL_SYR2K_t sy;
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero,ATL_rzero};
@ROUT ATL_ther2k 
      const TYPE alpha2[2]={*alpha,(alpha[1]!=ATL_rzero)?-alpha[1]:ATL_rzero};
      const TYPE beta[2] = {beta0, ATL_rzero};
@ROUT ATL_tsyr2k ATL_ther2k
   #endif

   if (N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha) || K < 1)
   {
@ROUT ATL_ther2k 
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyr2k 
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
/*
 * This call to serial is usually a performance optimization, but was actually
 * put in to avoid a failure in the LAPACK eigenvalue tests that is caused
 * by subtractive cancellation.  There is no bug, but our parallel algorithm
 * does the operations in a different order than small-case serial, and in
 * one case used by the lapack testers, this causes subtractive cancellation
 * to occur (the serial code can use different orders for differing problem
 * sizes).  Other problems will experience cancelation under the serial order,
 * so AFAIK, neither is more correct numerically.
 */
   if (N < 3*ATL_sqAMM_LASTLCMU && K < 3*ATL_sqAMM_LASTLCMU)
   {
@ROUT ATL_tsyr2k
      Mjoin(PATL,syr2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_ther2k 
      Mjoin(PATL,her2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, 
                        beta0, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
@ROUT ATL_tsyr2k
   sy.alpha2 = sy.alpha = SADD alpha;
   sy.beta  = SADD beta;
@ROUT ATL_ther2k
   sy.alpha = SADD alpha;
   sy.alpha2 = alpha2;
   sy.beta  = beta;
@ROUT ATL_tsyr2k ATL_ther2k
   sy.one = SADD ONE;
   sy.zero = SADD ZERO;
   sy.tvgemm = Mjoin(PATL,tvgemm);
   sy.tvApAt = Mjoin(PATL,tv@(ApA));
   sy.K = K;
   sy.lda = lda;
   sy.ldb = ldb;
   sy.ldc = ldc;
   sy.eltsh = Mjoin(PATL,shift);
   sy.Uplo = Uplo;
   sy.trans = Trans;
   if (Trans == AtlasNoTrans)
   {
      sy.TA = AtlasNoTrans;
      sy.TB = @(trans);
      sy.TA2 = @(trans);
      sy.TB2 = AtlasNoTrans;
   }
   else
   {
      sy.TA = @(trans);
      sy.TB = AtlasNoTrans;
      sy.TA2 = AtlasNoTrans;
      sy.TB2 = @(trans);
   }
   sy.nb = ATL_sqAMM_66KB;
   ATL_tvsyr2k_rec(&sy, N/sy.nb, N%sy.nb, A, B, C);
}

@beginskip
/*
 * NOTE: want to put these primitives in src/auxil, then I write higher
 *       level drivers that call them on blocks
 */
#ifdef TREAL
void Mjoin(PATL,putAAt_L)
   (ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    TYPE *L, ATL_CINT ldl)
/*
 * L <- A+At, L lower triangular
 */
{
   TYPE *Ar, *Ac=A;
   ATL_INT i, j;

   for (j=0; j < N; j++)
   {
      for (Ar=A+j,i=j; i < M; i++, Ar += lda)
      #ifdef BETA0
         Cc[i] = Ac[i] + *Ar;
      #elif defined(BETA1)
         Cc[i] += Ac[i] + *Ar;
      #else
         Cc[i] = beta*C[i] + Ac[i] + *Ar;
      #endif
      Ac += lda;
   }
}
void Mjoin(PATL,putABt_L)
   (ATL_CINT M, ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, TYPE *C, ATL_CINT ldc)
/*
 * C <- beta*C + A + B'
 */
{
   TYPE *Br;
   for (j=0; j < N; j++)
   {
      for (Br=B,i=0; i < M; i++, Br += ldb)
      #ifdef BETA0
         Cc[i] = A[i] + *Br;
      #elif defined(BETA1)
         Cc[i] += A[i] + *Br;
      #else
         Cc[i] = beta*C[i] + A[i] + *Br;
      #endif
      C += ldc;
      A += lda;
      B++;
   }
}
#else
#endif

void Mjoin(PATL,vsyr2k_putL)(ATL_CINT N, const void *beta0, const void *A, 
                             ATL_CINT lda, void *C, ATL_CINT ldc)
/* 
 * Takes A with property (A + A') = (A + A')', 
 *    C <- beta*C + A + A'
 * NOTE: This kernel is unblocked for initial try, which could cause TLB 
 *        disaster.  Need to write blocked version, and perhaps thread.
 */
{
   const TYPE *Ac = A, *Ar;
   const TYPE beta = *((const TYPE*)beta0);
   TYPE *Cc = C;
   ATL_CINT i, j;

   if (beta == ATL_rzero)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = Ac[i] + *Ar;
      }
   }
   else if (beta == ATL_rone)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] += Ac[i] + *Ar;
      }
   }
   else
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = beta*Cc[i] + Ac[i] + *Ar;
      }
   }
}
@endskip
@ROUT ATL_Xtsyrk
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "math.h"
/*
 * Recursive decompositon on trapazoidal-shaped matrix ($C$ after splitting)
 */
#ifndef ATL_MINL3THRFLOPS
   #ifdef ATL_TGEMM_ADDP
      #define ATL_MINL3THRFLOPS \
         (((2.0*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)
   #else
      #define ATL_MINL3THRFLOPS (((2.0*MB)*NB)*KB)
   #endif
#endif
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc)
{
   double flops;
   double percL;  /* % of calculation to do on left size */
   int pL, pR; 
   ATL_INT i, nL, nR, rL, rR;
   const int nb = psyrk->nb;
  
   pR = (P>>1);
   pL = P - pR;
@skip pL=1; pR=P-1;   /* HERE HERE HERE: debug */
   percL = (pL == pR) ? 0.5 : ((double)pL)/((double)P);
   
/*
 * If problem is triangular, divide up problem so LEFT does SYRK+GEMM, and
 * RIGHT does SYRK only; this means nL = T(1-sqrt(percentL))
 */
   if (!Mblks && !Nblks)
   {
@skip      nL = 0.5 + ((pL == pR) ? 0.29289322*Tblks : 2.0*percL*Tblks*(1.0-0.5*sqrt(2)));
      nL = (0.58578664*Tblks)*percL;
      nR = Tblks - nL;
      flops = nR*nb;
      flops *= flops;
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
fprintf(stderr, "FLOPS=%.2f\n", (1.0*psyrk->T)*psyrk->T);
         psyrk->M = psyrk->N = 0;
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
      rL = (nL > nR) ? 0 : tr;
      rR = tr - rL;
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, rL, nR, rR, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, rR, 0, 0, 0, 0, K,
                              ia+nL*nb+rL, ja, ib, jb+nL*nb+rL, 
                              ic+nL*nb+rL, jc+nL*nb+rL);
      return(i);
   }
/*
 * Only divide M if all the SYRK flops can be done in LEFT's work.
 * Divides gemm's M asymmetrically to match the SYRK flops
 */
   if (Mblks>=Nblks)
   {
      if (Tblks)
         ATL_assert(Nblks == Tblks && nr == tr);
      nL = 0.5*percL*(Mblks+Mblks-Tblks);
      if (nL < 1) nL = 1;
      nR = Mblks - nL;
      flops = 2.0*nR*nb*Nblks*nb*K;
      if (P < 2 || !nL || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "T=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (!Tblks && nL <= nR) ? mr : 0;
      rR = mr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, Tblks, tr, nL, rL, Nblks, nr, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, nR, rR, Nblks, nr, K,
                              ia+(nL+Tblks)*nb+rL+tr, ja, ib, jb, 
                              ic+(nL+Tblks)*nb+rL+tr, jc);
      return(i);
   }
/*
 * As a last choice, cut both GEMM and SYRK (N & T) together
 * Must be done asymmetrically to balance differently sized gemms
 */
   if (Nblks && Tblks && nr == tr) /* divide N & T*/
   {
      nL = 2.0*percL*(Nblks+Mblks-
         sqrt((0.5*Nblks*Nblks)+((double)Nblks)*Mblks+((double) Mblks)*Mblks));
      if (nL < 1) nL = 1;
      nR = Nblks - nL;
      flops = nR * nb;
      flops = flops*flops + (2.0*Mblks)*(nb*flops);
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, 0, Mblks+nR, tr, nL, 0, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, tr, Mblks, mr, nR, tr, K,
                              ia+nL*nb, ja, ib, jb+nL*nb, ic+nL*nb, jc+nL*nb);
      return(i);
   }
   else  /* dividing a GEMM on N-dimension only */
   {
      ATL_assert(!Tblks && !tr && Nblks >= Mblks);
      nL = percL*Nblks + 0.5;
      nR = Nblks - nL;
      flops = ((2.0*nR*nb)*Mblks)*nb*K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (nL > nR) ? 0 : nr;
      rR = nr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, 0, 0, Mblks, mr, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, Mblks, mr, nR, rR, K,
                              ia, ja, ib, jb+nL*nb+rL, ic, jc+nL*nb+rL);
      return(i);
   }
}

#include <string.h>
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp)
{
   ATL_TSYRK_t stmp;
   int i, j, ib, jj;
   double mf0, mf;

   for (i=0; i < P-1; i++)
   {
      ib = ATL_launchorder[i];
      if (syp[ib].ia < 0)
         continue;
      mf0 = ((double)syp[ib].T)*syp[ib].T + (2.0*syp[ib].M)*syp[ib].N;
      for (j=i+1; j < P; j++)
      {
         jj = ATL_launchorder[j];
         if (syp[jj].ia < 0)
            continue;
         mf = ((double)syp[jj].T)*syp[jj].T + (2.0*syp[jj].M)*syp[jj].N;
         if (mf > mf0)
         {
            mf0 = mf;
            ib = jj;
         }
      }
      jj = ATL_launchorder[i];
      if (ib != jj)
      {
         memcpy(&stmp, syp+ib, sizeof(ATL_TSYRK_t));
         memcpy(syp+ib, syp+jj, sizeof(ATL_TSYRK_t));
         memcpy(syp+jj, &stmp, sizeof(ATL_TSYRK_t));
      }
   }
}
int ATL_StructIsInitSYRK(void *vp)
{
   return( ((ATL_TSYRK_t*)vp)->ia >= 0 );
}

void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_t *syp = ((ATL_TSYRK_t*)lp->opstruct)+tp->rank;
   const int lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;

   if (syp->T)
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->T, syp->K, syp->alpha, 
                  syp->A+((syp->ia+syp->ja*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->jc*ldc)<<eltsh), ldc);
   if (syp->M && syp->N)
      syp->tvgemm(AtlasNoTrans, AtlasTrans, syp->M, syp->N, syp->K, syp->alpha,
                  syp->A+((syp->ia+syp->T+syp->ja*lda)<<eltsh), lda,
                  syp->A+((syp->jb+syp->ib*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->T+syp->jc*ldc)<<eltsh), ldc);
}
@endskip
@beginskip
#ifndef ATL_DoMMParallel
   #ifndef ATL_TGEMM_MINFLOPS
      #define ATL_TGEMM_MINFLOPS 512000.0
   #endif
   #define ATL_DoMMParallel(M_, N_, K_) \
      (((((double)(M_))*(N_))*(K_)) >= ATL_TGEMM_MINFLOPS)
#endif

int ATL_tsyrkdecomp_MM(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
/*
 * This routine splits a gemm coming from SYRK until nthr is exhausted
 */
{
   int lo[ATL_NTHREADS];
   ATL_CINT M=psy->M, N=psy->N, K=psy->K, nb=psy->nb;
   ATL_INT m, nblks, nr, minblks, extrablks;
   const int eltsh=psy->eltsh, amul=(psy->TA == AtlasNoTrans) ? 1 : psy->lda;
   const int bmul = (psy->TB == AtlasNoTrans) ? 1 : psy->lda;
   int i, j, k, nt, np;
   const void *a;
   void *c;

   if (nthr == 1 || !psy->numthr(M, N, K))
      return(1);
   nt = nthr >> 1;
/*
 * If M is large enough, cut it for entire GEMM distribution in order to
 * optimize ATLAS's common JIK pattern
 */
   if (M >= nb*nthr*ATL_TMMMINMBLKS)
   {
/*
 *    Determine launchorder on this subset of processors
 */
      lo[0] = 0;
      for (i=0; (1<<i)^nthr; i++);
      lo[0] = 0;
      k=1;
      for (i--; i >= 0; i--)
      {
         for (j=0; j < k; j++)
            lo[k+j] = lo[j] + (1<<i);
         k += k;
      }
/*
 *    Find how many blocks we've got
 */
      nblks = M / nb;
      nr = M - nblks*nb;
      minblks = nblks / nthr;
      extrablks = nblks - minblks*nthr;
/*
 *    Get everyone a copy of entire data structure & assign subpieces
 */
      c = psy->C;
      a = psy->A0;
      for (i=0; i < nthr; i++)
      {
         j = lo[i];
         if (i) { McpSYN(psy, j, 0); }
         if (i < extrablks)
            m = (minblks+1)*nb;
         else if (i == extrablks)
            m = minblks*nb + nr;
         else
            m = minblks*nb;
         psy[j].M = m;
         psy[j].C = c;
         psy[j].A0 = a;
         m <<= eltsh;
         a = MindxT(a,m*amul);
         c = MindxT(c,m);
      }
      return(nthr);
   }
   else if (M >= N) /* split M */
   {
      McpSYN(psy, nt, 0);
      m = M>>1;
      psy[nt].M = m;
      psy->M = m = M - m;
      m <<= eltsh;
      psy[nt].A0 = MindxT(psy->A0,m*amul);
      psy[nt].C = MindxT(psy->C,m);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
   else        /* split N */
   {
      McpSYN(psy, nt, 0);
      m = N>>1;
      psy[nt].N = m;
      psy->N = m = N-m;
      m <<= eltsh;
      psy[nt].C = MindxT(psy->C,m*psy->ldc);
      psy[nt].A1 = MindxT(psy->A1,m*amul);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
}

int ATL_IsInitSYRK_N(void *vp)
{
   return( ((ATL_TSYRK_N_t*)vp)->K );
}

int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
{
   void *vp;
   ATL_INT nL, nR, iL, iR;
   int nt, p;
   const int ISUPPER = (psy->Uplo == AtlasUpper), 
             ISNOTRANS = (psy->TA == AtlasNoTrans);

   if (nthr == 1)
      return(1);
   nt = nthr >> 1;  /* nthr is power of two, so both sides get nt threads */
/*
 * Copy present psy struct into new one, and then modify both as required
 */
   if (psy->C)   /* we are splitting two SYRKs */
   {
      if (!psy->numthr(psy->N, psy->N, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      psy[nt].T = psy->C;
      psy[nt].A0 = psy[nt].A1;
      psy[nt].C = psy->C = NULL;
      psy[nt].M = psy->N;
      psy->N = psy[nt].N = 0;
      p = ATL_tsyrkdecomp_N(psy, nt);
      p += ATL_tsyrkdecomp_N(psy+nt, nt);
      return(p);
   }
   else         /* we are splitting one SYRK into 2 SYRKS and one GEMM */
   {
      if (!psy->numthr(psy->M, (psy->M)>>1, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      nL = (psy->M)>>1;
      nR = psy->M - nL;
      iL = nL << psy->eltsh;
      iR = nR << psy->eltsh;
/*
 *    Give psy[0] both SYRKs to do, and continue splitting
 */
      psy->C = MindxT(psy->T, iL*(psy->ldc+1));
      psy->A1 = (ISNOTRANS) ? MindxT(psy->A0, iL) : MindxT(psy->A0,iL*psy->lda);
      psy->M = nL;
      psy->N = nR;
      p = ATL_tsyrkdecomp_N(psy, nt);
/*
 *    Give psy[nt] a GEMM to do, and call routine to split GEMMs
 */
      psy[nt].C = (ISUPPER) ? MindxT(psy[nt].T,iL*psy[nt].ldc) : 
                              MindxT(psy[nt].T,iL);
      psy[nt].T = NULL;
      if (ISUPPER)
      {
         psy[nt].M = nL;
         psy[nt].N = nR;
         psy[nt].A1 = (ISNOTRANS) ? 
            MindxT(psy[nt].A0,iL) : MindxT(psy[nt].A0,iL*psy[nt].lda);
      }
      else
      {
         psy[nt].M = nR;
         psy[nt].N = nL;
         if (ISNOTRANS)
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL);
         }
         else  /* A is Transposed */
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL*psy[nt].lda);
         }
      }
      p += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(p);
   }
}

void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_N_t *syp=((ATL_TSYRK_N_t*)lp->opstruct)+tp->rank;
   if (syp->T)  /* doing SYRKs */
   {
      syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
                  syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
      if (syp->C)
         syp->tvsyrk(syp->Uplo, syp->TA, syp->N, syp->K, syp->alpha, 
                     syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
   }
   else /* doing GEMM */
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A0, syp->lda, 
                 syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
}
@endskip

int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    int np, const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc)
{
@skip   ATL_CINT minblks = Kblks / ATL_NTHREADS, 
@skip            extrablks = Kblks - minblks*ATL_NTHREADS;
   ATL_INT minblks, extrablks, j, k, ldcw;
   int i;

/*
 * Note that this routine is essentially for large K, so we don't consider
 * any K smaller than NB for a processor
 */
   minblks = Kblks / np;
   if (minblks)
      extrablks = Kblks - minblks*np;
   else
   {
      np = Kblks;
      minblks = 1;
      extrablks = 0;
   }
/*
 * Find a good ldcw: multiple of 4 that is not a power of two
 */
   ldcw = ((N+3)>>2)<<2;   /* multiple of 4 */
   if (!(ldcw&(ldcw-1)))
      ldcw += 4;
@beginskip
   for (i=0; i < sizeof(ldcw)*8; i++)
   {
      if (!((1<<i)^ldcw))  /* if it is a power of two this is eventually 0 */
      {
         ldcw += 4;
         break;
      }
   }
@endskip
   if ((ldcw<<eltsh)*N > ATL_PTMAXMALLOC)
      return(0);
   for (i=0; i < np; i++)
   {
      if (i < extrablks)
         k = (minblks + 1)*nb;
      else if (i == extrablks)
         k = minblks*nb + kr;
      else
         k = minblks * nb;
      j = N;
@skip      lo = ATL_launchorder[i];   /* use log2-launch order */
      psyrk[i].alpha = alpha;
      psyrk[i].beta  = beta ;
      psyrk[i].one   = one  ;
      psyrk[i].zero  = zero ;
      psyrk[i].Uplo = Uplo;
      psyrk[i].Trans = Trans;
      psyrk[i].N = N;
      psyrk[i].K = k;
      psyrk[i].A = A;
      psyrk[i].C = C;
      psyrk[i].lda = lda;
      psyrk[i].ldc = ldc;
      psyrk[i].eltsh = eltsh;
      if (!i)
         psyrk[0].nCw = psyrk[0].ldcw = 0;
      else
      {
         psyrk[i].nCw = 1;
         psyrk[i].ldcw = ldcw;
      }
      psyrk[i].Cw = NULL;
      psyrk[i].Cinfp[0] = psyrk + i;
      psyrk[i].tvsyrk = syrkK;
      k = (Trans == AtlasNoTrans) ? lda * k : k;
      k <<= eltsh;
      A = MindxT(A,k);
   }
   for (; i < ATL_NTHREADS; i++)
      psyrk[i].N = 0;
   return(np);
}

void ATL_tsyrk_K(ATL_TSYRK_K_t *syp, int np, ATL_CINT N, ATL_CINT K, 
                 const void *A, void *C)
{
   const int nb = syp->nb;
   void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);

   if (np < 1 || Mmin(N,K) < 8)
      np = 1;
   else
      np = ATL_tsyrkdecomp_K(syp, syp->tvsyrk, np, syp->eltsh, nb, syp->zero,
                             syp->one, syp->Uplo, syp->Trans, N, K/nb, K%nb, 
                             syp->alpha, A, syp->lda, syp->beta, C, syp->ldc);
   if (np < 2)
   {
      syp->tvsyrk(syp->Uplo, syp->Trans, N, K, syp->alpha, A, syp->lda, 
                  syp->beta, C, syp->ldc);
      return;
   }
   ATL_goparallel(np, ATL_DoWorkSYRK_K, syp, syp->DoComb);
@beginskip
   ATL_thread_start(syp->lp->rank2thr, 0, 1, ATL_tlaunch, syp->lp->rank2thr);
   ATL_thread_join(syp->lp->rank2thr);
@endskip
}

void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A0, void *C00)
/*
 * This routine recurs on N until we can allocate the full NxN workspace,
 * at which point it stops the recursion and distributes K for parallel
 * operation
 */
{
   const enum ATLAS_TRANS TA = syp->Trans;
   ATL_CINT lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;
   ATL_CINT nb = syp->nb, N = Nblks*nb+nr;
   ATL_INT sz, nblksL, nblksR, nrL, nrR, nL, nR;
   const void *A1;
   void *C10, *C01, *C11;
/*
 * Stop recursion & call threaded SYRK if we can allocate workspace for all of C
 */
   sz = (N * N) << eltsh;
/*
 * Quit recurring if we can allocate space for C workspace and we can
 * no longer usefully split Nblks, or we can usefully split K
 */
   if (sz <= ATL_PTMAXMALLOC && (nb*ATL_NTHREADS < K || Nblks < ATL_NTHREADS))
   {
      ATL_tsyrk_K(syp, np, Nblks*nb+nr, K, A0, C00);
      return;
   }
   nblksL = (Nblks+1)>>1;
   nblksR = Nblks - nblksL;
   if (nblksL >= nblksR)
   {
      nrL = nr;
      nrR = 0;
   }
   else
   {
      nrL = 0;
      nrR = nr;
   }

   nL = nblksL * nb + nrL;
   nR = nblksR * nb + nrR;
   if (syp->Uplo == AtlasUpper) 
   {
      sz = nL<<eltsh;
      C01 = MindxT(C00,sz*ldc);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      C11 = MindxT(C01,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nL, nR, K, syp->alpha, A0, lda, A1, lda, 
                 syp->beta, C01, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
   else /* Lower triangular matrix */
   {
      sz = nL<<eltsh;
      C10 = MindxT(C00,sz);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      sz += (ldc*nL)<<eltsh;
      C11 = MindxT(C00,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nR, nL, K, syp->alpha, A1, lda, A0, lda, 
                 syp->beta, C10, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
}

void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_K_t *syp = ((ATL_TSYRK_K_t *)lp->opstruct)+tp->rank;
/*
 * Allocate space if needed, and then do SYRK into it
 */
   if (syp->nCw)
   {
      syp->Cw = malloc((syp->ldcw << syp->eltsh)*syp->N+ATL_Cachelen);
      if (syp->Cw)
         syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, syp->A,
                     syp->lda, syp->zero, ATL_AlignPtr(syp->Cw), syp->ldcw);
   }
   else /* do SYRK directly into original C: no poss of failure */
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, 
                  syp->A, syp->lda, syp->beta, syp->C, syp->ldc);
}

int ATL_IsInitSYRK_K(void *vp)
{
   return( ((ATL_TSYRK_K_t*)vp)->N );
}

@ROUT tsttr
#include <stdio.h>
#include <stdlib.h>
#ifndef ATL_MU
   #define ATL_MU 4
#endif
#ifndef ATL_NTHREADS
   #define ATL_NTHREADS 8
#endif
#ifndef NB
   #define NB 56
#endif
#define ATL_INT int
#define ATL_CINT int
#define ATL_MINL3THRFLOPS (2.0*NB*NB*NB)
#define ATL_TGEMM_PERTHR_MF (2.0*NB*NB*NB)
@ROUT ATL_Xtsyrk tsttr
int ATL_tsyrkdecomp_tr1D(int P, ATL_CINT N, ATL_CINT K, 
                         ATL_CINT nb, ATL_CINT mu, double minmf, ATL_INT *Ms)
/*
 * Partitions triangular matrix from SYRK into roughly equal flop count
 * regions, with the first such region being strictly triangular, and the
 * rest trapazoidal row-panels.
 * Ms : must be of length P at least, on output contains the correct size
 *      matrix to give to each processor.
 * RETURNS: number of processors used
 */
{
   double Pflops, myflops, tflops, pinv;
   const int incM = (nb >= 60) ? ((24+mu-1)/mu)*mu : 
                    ((nb < 16) ? nb : ((16+mu-1)/mu)*mu);
   ATL_INT n, m, j;
   int k, p;

   for (k=0; k < P; k++)
      Ms[k] = 0;
   tflops = (((double)N)*N)*K;
   while (P && (Pflops = tflops/((double)P)) < minmf) P--;
   if (P < 2)
      return(0);

   tflops /= K;
/*
 * For each processor, find m that balances the flop count
 */
   for (n=p=0; p < P; p++)
   {
      Pflops = tflops / (P-p);
      if (tflops*K < minmf)
      {
         if (p < 2)
            return(0);
         Ms[p-1] += N - n;
         return(p);
      }
/*
 *    Finds the largest m that is a multiple of nb that generates <= Pflops
 */
      m = nb;  /* number of rows in row-panel */
      k = 1;   /* number of blocks in m */
      do
      {
         myflops = m;
         myflops *= myflops + n + n;
         if (myflops == Pflops)
            break;
         else if (myflops > Pflops)
         {
            m -= nb;
            k--;
            myflops = m;
            myflops *= myflops + n + n;
            break;
         }
         m += nb;
         k++;
      }
      while (1);
/*
 *    If we are below target flop count, see how to adjust
 */
      if (myflops < Pflops)
      {
         j = (k < 4) ? incM : mu;  /* for small M, don't tolerate cleanup */
         while ((((double)m)*((((double)m)+n)+n)) < Pflops) m += j;
         myflops = (((double)m)*((((double)m)+n)+n));
      }
      j = N - n;
      if (m >= j)
      {
         if (j < incM)
         {
            if (p < 1)
               return(0);
            Ms[p-1] += j;
            return(p);
         }
         Ms[p] = j;
         return(p+1);
      }
      else if (p == P-1)
         m = N - n;
      n += m;
      Ms[p] = m;
      tflops -= myflops;
   }
   return(p);
}
@ROUT tsttr
int main(int nargs, char **args)
{
   int N=1000, K=1000;
   int i, p, n;
   ATL_INT Ms[ATL_NTHREADS];
   double myflops, tflops;
   if (nargs > 1)
      N = atoi(args[1]);
   if (nargs > 2)
      K = atoi(args[2]);
   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, NB, ATL_MU, 
                            ATL_TGEMM_PERTHR_MF, Ms);
   printf("\n\nN=%d, K=%d:\n", N, K);
   if (p < 1)
      printf("   Unable to distribute!\n");
   else
   {
      tflops = (((double)N)*N)*K;
      printf("   P       M       N       FLOPS\n");
      printf("====  ======  ======  ==========\n\n");
      n = 0;
      for (i=0; i < p; i++)
      {
         myflops = (((double)Ms[i])*(Ms[i]+n+n))*K;
         printf("%4d %7d %7d %.0f (%.2f)\n", i, Ms[i], N, 
                myflops, myflops/tflops);
                
         n += Ms[i];
      }
      printf("Total M = %d\n\n", n);
   }
   return(0);
}
@ROUT ATL_Xtsyrk
int ATL_IsInitSYRK_M(void *vp)
{
   return( ((ATL_TSYRK_M_t*)vp)->K );
}

int ATL_tsyrkdecomp_M
(
   ATL_TSYRK_M_t *syp,          /* output: parallel decomposition structs */
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS TA,
   ATL_CINT N, ATL_CINT K,      /* original problem size */
   const void *alpha,
   const void *A,
   ATL_CINT lda,
   const void *beta,
   void *C,
   ATL_CINT ldc,
   ATL_CINT nb,                 /* MB of GEMM kernel */
   const int mu,                /* reg blking factor along M of MM kernel */
   const int eltsh,
   const enum ATLAS_TRANS TB,   /* Dual of TA (Conj for herk, trans for syrk) */
   double minmf,
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT),
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT)
)
{
   ATL_INT Ms[ATL_NTHREADS];
   int k, j, p;
   ATL_CINT incA = lda << eltsh, incC = (ldc+1) << eltsh;
   ATL_INT n, m, JJ;
   const int ISNOTRANS = (TA == AtlasNoTrans);

   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, nb, mu, minmf, Ms);
   if (p < 2)
      return(0);
   if (Uplo == AtlasLower)
   { 
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         syp[k].T = MindxT(C,((size_t)n*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)n<<eltsh)) : NULL;
         syp[k].A0 = (ISNOTRANS) ? MindxT(A,((size_t)n<<eltsh)) 
                                 : MindxT((size_t)A,n*incA);
         syp[k].A = syp[k].A0;
         syp[k].B = A;
         n += m;
      }
   }
   else  /* Uplo == AtlasUpper */
   {
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         JJ = N - n - m;
         syp[k].T = MindxT(C,((size_t)JJ*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)JJ*incC+m*(ldc<<eltsh))) : NULL;
         if (ISNOTRANS)
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ<<eltsh));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m<<eltsh)); 
         }
         else
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ*incA));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m*incA)); 
         }
         n += m;
      }
   }
   for (k=p; k < ATL_NTHREADS; k++)
      syp[k].K = 0;
   return(p);
}

void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_M_t *syp = ((ATL_TSYRK_M_t*)lp->opstruct) + tp->rank;

   syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
               syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
   if (syp->C)
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A, syp->lda,
                 syp->B, syp->lda, syp->beta, syp->C, syp->ldc);
}

@ROUT ATL_tsyrk
   @define TAC @T@
   @define TRANS @AtlasTrans@
   @define sadd @SADD@
   @define rt @syrk@
   @define styp @SCALAR@
@ROUT ATL_therk
   @define TAC @C@
   @define TRANS @AtlasConjTrans@
   @define sadd @&@
   @define rt @herk@
   @define styp @TYPE@
@ROUT ATL_tsyrk ATL_therk
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"

/*
 * Prototype functions in ATL_Xtsyrk
 */
int ATL_IsInitSYRK_M(void *vp);
void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_tsyrkdecomp_M
   (ATL_TSYRK_M_t *syp, const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA,
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc, ATL_CINT nb, const int mu,
    const int eltsh, const enum ATLAS_TRANS TB, double minmf,
    void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                  ATL_CINT,const void*, ATL_CINT, const void*, void*, ATL_CINT),
    void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                   ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                   void*, ATL_CINT));
@beginskip
int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr);
int ATL_IsInitSYRK_N(void *vp);
void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp);
@endskip
int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc);
void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A, void *C);
void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_IsInitSYRK_K(void *vp);
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc);
int ATL_StructIsInitSYRK(void *vp);
void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp);
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp);
@endskip

void Mjoin(PATL,tv@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda, 
    const void *beta, void *C, ATL_CINT ldc)
{
@ROUT ATL_tsyrk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, SVVAL((TYPE*)alpha), A, lda, 
                   SVVAL((TYPE*)beta), C, ldc);
@ROUT ATL_therk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, *((TYPE*)alpha), A, lda, 
                   *((TYPE*)beta), C, ldc);
@ROUT ATL_tsyrk ATL_therk
}

@beginskip
static void ATL_init@up@(rt)_t
   (const int P, ATL_TSYRK_t *syp, const void *alpha, const void *beta, 
    const void *one, const void *zero, enum ATLAS_UPLO Uplo, 
    enum ATLAS_TRANS Trans, ATL_CINT K, const void *A, ATL_CINT lda, 
    void *C, ATL_CINT ldc)
{
   int i, nb;
   nb = ATL_sqAMM_66KB;

   for (i=0; i < P; i++)
   {
      syp[i].alpha = alpha;
      syp[i].beta  = beta ;
      syp[i].one   = one  ;
      syp[i].zero  = zero ;
      syp[i].Uplo = Uplo;
      syp[i].Trans = Trans;
      syp[i].A = A;
      syp[i].lda = lda;
      syp[i].ldc = ldc;
      syp[i].K = K;
      syp[i].C = C;
      syp[i].tvgemm = Mjoin(PATL,tvgemm);
      syp[i].tvsyrk = Mjoin(PATL,tv@(rt));
      syp[i].eltsh = Mjoin(PATL,shift);
      syp[i].ia = -1;  /* flag that this entry is not being used */
      syp[i].nb = nb;
   }
}
@endskip

static int CombineCw(ATL_TSYRK_K_t *me, ATL_TSYRK_K_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw, if possible.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif

/*
 * If I'm the master (owner of original C), then I can always do combine
 * into the original C
 */
   if (me->nCw == 0)
   {
      if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
   meB  = (size_t) me->C; 
   meE  = meB + (((me->N*(me->ldc + 1)))<<(me->eltsh));
   himB = (size_t)him->C; 
   himE = himB + (((him->N*(him->ldc + 1)))<<(me->eltsh));
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>(him->eltsh);           /* gap in elts */
      J = I / him->ldc;                         /* col coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, w, him->ldcw);
         free(him->Cw);
      }
      else          /* must do SYRK since he didn't */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>(him->eltsh);           /* gap in elements */
      J = I / him->ldc;                         /* column coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,tradd)(me->Uplo, me->N, ATL_AlignPtr(me->Cw),
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my SYRK into his workspace since I couldn't */
         him->tvsyrk(me->Uplo, me->Trans, me->N, me->K, me->alpha, 
                     me->A, me->lda, me->one, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->N = him->N;
      me->K = him->K;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,CombineStructs@up@(rt))
   (void *opstruct, const int myrank, const int hisrank)
/*
 * This routine written like GEMM, so that SYRK can have been split
 * with N, even though present code only splits K (so everyone is writing
 * to entire C).  I may want the extra functionality later, so programmed
 * it using GEMM as model.
 * NOTE: this version actually wouldn't work if we split both N & K for
 *       all cases; I later had to redesign the GEMM combine to account
 *       for the fact that you have to sum up the pieces of the original C
 *       you own, instead of always modifying C when you own only  a piece
 *       of it.  This problem only shows up on systems with non-power-of-2
 *       # of processors, where the launch recursive distribution doesn't
 *       match the recursive launch/combine procedure.  Will need to rewrite
 *       based on present GEMM combine if I ever go to true recursive
 *       distribution on both N & K.
 */
{
   #ifdef TREAL
      TYPE ONE = ATL_rone;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
   #endif
   ATL_TSYRK_K_t *me = ((ATL_TSYRK_K_t*)opstruct)+myrank; 
   ATL_TSYRK_K_t *him = ((ATL_TSYRK_K_t*)opstruct)+hisrank, *himcp, *mycp;
   int i, j;

/*
 * Need to combine only if joining thread has C in workspace
 */
   if (him->nCw)
   {
/*
 *    For all his workspaces, find out where to combine them into
 */
      for (i=0; i < him->nCw; i++)
      {
/*
 *       If I can't combine his data into my primary workspace, see if it
 *       can be combined with any of my other workspaces
 */
         if (CombineCw(me, him->Cinfp[i]))
         {
            for (j=1; j < me->nCw; j++)
               if (!CombineCw(me->Cinfp[j], him->Cinfp[i]))
                  break;
/*
 *          If I can't combine his data into any existing auxiliary space,
 *          add his node to my list of workspaces to be combined later
 */
            if (j == me->nCw)
            {
               me->Cinfp[j] = him->Cinfp[i];
               me->nCw = j + 1;
            }
         }
      }
   }
}

void Mjoin(PATL,t@(rt)_K_rec)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const SCALAR beta, TYPE *C, ATL_CINT ldc, ATL_CINT nb)
/*
 * This typed wrapper routine sets up type-specific data structures, and
 * calls the appropriate typeless recursive routine in order to recursively
 * cut N until workspace can be allocated, and then the K-dimension will be
 * threaded.  During the recursion, parallel performance is achieved by
 * calling the threaded GEMM.
 */
{
   ATL_CINT Nblks = N/nb, nr = N - nb*Nblks;
   ATL_TSYRK_K_t syp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
@skip   ATL_thread_t tp[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ZERO[2] = {ATL_rzero, ATL_rzero}, ONE[2] = {ATL_rone, ATL_rzero};
   #else
      TYPE ZERO=ATL_rzero, ONE=ATL_rone;
   #endif
   int i;

@beginskip
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   syp[0].lp = &ls;
@endskip
   syp[0].DoComb = Mjoin(PATL,CombineStructs@up@(rt));
   syp[0].Uplo = Uplo;
   syp[0].Trans = Trans;
@ROUT ATL_therk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasConjTrans : AtlasNoTrans;`
@ROUT ATL_tsyrk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasTrans : AtlasNoTrans;`
   syp[0].K = K;
   syp[0].alpha = SADD alpha;
   syp[0].beta = SADD beta;
   syp[0].zero = SADD ZERO;
   syp[0].one  = SADD ONE;
   syp[0].lda = lda;
   syp[0].ldc = ldc;
   syp[0].gemmT = Mjoin(PATL,tvgemm);
@skip   syp[0].gemmT = (Trans == AtlasNoTrans) ? 
@skip      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   syp[0].tvsyrk = Mjoin(PATL,tv@(rt));
   syp[0].eltsh = Mjoin(PATL,shift);
   syp[0].nb = nb;
@skip   ls.opstruct = (char*) syp;
   ATL_tsyrk_K_rec(syp, Mjoin(PATL,threadMM)(Trans, syp[0].TB, N>>1, N>>1, K), 
                   Nblks, nr, K, A, C);
}

static int ATL_t@(rt)_M
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_M_t syp[ATL_NTHREADS];
   int i, p;
   p = ATL_tsyrkdecomp_M(syp, Uplo, TA, N, K, alpha, A, lda, beta, C, ldc,
                         ATL_sqAMM_LASTLCMU, ATL_sqAMM_LASTMU, Mjoin(PATL,shift), 
                         (TA == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans,
                         ATL_TGEMM_PERTHR_MF, (TA == AtlasNoTrans) ? 
                         Mjoin(PATL,tsvgemmN@(TAC)):Mjoin(PATL,tsvgemm@(TAC)N),
                         Mjoin(PATL,tv@(rt)));
   if (p < 2)
      return(0);
   ATL_goparallel(p, ATL_DoWorkSYRK_M, syp, NULL);
@beginskip
   ls.opstruct = (char*) syp;
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_M;
   ls.DoWork = ATL_DoWorkSYRK_M;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(p);
}

@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
static int ATL_t@(rt)_N
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_N_t psy[ATL_NTHREADS];
   int i, p;

   psy[0].gemmK = (Trans == AtlasNoTrans) ? 
      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   psy[0].tvsyrk = Mjoin(PATL,tv@(rt));   
   psy[0].numthr = Mjoin(PATL,tNumGemmThreads);
   psy[0].A0 = A;
   psy[0].A1 = NULL;
   psy[0].T = C;
   psy[0].C = NULL;
   psy[0].alpha = alpha;
   psy[0].beta  = beta;
   psy[0].M = N;
   psy[0].N = 0;
   psy[0].K = K;
   psy[0].lda = lda;
   psy[0].ldc = ldc;
   psy[0].nb = ATL_sqAMM_66KB;
   psy[0].eltsh = Mjoin(PATL,shift);
   psy[0].Uplo = Uplo;
   psy[0].TA = Trans;
   psy[0].TB = (Trans == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans;
   for (i=1; i < ATL_NTHREADS; i++)
      psy[i].K = 0;
   p = ATL_tsyrkdecomp_N(psy, ATL_NTHREADS);
   if (p < 2)
      return(0);
   ls.opstruct = (char*) psy;
   ls.opstructstride = (int) ( ((char*)(psy+1)) - (char*)psy );
   ls.OpStructIsInit = ATL_IsInitSYRK_N;
   ls.DoWork = ATL_DoWorkSYRK_N;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return(p);
}
#endif
@endskip

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
@rout ATL_therk
    ATL_CINT K, const @(styp) alpha0, const TYPE *A, ATL_CINT lda,
    const @(styp) beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk
    ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
    const @(styp) beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk ATL_therk
{
@beginskip
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_t syrks[ATL_NTHREADS];
   ATL_TSYRK_K_t psyrks[ATL_NTHREADS];
@endskip
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero, ATL_rzero};
@ROUT ATL_therk `      const TYPE alpha[2]={alpha0, ATL_rzero}, beta[2]={beta0, ATL_rzero};`
   #endif
   size_t nblksN;
   int i, np, nb;
   void Mjoin(PATL,pt@(rt))
      (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
       ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
       const @(styp) beta, TYPE *C, ATL_CINT ldc);
/*
 * dynamic scheduling overhead makes this a loss on most machines.  I suspect
 * it'll win on Phi, and maybe other extreme-scale machines, so we'll keep it
 * around for now
 */
   #if 0
   if (N <= ATL_sqAMM_LASTNB)
   {
@ROUT ATL_therk
      Mjoin(PATL,therk_tN)(Uplo, Trans, N, K, alpha0, A, lda, beta0, C, ldc);
@ROUT ATL_tsyrk
      Mjoin(PATL,tsyrk_tN)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
@ROUT ATL_tsyrk ATL_therk
      return;
   }
   #endif
   #if 0
   if (Mjoin(PATL,threadMM)(Trans, 
                            (Trans == AtlasNoTrans) ? AtlasTrans:AtlasNoTrans,
                            N, N>>1, K) < 2)
      goto DOSERIAL;
   #endif
   if (N < 1)
      return;
@ROUT ATL_therk `   if (alpha0 == ATL_rzero || K < 1)`
@ROUT ATL_tsyrk `   if (SCALAR_IS_ZERO(alpha) || K < 1)`
   {
@ROUT ATL_therk
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyrk
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyrk ATL_therk
      return;
   }
//   if (Uplo == AtlasLower && N > ATL_sqAMM_LASTNB)
   if (K > 3)
   {
      void Mjoin(PATL,t@(rt)_amm)
         (const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT N,
          ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
          const SCALAR beta, TYPE *C, ATL_CINT ldc);
      Mjoin(PATL,t@(rt)_amm)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
      return;
   }
@ROUT ATL_tsyrk ATL_therk

   nb = ATL_sqAMM_LASTLCMU;
   if (K > (N<<ATL_NTHRPOW2) && (((size_t)N)*N*sizeof(TYPE) <= ATL_PTMAXMALLOC))
   {
      Mjoin(PATL,t@(rt)_K_rec)(Uplo, Trans, N, K, alpha, A, lda, beta, 
                               C, ldc, nb);
@ROUT ATL_therk `      Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
      return;
   }
@beginskip
#if 0 && ATL_NTHREADS == (1<<ATL_NTHRPOW2)
   np = ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
#endif
@endskip
   np = ATL_t@(rt)_M(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
   if (np < 2)
   {
DOSERIAL:
@ROUT ATL_tsyrk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);`
@ROUT ATL_therk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha0, A, lda, beta0, C, ldc);`
      return;
   }
}
@beginskip
/*
 *  Can only use special N-only decomposition of #proc is a power of two
 */
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)

/*
 * Distribute N unless K dominates N, or N is degenerate 
 */
   nblksN = N/nb;
   nblksN = nblksN*nblksN - nblksN;
   if ( ((K+K)/N < N && nblksN >= ATL_NTHREADS+ATL_NTHREADS) || 
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) )
   if ( (N > (nb<<(ATLNTHRPOW2+1))) || N > K ||
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) ||
        ((K>>ATL_NTHRPOW2) < nb && N >= (nb<<ATL_NTHRPOW2-1)) ||
        (N >= K && N > (nb<<(ATL_NTHRPOW2+1))) )
    {
       if (ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                        SADD beta, C, ldc))
          return;
    }
#endif
@endskip
@beginskip
   np = ATL_tsyrkdecomp_K(psyrks, 
      Mjoin(PATL,tv@(rt)), Mjoin(PATL,shift), nb, SADD ZERO, SADD ONE, 
      Uplo, Trans, N, K/nb, K%nb, SADD alpha, A, lda, SADD beta, C, ldc);
   if (np < 2)
      goto DOSERIAL;
   ls.opstruct = (char*) psyrks;
   ls.opstructstride = (int) ( ((char*)(psyrks+1)) - (char*)psyrks );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return;
@ROUT ATL_therk `   Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
   return;

   if (Uplo == AtlasLower && Trans == AtlasNoTrans)
   {
      ATL_init@up@(rt)_t(ATL_NTHREADS, syrks, @(sadd) alpha, @(sadd) beta, 
                        SADD ONE, SADD ZERO, Uplo, Trans, K, A, lda, C, ldc);
      nb = syrks[0].nb;
      np = ATL_tsyrkdecomp_tr(syrks, ATL_NTHREADS, N/nb, N%nb, 0, 0, 0, 0, K, 
                              0, 0, 0, 0, 0, 0);
      if (np < 2 || Mmin(N,K) < 8)
      {
         Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
         return;
      }
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
@skip      ls.CombineOpStructs = NULL;
      ls.DoComb = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
   else
   {
      SortSYRKByFlopCount(np, syrks);
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
      ls.DoComb = NULL;
@skip      ls.CombineOpStructs = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
}
@endskip
@ROUT atlas_tlvl2.h
#ifndef ATLAS_TLVL2_H
   #define ATLAS_TLVL2_H

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl2.h"
#endif
 
#endif          /* end of ifndef ATLAS_TLVL2_H */
@ROUT atlas_tlapack.h
#ifndef ATLAS_TLAPACK_H
   #define ATLAS_TLAPACK_H

@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_lapack.h"

typedef struct
{
   ATL_INT M;       /* matrix rows to distribute across processors */
   ATL_INT N;       /* matrix columns */
   volatile ATL_INT *maxindx;  /* this array starts wt all values -1 */
   volatile ATL_INT *stage;    /* this ptr starts wt all values -1 */
   void *A;
   ATL_INT lda;
   int *ipiv;
   int rank, p, info;
   void *works;    /* ptr to array of ptrs */
} ATL_TGETF2_M_t;

typedef struct
{
   ATL_INT nblks, nr, K1, K2, inci, lda;
   void *A;
   const int *ipiv;
} ATL_TLASWP_N_t;
#endif                  /* end of ifndef ATLAS_TLAPACK_H */
@ROUT atlas_pca.h
#ifndef ATLAS_PCA_H
   #define ATLAS_PCA_H

#include "atlas_misc.h"
#include "atlas_lapack.h"
@beginskip
/*
 * This structure is used to encode block-cyclic index info in ipiv using
 * bitfields.  See src/threads/lapack for routines that use it.
 */
typedef struct ATL_bcpiv ATL_bcpiv_t;
struct ATL_bcpiv
{
   ATL_UINT P;       /* # of procs matrix dim distributed over */
   ATL_UINT N;       /* Size of matrix dimension being distributed over P */
   ATL_UINT B;       /* block factor used to distribute N over P */
   ATL_UINT U;       /* amm's unrolling along the N dim (M for row-piv LU) */
   ATL_UINT nbRNK;   /* # of bits encoding prank of local block owner */
   ATL_UINT nbSBN;   /* # of bits encoding subblock number */
   ATL_UINT nbSBR;   /* # of bits encoding subblock row index */
   ATL_UINT *ipiv;   /* ptr to pivot array being localized */
   ATL_UINT neSB;    /* # of elts in subblock (mu*nu) */
   ATL_UINT neMB;    /* # elts in major block (B*B) */
   void **larrs;     /* ptrs to beginning of local arrays */
};

void *ATL_bcIpivInit(int *ipiv, int P, int N, int B, int mu, int nu, 
                     void *w, int incw);
void ATL_bcIpivEncode(ATL_bcpiv_t *bp, int n, int I, int iadj);
void ATL_bcIpivDecode(ATL_bcpiv_t *bp, int n, int I);
#define ATL_Free_bcpiv_t(m_) free(m_)
@endskip

/*
 * OpenMP provides horrible performance in general, but it is worse than serial
 * for PCA panel factorizations, so turn it off if the user has demanded OpenMP
 */
#ifdef ATL_OMP_THREADS
/*      #define ATL_USEPCA 1 */

/*
 * PowerPCs, POWERs and ARMs are weakly ordered, meaning that a given
 * processor's writes  may appear out-of-order to other processors, 
 * which breaks PCA's syncs since PCA depends on in-order writes.
 * To fix, we must issue a memory barrier call before giving the go-ahead.  
 * PowerPC: SYNC ensures that all prior stores complete before the next one.
 * POWER: DCS waits until all pending writes are written before preceeding
 * ARM: DMB (data mem barrier) - all prior mem accesses (in program order)
 *      complete before DMB returns
 *
 * Older x86's have a special mode where stores can become out-of-order, but
 * it was rarely enabled and does not seem to exist on modern hardware, so
 * we don't have to bother there.
 *
 * SPARCs do not change the order of stores.
 *
 * PowerPC and ARM syncs do not fix problem, so don't allow PCA on machines
 * with out-of-order write schemes.
 */
#elif defined(ATL_ARCH_PPCG4) || defined(ATL_ARCH_PPCG5)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("sync")
/*      #define ATL_USEPCA 1 */
   #endif
#elif defined(ATL_ARCH_POWER3) || defined(ATL_ARCH_POWER4) || \
      defined(ATL_ARCH_POWER5) || defined(ATL_ARCH_POWER6) || \
      defined(ATL_ARCH_POWER7)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("dcs")
/*      #define ATL_USEPCA 1 */
   #endif
/*
 * Unfortunately, none of the memory fence instructions seems to work
 * adequately on ARM
 */
#elif defined(ATL_ARCH_ARM64)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("dmb sy" : : : "memory")
/*      #define ATL_USEPCA 1 */  
   #endif
#elif defined(ATL_ARCH_ARMv7)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("dmb")
/*      #define ATL_USEPCA 1 */  
   #endif
#elif defined(ATL_ARCH_IA64Itan) || defined(ATL_ARCH_IA64Itan2)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("mf")
/*      #define ATL_USEPCA 1 */
   #endif
/*
 * All known x86 machines are strongly-ordered by default (can override
 * on PHI using special instructions).
 */
#elif defined(ATL_GAS_x8664) || defined (ATL_GAS_x8632)
   #define ATL_membarrier
   #define ATL_USEPCA 1
#else
   #define ATL_membarrier
#endif

#endif
@ROUT ATL_bcIpivInit
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
#include "atlas_pca.h"
#include "atlas_bcamm.h"

static ATL_UINT GetBits(ATL_UINT n)
{
   int i;
   if (n <= 32)
   {
      if (n < 2)
         return(1);
      if (n <= 8) /* 2 <= n <= 8 */
      {
         if (n <= 4) /* 2 <= n <= 4 */
         {
            if (n == 2)
               return(1);
            return(2);
         }
         else        /* 5 <= n <= 8 */
            return(3);
      }
      else if (n <= 16)  /* 8 < n <= 16 */
         return(4);
      else               /* 17 <= n <= 32 */
         return(5);
   }
   for (i=6; (1<<i) < n; i++);
   return(i);
}
/*
 * Decompose a global index into the tuple (rank,lbn,sbn,sbr) where:
 *   rank: rank along given dimension of process grid
 *   lbn : local block number (offset to start of amm's block for index)
 *   sbn : offset within local block of C-format's mu x nu subblock
 *   sbr : row offset with mu x nu sub-block
 * Since each of these quantities is possibly not a power of two, encoding
 * in this form may lose up to 3 bits of range.  This still gives range
 * of 268,435,455 even if we don't use the sign bit.  We will never have
 * enough memory to exceed this for square matrices, and in practice we never
 * will for non-square.  If called with very highly non-square above this
 * range, just use ATLAS's native code rather than calling these amm routs.
 */
void *ATL_bcIpivInit
(
   int MN,         /* length of ipiv array */
   int *ipiv,      /* getrf's ipiv array */
   int inci,       /* piv stride; If inci<0, pivs applied in reverse order */
   int R,          /* max # of procs in row dimension of pgrid */
   int C,          /* max # of procs in column dimension of pgrid */
   int N,          /* dimension of distributed data */
   int B,          /* size of block for distributed dimension */
   int mu,         /* amm unrolling in distributed dim (mu) */
   int nu          /* amm unrolling in non-dist dim (nu) */
)
{
   ATL_UINT nbSBR, nbRNK, nbSBN, n;
   ATL_bcpiv_t *bp;
   void *vp;
   void **lwrks;
   ATL_INT *lldps;
   int i;

/*
 * Return failure if there is not room to safely encode rank in pivot array
 * If this ever happens, simply change ipiv to a 64-bit int, and copy back
 * to 32-bit ipiv at end of computation during decode step.
 */
   nbSBR = GetBits(mu);
   nbRNK = GetBits(R);
   nbSBN = GetBits(B/mu);
   n = 32 - nbSBR - nbRNK - nbSBN;
   if ((1<<n) < N)
      return(NULL);
/*
 * Get required space, initialize it, and return
 */
   bp = malloc(C*sizeof(ATL_bcpiv_t) + MN*sizeof(int) + 
                  R*C*(sizeof(void*)+sizeof(int))+ATL_Cachelen);
   if (!bp)
      return(NULL);
   bp->ipiv = (ATL_UINT*)(bp+C); /* for local copy of ipiv0 */
   vp = bp->ipiv + MN;
   lwrks = ATL_AlignPtr(vp);
   lldps = (ATL_INT*)(lwrks+(R*C));

   for (i=0; i<C; i++)
   {
      bp[i].R = R;
      bp[i].C = C;
      bp[i].B = B;
      bp[i].MU = mu;
      bp[i].NU = nu;
      bp[i].neSB = mu*nu;
      bp[i].neMB = B*B;
      bp[i].ipiv = bp->ipiv;
      bp[i].ipiv0 = ipiv;
      bp[i].inci = inci;
      bp[i].nbSBR = nbSBR;
      bp[i].nbRNK = nbRNK;
      bp[i].nbSBN = nbSBN;
      bp[i].larrs = lwrks + (i*R);
      bp[i].lldps = lldps + (i*R);
   }
   return(bp);
}
@ROUT ATL_bcIpivEncode
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
#include "atlas_misc.h"
#include "atlas_pca.h"
#include "atlas_bcamm.h"
/*
 * ===========================================================================
 * Takes ipiv with lapack-style global row indices, and translates it into
 * block-cyclic friendly tuple  (rank,lbn,sbn,sbr):
 *   rank: prow owning the row the max was found at
 *   lbn: local block number the row is found in
 *   sbn: sub-block in C-major storage row is found in
 *   sbr: sub-block row offset 
 * With this tuple, we can compute indxl2g by:
 *   lbn*P*B + rank*B + sbn*mu+sbr
 * And we can determine this local location in the C-format column by:
 *   start = workspace of rank
 *   li = lbn*mb*nb + sbn*mu*nu+sbr
 * Note P (# of procs), B (nb=mb), mu, nu are all stored in bcipiv_t struct.
 *
 * After encoding, ipiv looks like (low order bits on right):
 *   {lbn,sbn,sbr,rank} --> of len --> {32-x, nbSBN, nbSBR, nbRNK}
 *
 *
 * NOTE ON SAFETY OF STORING TUPLE IN ORIGINAL IPIV:
 * -------------------------------------------------
 * For now we store this back in the original ipiv.  In the future we may
 * need to copy it to an N-length 64-bit array, but it should be safe for
 * now.  Note that lbn, sbn, sbr are all indexing the same range as N,
 * so at most we would lose 2 bits of range from this subpartioning
 * (we lose no bits of range if all parts are powers-of-two).  However,
 * the owner rank bits are extra storage, so this is not safe in theory,
 * but it should be in practice.
 * In practice, roughly 1024 cores is the right order to assume for
 * reasonably short term, and this routine is specifically for LU, where
 * we want R<<C (RxC pgrid).  So, this would suggest 32 or 5 bits as 
 * a worst-case range loss.  We gain one bit by making unsigned, which
 * gives us 32-5-2+1 = 26 bits of range.  This yields a max N of 65,536
 * for the reasonably foreseeable future.  On current machines with O(100)
 * cores, more like : 32-3-1= 28 = 26,843,546 which is far larger than
 * we can malloc (we have to copy the matrix for this algorithm).
 * So, for now we will return NULL if out of bits (allowing us to safely use
 * ATLAS recursive algorithm, which should be fine performance-wise for
 * asymptotically large problems).  Running out of range is not likely to be
 * a problem before machine evolution forces another threading rewrite.
 * ===========================================================================
 */
/*
 * NOTES:
 * (1) Assumes that all n pivots are in the same block as I.  In practice:
 *     n == nb (< nb for partial blk at end)
 *     I%nb == 0
 * (2) Only the nb rows of IPIV starting at I are encoded.  The destination
 *     rows must be given global index to avoid double encoding!
 */
void ATL_bcIpivEncode
(
   ATL_bcpiv_t *bp,/* block-cyclic pivot ptr returned by init */
   int n,          /* number of pivot entries to encode */
   int I,          /* ipiv index to start the encoding at */
   int iadj        /* amount to adjust pivot entries by */
)
{
   ATL_UINT nprior, k;
   ATL_UINT *ipiv0 = bp->ipiv0 + I;
   ATL_UINT *ipiv = bp->ipiv + I;
   ATL_CUINT P=bp->R, MU=bp->MU, B=bp->B;
   ATL_CUINT nbSBR=bp->nbSBR, nbRNK=bp->nbRNK, nbSBN=bp->nbSBN;
   ATL_CUINT nb2 = nbSBR+nbRNK, nb3=nb2+nbSBN;
   
   for (k=0; k < n; k++)
   {
      ATL_CUINT d = ipiv0[k]+iadj;    /* global destination row index */
      ATL_CUINT gbn = d/B;           /* global blk num for d'th elt (dst row) */
      ATL_CUINT lbn =  gbn/P;        /* local block # of dest row */
      ATL_CUINT rank = gbn-(lbn)*P;  /* prow of Ith mat row */
      ATL_CUINT boff = d - gbn*B;    /* row within block we want */
      ATL_CUINT sbn = boff/MU;       /* sub-block number of dest row */
      ATL_CUINT sbr = boff - sbn*MU; /* row indx within sub-block of dest row */
/*
 *    Encoding:{lbn,sbn,sbr,rank} --> of len --> {32-x, nbSBN, nbSBR, nbRNK}
 */
      ipiv[k] = (lbn<<nb3) | (sbn<<nb2) | (sbr<<nbRNK) | rank;
   }
}
@ROUT ATL_bcIpivDecode
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
#include "atlas_misc.h"
#include "atlas_pca.h"
#include "atlas_bcamm.h"
/*
 * Takes ipiv split in (rank,lbn, sbn,sbr) bitpattern, and translates it back
 * to standard getrf global row indices.  See ATL_tipivEncode for more details.
 */
void ATL_bcIpivDecode
(
   ATL_bcpiv_t *bp,/* block-cyclic pivot ptr returned by init */
   int n,          /* number of pivot entries to decode */
   int I           /* ipiv index to start the decoding at */
)
{
   ATL_UINT nprior, k;
   ATL_UINT *ipiv = bp->ipiv + I;
   ATL_CUINT P=bp->R, MU=bp->MU, B=bp->B;
   ATL_CUINT nbSBR=bp->nbSBR, nbRNK=bp->nbRNK, nbSBN=bp->nbSBN;
   ATL_CUINT mskSBR=(1<<nbSBR)-1, mskRNK=(1<<nbRNK)-1, mskSBN=(1<<nbSBN)-1;
   
   for (k=0; k < n; k++)
   {
      ATL_UINT rank, sbn, sbr, lbn = ipiv[k];
      rank = lbn & mskRNK;
      lbn >>= nbRNK;
      sbr = lbn & mskSBR;
      lbn >>= nbSBR;
      sbn = lbn & mskSBN;
      lbn >>= nbSBN;
/*
 *    Scalapack's indxl2g with srcproc=0
 */
      ipiv[k] = lbn*P*B + rank*B + sbn*MU+sbr;
   }
}
@ROUT ATL_bcLaswp_amm
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
#include "atlas_misc.h"
#include "atlas_pca.h"
#include "atlas_bcamm.h"
void Mjoin(PATL,bcLaswp_amm)
(
   ATL_bcpiv_t *bp,
   ATL_CINT N,     /* row-length to swap */
   ATL_CINT coff,  /* column offset to enable partial swap */
   TYPE *A,        /* column-major matrix holding top block */
   ATL_CINT lda,   /* stride between row elts in A */
   ATL_CINT K1,    /* First elt of ipiv for which a swap will be done */
   ATL_CINT K2,    /* Last elt of ipiv for which a row intrchg will be done */
   ATL_CINT lpj,   /* local panel no. to apply pivots on */
   ATL_CINT nnu,   /* no. of nus in current panel */
   ATL_CINT ZOFF   /* offset for complex block from real */
)
{
   int i, i1, i2, KeepOn, ii;
   void **larrs = bp->larrs;
   int* lldps = bp->lldps;
   ATL_UINT *ipiv = bp->ipiv;
   ATL_CINT inci = bp->inci;
   ATL_CUINT P=bp->R, B=bp->B, MU=bp->MU; 
   ATL_CUINT nbRNK=bp->nbRNK, nbSBN=bp->nbSBN, nbSBR=bp->nbSBR;
   ATL_CUINT BB=bp->neMB, PB = P*B, NN=nnu*bp->neSB;
   ATL_CUINT mskSBR=(1<<nbSBR)-1, mskRNK=(1<<nbRNK)-1, mskSBN=(1<<nbSBN)-1;
   ATL_CUINT cmaj_off = coff*MU;
   #ifdef TCPLX
      ATL_CUINT lda2 = lda+lda;
   #endif
   if (K2 < K1)
      return;
   if (inci >= 0)
   {
      ipiv += K1*inci;
      i1 = K1;
      i2 = K2-1;
   }
   else
   {
      ipiv -= (K2-1) * inci;
      i1 = K2-1;
      i2 = K1;
   }
   A += coff*(lda SHIFT);  /* adjust for partial block swap */
   i = i1;
   ii = 0;
   do
   {
      ATL_UINT rank, sbn, sbr, lbn = ipiv[0];
      ATL_SZT k;
/*
 *    Decode ipiv entry
 */
      rank = lbn & mskRNK;   /* rank of row owner */
      lbn >>= nbRNK;
      sbr = lbn & mskSBR;    /* subblock row */
      lbn >>= nbSBR;
      sbn = lbn & mskSBN;    /* subblock number */
      lbn >>= nbSBN;         /* local block number */
      k = lbn*PB + rank*B + sbn*MU+sbr;  /* indxl2g wt psrc=0 */
      if (k < K2)
         Mjoin(PATL,swap)(N, A+(ii SHIFT), lda, A+((k-K1) SHIFT), lda);
      else  /* (rank,lbn,sbn,sbr) */
      {
         TYPE *w;
         k = ((lbn*BB) SHIFT)+sbn*NN+sbr;
         w = larrs[rank];
         w += ((lpj*lldps[rank]) SHIFT);  /* move to the correct panel */
         w += cmaj_off;                   /* adjust for partial block */
         w += k;
         #ifdef TCPLX
            Mjoin(PATL,swap_cplx2real)(N, A+ii+ii, lda, w, MU, w+ZOFF, MU);
         #else
            Mjoin(PATL,swap)(N, A+ii, lda, w, MU);
         #endif
      }
      ipiv += inci;
      ii++;
      if (inci >=0)
         KeepOn = (++i <= i2);
      else
         KeepOn = (--i >= i2);
   }
   while(KeepOn);
}
@ROUT ATL_tgetf2
#include "atlas_cbc.h"
#include "atlas_tlapack.h"
#include "atlas_level2.h"
#ifdef TCPLX
   #define GER Mjoin(PATL,geru)
#else
   #define GER Mjoin(PATL,ger)
#endif
int Mjoin(PATL,tgetf2C)
(
   ATL_CUINT P,   /* number of threads working on this problem */
   ATL_CUINT rank,/* rank of this thread */
   void *vchk,    /* boolean sync array */
   ATL_CUINT M,   /* number of rows in A;  M >= P*N for parallelism! */
   ATL_CUINT N,   /* number of cols in A */
   TYPE *A,       /* IN: matrix to factor; OUT: LU */
   size_t lda1, /* element stride between cols of A */
   int *ipiv      /* pivot array */
)
{
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
      size_t lda2 = lda1+lda1;
   #else
      #define none ATL_rnone
      #define lda2 lda1
   #endif
   int info=0;
/*
 * Rank 0 owns the diagonal, so these cores will just perform updates as
 * driven by rank 0
 */
   ATL_INT j, mp, mr;
   ATL_CUINT Mp = M / P;     /* # of rows all cores get */
   ATL_CUINT Mr = M - Mp*P;  /* extra rows left over */
/*
 * Quick return if no work to do
 */
   if (M < 1 || N < 1)
      return(0);
/*
 * For parallism, the least number of local rows must be larger than N; i.e.
 * i.e., the whole diagonal block must be owned by the rank 0.
 */
   if (Mp < N)
      return(Mjoin(PATL,getf2)(M, N, A, lda1, ipiv));
/*
 * All ranks except 0 have only trailing matrix to worry about, so their
 * local row count never changes, they don't have a diagonal, etc.  
 */
   if (rank)
   {
      ATL_CUINT Mprev = Mp*rank + Mmin(rank-1, Mr);
      TYPE *Ac=A+(Mprev SHIFT);
      ATL_CUINT ML = (rank <= Mr) ? Mp+1 : Mp;

      for (j=0; j < N; j++, Ac += lda2)
      {
         #ifdef TCPLX
            TYPE pv[2];
         #else
            TYPE pv;
         #endif
         int pivL, pivG;
         pivL = cblas_iamax(ML, Ac, 1);
         pivG = pivL + Mprev;
         #ifdef TCPLX
            pv[0] = Ac[pivL+pivL];
            pv[1] = Ac[pivL+pivL+1];
            Mjoin(PATL,cbc_comb_iamax_nopost0)(P, rank, &pivG, pv, vchk);
         #else
            pv = Ac[pivL];
            Mjoin(PATL,cbc_comb_iamax_nopost0)(P, rank, &pivG, &pv, vchk);
         #endif
         #ifdef TCPLX
            if (pv[0] != ATL_rzero || pv[1] != ATL_rzero)
            {
               Mjoin(PATL,cplxinvert)(1, pv, 1, pv, 1);
               cblas_scal(ML, pv, Ac, 1);
            }
         #else
            if (pv != ATL_rzero)
               cblas_scal(ML, ATL_rone/pv, Ac, 1);
         #endif
            else if (!info)
               info = j+1;
         GER(ML, N-j-1, none, Ac,1, A+(((j+1)*lda1+j)SHIFT),lda1, Ac+lda2,lda1);
      }
   }
/*
 * Rank 0 owsn the diagonal block and so it will drive the algorithm,
 * and do pivoting and solving.
 */
   else
   {
      size_t ldap1 = (lda1+1)SHIFT;
      TYPE *Ac=A, *Ad=A;
      int m = Mp;

      for (j=0; j < N; j++, Ac += lda2, Ad += ldap1)
      {
         #ifdef TCPLX
            TYPE pv[2];
         #else
            TYPE pv;
         #endif
         int pivL, pivG;
         int bval;
         pivL = cblas_iamax(m--, Ad, 1);
         pivG = j + pivL;
         #ifdef TCPLX
            pv[0] = Ad[pivL+pivL];
            pv[1] = Ad[pivL+pivL+1];
            bval = Mjoin(PATL,cbc_comb_iamax_nopost0)(P, 0, &pivG, pv, vchk);
         #else
            pv = Ad[pivL];
            bval = Mjoin(PATL,cbc_comb_iamax_nopost0)(P, 0, &pivG, &pv, vchk);
         #endif
//         printf("pivG=%d\n", pivG);
         ipiv[j] = pivG;
         if (pivG != j)
            cblas_swap(N, A+(j SHIFT), lda1, A+(pivG SHIFT), lda1);
         ATL_cbc_post(0, vchk);  /* allow other cores to leave iamax! */
         #ifdef TCPLX
            if (pv[0] != ATL_rzero || pv[1] != ATL_rzero)
            {
               Mjoin(PATL,cplxinvert)(1, pv, 1, pv, 1);
               cblas_scal(m, pv, Ad+2, 1);
            }
         #else
            if (pv != ATL_rzero)
               cblas_scal(m, ATL_rone/pv, Ad+1, 1);
         #endif
            else if (!info)
               info = j+1;
         GER(m, N-j-1, none, Ad+(1 SHIFT), 1, Ad+lda2, lda1, Ad+ldap1, lda1);
      }
   }
   return(info);
}
#undef GER
#ifdef none
   #undef none
#endif
#ifdef lda2
   #undef lda2
#endif
#if 0  /* untested code! */
int Mjoin(PATL,tgetf2CR)
(
   ATL_CUINT P,   /* number of threads working on this problem */
   ATL_CUINT rank,/* rank of this thread */
   void *vchk,    /* boolean sync array */
   ATL_CUINT M,   /* number of rows in A;  M >= P*N for parallelism! */
   ATL_CUINT N,   /* number of cols in A */
   TYPE *A,       /* IN: matrix to factor; OUT: LU */
   size_t lda,    /* element stride between cols of A */
   int *ipiv      /* pivot array */
)
{
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
      const TYPE one[2]  = {ATL_rone, ATL_rzero};
      size_t lda2 = lda+lda;
   #else
      #define one ATL_rone
      #define none ATL_rnone
      #define lda2 lda
   #endif
   int ierr=0;
   ATL_CUINT Nleft = (N>>2)<<1;
   if (Nleft)
   {
       ATL_CUINT Nright=N-Nleft, NRp = Nright/P, NRpr = Nright-NRp*P;
       ATL_CUINT nrp = (rank < NRpr) ? NRp+1: NRp;
       ATL_CUINT nrprev = (NRp*rank + Mmin(rank, NRpr)) SHIFT;
       ATL_CUINT MN = Mmin(M,N);
       ATL_INT MM, Mp;
       TYPE *Ac, *An;
       int i;
/*
 *     Factor left portion, the apply pivots to right, then sync so we
 *     we know it is save to start writing TRSM on pivoted matrix
 */
       ierr = Mjoin(PATL,tgetf2CR)(P, rank, vchk, M, N, A, lda, ipiv);
       Ac = A + Nleft*lda2;
       if (nrp)
          ATL_laswp(nrp, Ac+nrprev, lda, 0, Nleft, ipiv, 1);
       An = Ac + (Nleft SHIFT);
       ATL_barrier();
       ATL_membar;   /* core must 'see' all changes made above! */
/*
 *     If local NRHS < 4, reduce parellism to avoid L1BLAS perf with contention
 */
       if (NRp < 4)
       {
          ATL_CUINT p4 = (Nright > 4) ? (Nright>>2) : 1;
          if (rank < p4)
          {
             if (p4 > 1)
             {
                ATL_CUINT np = Nright/p4, nr = Nright-np*p4;
                ATL_CUINT n = (rank < nr) ? np+1: np;
                ATL_CUINT prev = (np*rank + Mmin(rank, nr)) SHIFT;
                cblas_trsm(CblasColMajor, CblasLeft, CblasLower, CblasNoTrans, 
                           CblasUnit, Nleft, n, one, A, lda, Ac+prev, lda);
             }
             else
                cblas_trsm(CblasColMajor, CblasLeft, CblasLower, CblasNoTrans, 
                           CblasUnit, Nleft, Nright, one, A, lda, Ac, lda);
          }
       }
       else
          cblas_trsm(CblasColMajor, CblasLeft, CblasLower, CblasNoTrans, 
                     CblasUnit, Nleft, nrp, one, A, lda, Ac+nrprev, lda);
/*
 *     Await completion of solve, and update with GEMM by splitting M.
 *     This algorithm really designed for panel factorization, where M
 *     dominates N.  Would not be good for problems where M is very small,
 *     and N very large!  Can add case to that affect if it becomes important.
 */
       MM = M - Nleft;
       Mp = MM / P;
       ATL_barrier();
       ATL_membar;   /* core must 'see' all changes made above! */
       if (Mp >= 4)
       {
          ATL_CUINT mr = M-Mp*P, m = (rank < mr) ? Mp+1: Mp;
          ATL_CUINT prev = (Mp*rank + Mmin(rank, mr)) SHIFT;
          cblas_gemm(CblasColMajor, CblasNoTrans, CblasNoTrans, m, Nright,
                     Nleft, none, A+prev+(Nleft SHIFT), lda, 
                     Ac+prev, lda, one, An, lda);
       }
       else if (rank == P-1)
          cblas_gemm(CblasColMajor, CblasNoTrans, CblasNoTrans, MM, Nright,
                     Nleft, none, A+(Nleft SHIFT), lda, Ac, lda, one, An, lda);
/*
 *     Await completion of GEMM and then factor the right
 */
       ATL_barrier();
       ATL_membar;   /* core must 'see' all changes made above! */
       i = Mjoin(PATL,tgetf2CR)(P, rank, vchk, MM, Nright, An, lda, ipiv+Nleft);
/*
 *     Must use a special laswp rout that adds 1st arg to all piv entries.
 *     this avoids having a sync for update of ipiv, which is handled last,
 *     when we are forced to sync anyway
 * HERE HERE: still need to write this rout, and then parallelize this call!
 */

       ATL_laswp_off(Nleft, A, lda, Nleft, MN, ipiv, 1);
       if (i) if (!ierr) ierr = i + Nleft;
       if (rank == P-1)
          for (i=Nleft; i != MN; i++)
             ipiv[i] += Nleft;
   }
   else ierr = Mjoin(PATL,tgetf2C)(P, rank, vchk, M, N, A, lda, ipiv);
/*
 * Not safe to use data until everybody sees changes, so don't ret until done!
 */
   ATL_barrier();
   ATL_membar;   /* core must 'see' all changes before returning */
   return(ierr);
}
#ifdef one
   #undef one
#endif
#ifdef none
   #undef none
#endif
#ifdef lda2
   #undef lda2
#endif
#endif
@ROUT ATL_tgetf2
@skip #include "atlas_tlapack.h"
#include "atlas_pca.h"
@skip #include "atlas_level2.h"
@skip #include "atlas_cbc.h"

void Mjoin(PATL,DoWorkGETF2_nowrk)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i;
   #ifdef TCPLX
      ATL_CINT lda2 = lda+lda;
   #else 
      #define lda2 lda
      #define none ATL_rnone
   #endif
   TYPE *A, *Ac, *a, *v;
   TYPE pivval, apv, apv2;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   void (*my_ger)(const int M, const int N, const SCALAR alpha, 
                  const TYPE *X, const int incX, 
                  const TYPE *Y, const int incY, TYPE *A, const int lda);

   #if 1
      if (M >= N*p && p > 1)
      {
         int ierr;
         ierr = Mjoin(PATL,tgetf2C)(p, rank, NULL, M, N, lup->A, lda, ipiv);
         if (ierr)
            lup->info = ierr;
         return;
      }
   #endif
   #ifdef TCPLX
      my_ger = Mjoin(PATL,geru);
   #else
      my_ger = Mjoin(PATL,ger);
   #endif
   m = (rank) ? mp : mp+mr;
   Ac = A = lup->A;
   a = (rank) ? A + ((m*rank + mr)SHIFT) : A;
   for (j=0; j < MN; j++, Ac += lda2, a += lda2)
   {
      locpiv = cblas_iamax(m, a, 1);
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         #ifdef TCPLX
            apv = Mabs(Ac[globpiv+globpiv]) + Mabs(Ac[globpiv+globpiv+1]);
         #else
            apv = Mabs(Ac[globpiv]);
         #endif
         #if 1
            Mjoin(PATL,cbc_comb_iamax_nopost0)(p, rank, &globpiv, &apv, NULL);
         #else
         for (i=1; i < p; i++)
         {
            while(stage[i] < j);
            k = maxindx[i];
            apv2 = Mabs(Ac[k SHIFT]);
            #ifdef TCPLX
               apv2 += Mabs(Ac[k+k+1]);
            #endif
            if (apv < apv2)
            {
               apv = apv2;
               globpiv = k;
            }
            maxindx[i] = -1;
         }
         #endif
         ipiv[j] = globpiv;
//         printf("globpiv=%d\n", globpiv);
         if (globpiv != j)
            cblas_swap(N, A+(j SHIFT), lda, A+(globpiv SHIFT), lda);
@skip         ATL_membar;
         ATL_cbc_post(rank, NULL);
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            a += 2;                                     /* one row */
         #else
            a++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         #ifdef TCPLX
            apv = Mabs(a[locpiv+locpiv]) + Mabs(a[locpiv+locpiv+1]);
         #else
            apv = Mabs(a[locpiv]);
         #endif
         #if 1
            globpiv = locpiv+rank*mp+mr;
            Mjoin(PATL,cbc_comb_iamax_nopost0)(p, rank, &globpiv, &apv, NULL);
         #else
         maxindx[rank] = locpiv+rank*mp+mr;
         stage[rank] = j;
         while (stage[0] < j);
         #endif
      }
      #ifdef TCPLX
         if (Ac[j+j] != ATL_rzero || Ac[j+j+1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, Ac+j+j, 1, inv, 1);
            cblas_scal(m, inv, a, 1);
         }
      #else
         pivval = Ac[j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, a, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         my_ger(m, N-j-1, none, a, 1, Ac+((j+lda)<<1), lda, a+lda2, lda);
         my_ger = Mjoin(PATL,geru_L2);
      #else
         my_ger(m, N-j-1, ATL_rnone, a, 1, Ac+j+lda, lda, a+lda, lda);
         my_ger = Mjoin(PATL,ger_L2);
      #endif
   }
}

void Mjoin(PATL,DoWorkGETF2)(ATL_LAUNCHSTRUCT_t *lp, void *vp0)
{
   ATL_thread_t *tp=vp0;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   int pivrank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i, ldw, ldw0, ldw1;
   void *vp;
   TYPE *a, *W, *Wc, *w, **WRKS=lup->works, *v;
   TYPE pivval, apv, apv2, pv2;
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif

   m = (rank) ? mp : mp+mr;
   a = (rank) ? (((TYPE*)lup->A)+((mp*rank + mr)SHIFT)) : lup->A;
/*
 * Make ldw's a multiple of 16 bytes that is not a power of 2; 0's ldw 
 * is larger by mr than all other ldws (ldw1)
 */
#if defined(DREAL) || defined(SCPLX)
   ldw0 = ((mp+mr+1)>>1)<<1;
   ldw1 = ((mp+1)>>1)<<1;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 2;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 2;
#elif defined(SREAL)
   ldw0 = ((mp+mr+3)>>2)<<2;
   ldw1 = ((mp+3)>>2)<<2;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 4;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 4;
#else
   ldw0 = mp+mr;
   ldw1 = mp;
   if (!(ldw0 & (ldw0-1)))
      ldw0++;
   if (!(ldw1 & (ldw1-1)))
      ldw1++;
#endif
   ldw = (rank) ? ldw1 : ldw0;
   vp = malloc(ATL_MulBySize(ldw)*N+ATL_Cachelen);
/*
 * If anyone fails to allocate the space, free any allocated spaces and
 * call the no-copy version
 */
   j = (vp != NULL);
   if (!rank)
   {
      for (i=1; i < p; i++)
      {
         while (stage[i] != -2);
         j &= maxindx[i];
         maxindx[i] = -1;
      }
      *maxindx = j;
      stage[0] = -2;
   }
   else
   {
      maxindx[rank] = j;
      stage[rank] = -2;
      while (stage[0] != -2);
   }
   if (*maxindx == 0)
   {
      if (vp)
         free(vp);
      Mjoin(PATL,DoWorkGETF2_nowrk)(lp, vp0);
      return;
   }
   ATL_assert(vp);
   WRKS[rank] = w = W = ATL_AlignPtr(vp);
   Mjoin(PATL,gecopy)(m, N, a, lda, W, ldw);
   for (j=0; j < MN; j++, w += (ldw SHIFT))
   {
      locpiv = cblas_iamax(m, w, 1);
      apv = Mabs(w[locpiv SHIFT]);
      #ifdef TCPLX
         apv += Mabs(w[locpiv+locpiv+1]);
      #endif
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         Mjoin(PATL,cbc_comb_iamax_nopost0)(p, rank, &globpiv, &apv, NULL);
         pivrank = (globpiv > mp+mr) ? (globpiv - mr) / mp : 0;
         ipiv[j] = globpiv;
         if (pivrank)
         {
            locpiv = globpiv-mr-pivrank*mp;
            cblas_swap(N, W+(j SHIFT), ldw, 
                       WRKS[pivrank]+(locpiv SHIFT), ldw1);
         }
         else
         {
            if (globpiv != j)
               cblas_swap(N, W+(j SHIFT), ldw, W+(globpiv SHIFT), ldw);
         }
         ATL_cbc_post(rank, NULL);
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            w += 2;                                     /* one row */
         #else
            w++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         globpiv = mr+rank*mp+locpiv;
         Mjoin(PATL,cbc_comb_iamax_nopost0)(p, rank, &globpiv, &apv, NULL);
      }
      #ifdef TCPLX
         v = &WRKS[0][(j*ldw0+j)SHIFT];
         if (*v != ATL_rzero || v[1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, v, 1, inv, 1);
            cblas_scal(m, inv, w, 1);
         }
      #else
         pivval = WRKS[0][j*ldw0+j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, w, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         Mjoin(PATL,geru_L2)(m, N-j-1, none, w, 1, 
                             WRKS[0]+((j*(ldw0+1)+ldw0)SHIFT), ldw0, 
                             w+ldw+ldw, ldw);
      #else
         Mjoin(PATL,ger_L2)(m, N-j-1, ATL_rnone, w, 1, WRKS[0]+j*(ldw0+1)+ldw0,
                            ldw0, w+ldw, ldw);
      #endif
   }
   stage[rank] = MN;  /* let core 0 know we are done */
/*
 * Copy answer back out of workspace and then free workspace
 */
   Mjoin(PATL,gecopy)(rank?mp:mp+mr, N, W, ldw, a, lda);
/*
 * Core 0 waits for all other cores to finish before he frees his work:
 * all non-zero cores access 0's workspace, but 0 does not access others' work
 * after iamax barrier
 */
   if (!rank)
   {
      for (i=1; i < p; i++)
         while(stage[i] != MN);
   }
   free(vp);
}

int Mjoin(PATL,StructIsInitGETF2)(void *vp)
{
   return(((ATL_TGETF2_M_t*)vp)->M);
}

@multidef trt Mjoin(PATL,DoWorkGETF2) Mjoin(PATL,DoWorkGETF2_nowrk)
@define pf @@
@whiledef pf _nocp
int Mjoin(PATL,tgetf2@(pf))(ATL_CINT M, ATL_CINT N, TYPE *A, ATL_CINT lda, int *ipiv)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TGETF2_M_t lu2s[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_INT maxindx[ATL_NTHREADS], stage[ATL_NTHREADS];
   TYPE *works[ATL_NTHREADS];

   ATL_CINT MN = Mmin(M,N);
   ATL_INT p = ATL_NTHREADS, m, mr, i, j;

   if (M < 1 || N < 1)
      return(0);
   m = M / ATL_NTHREADS;
   mr = M - m*ATL_NTHREADS;
/*
 * This logic is necessary since tgetf2 assumes only one processor owns entire
 * logical block.  Can remove if we rewrite tgetf2 to allow the diagonal to
 * span multiple processors
 */
   if (m+mr < N)
   {
      p = M / N;
      if (p)
         m = M / p;
   }
   if (p < 2)   /* not enough rows, call serial algorithm */
      return(Mjoin(PATL,getf2)(M, N, A, lda, ipiv));
   for (i=0; i < p; i++)
   {
      stage[i] = maxindx[i] = -1;
@skip      j = ATL_launchorder[i];
      lu2s[i].M = M;
      lu2s[i].N = N;
      lu2s[i].A = A;
      lu2s[i].lda = lda;
      lu2s[i].ipiv = ipiv;  /* only thread 0 will write ipiv */
      lu2s[i].info = 0;
      lu2s[i].maxindx = maxindx;
      lu2s[i].stage = stage;
      lu2s[i].p = p;
      lu2s[i].rank = i;
      lu2s[i].works = works;
   }
   for (; i < ATL_NTHREADS; i++)
      lu2s[i].M = 0;
   ATL_goparallel(p, @(trt), lu2s, NULL);
@beginskip
   ls.opstruct = (char*) lu2s;
   ls.opstructstride = (int) ( ((char*)(lu2s+1)) - (char*)(lu2s) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInitGETF2);
@skip   ls.DoWork = Mjoin(PATL,DoWorkGETF2);
   ls.DoWork = @(trt);
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(lu2s[0].info);
}
   @undef trt
@endwhile
#ifndef TCPLX
   #undef lda2
#endif
@ROUT ATL_tlaswp
#include "atlas_tlapack.h"
#include "atlas_pca.h"

void Mjoin(PATL,DoWorkLASWP)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TLASWP_N_t *swpp=((ATL_TLASWP_N_t*)lp->opstruct)+tp->rank;
   const int *piv=swpp->ipiv, *ipiv;
   ATL_CINT nr=swpp->nr, K1=swpp->K1, K2=swpp->K2, inci=swpp->inci;
   ATL_INT nblks=swpp->nblks;
   TYPE *A = swpp->A;
   size_t lda = (swpp->lda)SHIFT;
   const int n = K2 - K1;
   size_t incA = lda << 5;
   int i, ip, i1, i2, KeepOn;
   register int h;
   TYPE *a0, *a1;
   #ifdef TCPLX
      register TYPE r0, r1;
   #else
      register TYPE r;
   #endif

   if (K2 < K1) return;
   if (inci < 0)
   {
      piv -= (K2-1) * inci;
      i1 = K2-1;
      i2 = K1;
   }
   else
   {
      piv += K1*inci;
      i1 = K1;
      i2 = K2-1;
   }

   if (nblks)
   {
      do
      {
         ipiv = piv;
         i = i1;
         do
         {
            ip = *ipiv; ipiv += inci;
            if (ip != i)
            {
               a0 = A + (i SHIFT);
               a1 = A + (ip SHIFT);
               for (h=32; h; h--)
               {
                  #ifdef TCPLX
                     r0 = *a0;
                     r1 = a0[1];
                     *a0 = *a1;
                     a0[1] = a1[1];
                     *a1 = r0;
                     a1[1] = r1;
                  #else
                     r = *a0;
                     *a0 = *a1;
                     *a1 = r;
                  #endif
                  a0 += lda;
                  a1 += lda;
               }
            }
            if (inci > 0) KeepOn = (++i <= i2);
            else KeepOn = (--i >= i2);
         }
         while(KeepOn);
         A += incA;
      }
      while(--nblks);
   }
   if (nr)
   {
      ipiv = piv;
      i = i1;
      do
      {
         ip = *ipiv; ipiv += inci;
         if (ip != i)
         {
            a0 = A + (i SHIFT);
            a1 = A + (ip SHIFT);
            for (h=nr; h; h--)
            {
               #ifdef TCPLX
                  r0 = *a0;
                  r1 = a0[1];
                  *a0 = *a1;
                  a0[1] = a1[1];
                  *a1 = r0;
                  a1[1] = r1;
               #else
                  r = *a0;
                  *a0 = *a1;
                  *a1 = r;
               #endif
               a0 += lda;
               a1 += lda;
            }
         }
         if (inci > 0) KeepOn = (++i <= i2);
         else KeepOn = (--i >= i2);
      }
      while(KeepOn);
   }
}

void Mjoin(PATL,tlaswp)(const int N, TYPE *A, const int lda, const int K1,
                        const int K2, const int *ipiv, const int inci)
{
   ATL_TLASWP_N_t swps[ATL_NTHREADS];
   ATL_INT i, p = ATL_NTHREADS, nblks, nlblks, neblks, nr, n;
   TYPE *ac;
   if (N < 128)
   {
      Mjoin(PATL,laswp)(N, A, lda, K1, K2, ipiv, inci);
      return;
   }
   nblks = N >> 5;
   nr = N - (nblks<<5);
   if (nblks < p)
   {
      p = nblks;
      nlblks = 1;
      neblks = 0;
   }
   else
   {
      nlblks = nblks / p;
      neblks = nblks - nlblks*p;
   }
   ac = A;
   for (i=0; i < p; i++)
   {
      size_t n;
      n = swps[i].nblks = (i < neblks) ? nlblks+1 : nlblks;
      swps[i].nr = (i == neblks) ? nr : 0;
      n = (n<<5)+swps[i].nr;
      swps[i].A = ac;
      swps[i].K1 = K1;
      swps[i].K2 = K2;
      swps[i].ipiv = ipiv;
      swps[i].inci = inci;
      swps[i].lda = lda;
      ac += (((size_t)lda)SHIFT)*n;
   }
   ATL_goparallel(p, Mjoin(PATL,DoWorkLASWP), swps, NULL);
}
@ROUT ATL_ger_L2
#include "atlas_misc.h"
#include "atlas_lvl2.h"
#include "atlas_lvl3.h"
// #include "atlas_r1.h"


void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
     ATL_CINT incY1, TYPE *A, ATL_CINT lda1);

#ifdef TREAL
   #define ATL_ger Mjoin(PATL,ger_L2)
   #define ATL_ger1 Mjoin(PATL,ger1k)
#else
   #ifdef Conj_
      #define ATL_ger Mjoin(PATL,gerc)
   #else
      #define ATL_ger Mjoin(PATL,geru)
      #define ATL_ger1 Mjoin(PATL,ger1u_a1_x1_yX)
   #endif
#endif
void ATL_ger(const int M, const int N, const SCALAR alpha,
             const TYPE *X, const int incX, const TYPE *Y, const int incY,
             TYPE *A, const int lda)
{
   int imb, mb, mb0, m=M, i;
   int incy=incY;
   #ifdef TREAL
      #define one ATL_rone
   #else
      static TYPE one[2] = {ATL_rone, ATL_rzero};
   #endif
   void *vx=NULL;
   size_t Aa, Ax;
   TYPE *x, *y = (TYPE*) Y;
   void (*getX)(const int N, const SCALAR alpha, const TYPE *X,
                const int incX, TYPE *Y, const int incY);
   #ifdef Conj_
      void (*ATL_ger1)(const int M, const int N, const SCALAR alpha,
                       const TYPE *X, const int incX, const TYPE *Y,
                       const int incY, TYPE *A, const int lda);
      ATL_ger1 = Mjoin(PATL,ger1c_a1_x1_yX);
   #endif

   if ( !M || !N || SCALAR_IS_ZERO(alpha) ) return;
   if (lda&1)
   {
//    fprintf(stderr, "WARNING: not using L2-tuned GER kernel!\n");
      Mjoin(PATL,ger)(M, N, alpha, X, incX, Y, incY, A, lda);
      return;
   }
  
//   fprintf(stderr, "in %s! M=%i, N=%i, lda=%i, alpha=%f, A[64]=%i.\n", 
//   __FILE__, M, N, lda, alpha, (int) (((long unsigned int) A) & 63));
//   fflush(stderr);

   imb = mb = M;

   Aa = (size_t) A;
   Ax = (size_t) X;
   if (Aa%16 != Ax%16 || incX != 1 || !SCALAR_IS_ONE(alpha))
   {
/*
 *    Apply alpha to Y if X has stride 1 & Y is MUCH smaller
 *    The LAPACK barfs if you  switch which vector alpha is applied to,
 *    since it tests tiny matrices, so make it apply to X when they are
 *    close to even
 */
      if (incX == 1 && N < (M>>4) && Aa%16 == Ax%16)
      {
         vx = malloc(ATL_Cachelen + ATL_MulBySize(N));
         ATL_assert(vx);
         y = ATL_AlignPtr(vx);
         #ifdef Conj_
            Mjoin(PATL,moveConj)(N, alpha, Y, incY, y, 1);
            ATL_ger1 = Mjoin(PATL,ger1u_a1_x1_yX);
         #else
            Mjoin(PATL,cpsc)(N, alpha, Y, incY, y, 1);
         #endif
         incy = 1;
         getX = NULL;
      }
      else
      {
         i = Mmax(imb,mb);
         i = Mmin(i,M);
         vx = malloc(2*ATL_Cachelen + ATL_MulBySize(i));
         ATL_assert(vx);
         Ax = (size_t) vx;
         for (i=Aa%16; Ax%16 != i; Ax++);
         x = (TYPE*) Ax;
         getX = Mjoin(PATL,cpsc);
      }
   }
   else getX = NULL;

   if (imb) mb0 = Mmin(imb,m);
   else mb0 = Mmin(mb,m);
   do
   {
      if (getX) getX(mb0, alpha, X, incX, x, 1);
      else x = (TYPE*) X;
      ATL_ger1(mb0, N, one, x, 1, y, incy, A, lda);
      A += mb0 SHIFT;
      X += mb0*incX SHIFT;
      m -= mb0;
      mb0 = Mmin(m,mb);
   }
   while(m);
   if (vx) free(vx);
}
@ROUT ATL_gerk
#include <xmmintrin.h>
#include "atlas_misc.h"
#include <stdio.h>

void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
    ATL_CINT incY1, TYPE *A, ATL_CINT lda1)
{/* BEGIN GER: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
// ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - (((((size_t)A))>>4)<<4) )/sizeof(TYPE);
   ATL_CINT MAp = ( ((size_t)A)&(15) ) / sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, M2=((MA>>1)<<1)+MAp, N4=((N/4)*4), lda2=lda1+lda1, incY2=incY1+incY1, lda3=lda2+lda1, incY3=incY2+incY1, lda4=lda3+lda1, incY4=incY3+incY1;
   __m128d x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += incY4)
   {/* BEGIN N-LOOP UR=4 */
      y0 = _mm_load1_pd(Y);
      y1 = _mm_load1_pd(Y+incY1);
      y2 = _mm_load1_pd(Y+incY2);
      y3 = _mm_load1_pd(Y+incY3);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
         a0_1 = _mm_load_sd(A+i+0+lda1);
         m0_1 = _mm_mul_sd(x0, y1);
         a0_1 = _mm_add_sd(a0_1, m0_1);
         _mm_store_sd(A+i+0+lda1, a0_1);
         a0_2 = _mm_load_sd(A+i+0+lda2);
         m0_2 = _mm_mul_sd(x0, y2);
         a0_2 = _mm_add_sd(a0_2, m0_2);
         _mm_store_sd(A+i+0+lda2, a0_2);
         a0_3 = _mm_load_sd(A+i+0+lda3);
         m0_3 = _mm_mul_sd(x0, y3);
         a0_3 = _mm_add_sd(a0_3, m0_3);
         _mm_store_sd(A+i+0+lda3, a0_3);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         a0_1 = _mm_load_pd(A+i+0+lda1);
         m0_1 = _mm_mul_pd(x0, y1);
         a0_1 = _mm_add_pd(a0_1, m0_1);
         _mm_store_pd(A+i+0+lda1, a0_1);
         a2_1 = _mm_load_pd(A+i+2+lda1);
         m2_1 = _mm_mul_pd(x2, y1);
         a2_1 = _mm_add_pd(a2_1, m2_1);
         _mm_store_pd(A+i+2+lda1, a2_1);
         a4_1 = _mm_load_pd(A+i+4+lda1);
         m4_1 = _mm_mul_pd(x4, y1);
         a4_1 = _mm_add_pd(a4_1, m4_1);
         _mm_store_pd(A+i+4+lda1, a4_1);
         a6_1 = _mm_load_pd(A+i+6+lda1);
         m6_1 = _mm_mul_pd(x6, y1);
         a6_1 = _mm_add_pd(a6_1, m6_1);
         _mm_store_pd(A+i+6+lda1, a6_1);
         a0_2 = _mm_load_pd(A+i+0+lda2);
         m0_2 = _mm_mul_pd(x0, y2);
         a0_2 = _mm_add_pd(a0_2, m0_2);
         _mm_store_pd(A+i+0+lda2, a0_2);
         a2_2 = _mm_load_pd(A+i+2+lda2);
         m2_2 = _mm_mul_pd(x2, y2);
         a2_2 = _mm_add_pd(a2_2, m2_2);
         _mm_store_pd(A+i+2+lda2, a2_2);
         a4_2 = _mm_load_pd(A+i+4+lda2);
         m4_2 = _mm_mul_pd(x4, y2);
         a4_2 = _mm_add_pd(a4_2, m4_2);
         _mm_store_pd(A+i+4+lda2, a4_2);
         a6_2 = _mm_load_pd(A+i+6+lda2);
         m6_2 = _mm_mul_pd(x6, y2);
         a6_2 = _mm_add_pd(a6_2, m6_2);
         _mm_store_pd(A+i+6+lda2, a6_2);
         a0_3 = _mm_load_pd(A+i+0+lda3);
         m0_3 = _mm_mul_pd(x0, y3);
         a0_3 = _mm_add_pd(a0_3, m0_3);
         _mm_store_pd(A+i+0+lda3, a0_3);
         a2_3 = _mm_load_pd(A+i+2+lda3);
         m2_3 = _mm_mul_pd(x2, y3);
         a2_3 = _mm_add_pd(a2_3, m2_3);
         _mm_store_pd(A+i+2+lda3, a2_3);
         a4_3 = _mm_load_pd(A+i+4+lda3);
         m4_3 = _mm_mul_pd(x4, y3);
         a4_3 = _mm_add_pd(a4_3, m4_3);
         _mm_store_pd(A+i+4+lda3, a4_3);
         a6_3 = _mm_load_pd(A+i+6+lda3);
         m6_3 = _mm_mul_pd(x6, y3);
         a6_3 = _mm_add_pd(a6_3, m6_3);
         _mm_store_pd(A+i+6+lda3, a6_3);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            a0_1 = _mm_load_pd(A+i+0+lda1);
            m0_1 = _mm_mul_pd(x0, y1);
            a0_1 = _mm_add_pd(a0_1, m0_1);
            _mm_store_pd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_pd(A+i+0+lda2);
            m0_2 = _mm_mul_pd(x0, y2);
            a0_2 = _mm_add_pd(a0_2, m0_2);
            _mm_store_pd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_pd(A+i+0+lda3);
            m0_3 = _mm_mul_pd(x0, y3);
            a0_3 = _mm_add_pd(a0_3, m0_3);
            _mm_store_pd(A+i+0+lda3, a0_3);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
            a0_1 = _mm_load_sd(A+i+0+lda1);
            m0_1 = _mm_mul_sd(x0, y1);
            a0_1 = _mm_add_sd(a0_1, m0_1);
            _mm_store_sd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_sd(A+i+0+lda2);
            m0_2 = _mm_mul_sd(x0, y2);
            a0_2 = _mm_add_sd(a0_2, m0_2);
            _mm_store_sd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_sd(A+i+0+lda3);
            m0_3 = _mm_mul_sd(x0, y3);
            a0_3 = _mm_add_sd(a0_3, m0_3);
            _mm_store_sd(A+i+0+lda3, a0_3);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, Y += incY1)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_load1_pd(Y);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_threadMM
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
@skip #include Mstr(Mjoin(atlas_,Mjoin(Mjoin(Mjoin(PRE,tXover_),ATL_NCPU),p.h)))

#ifdef DEBUG
#define T2c(ta_) ((ta_) == AtlasNoTrans) ? 'N' : 'T'
#endif
#ifndef ATL_TXOVER_H
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * This dummy routine used when crossover is not tuned 
 */
{
#if 0
   size_t minD, maxD;

   minD = Mmin(M,N);
   minD = Mmin(minD,K);
   maxD = Mmax(M,N);
   maxD = Mmax(maxD,K);
   if (M >= (NB<<(ATL_NTHRPOW2+2)))
      return(2);
   else if (minD >= 8 && maxD >= 2*NB)
      return(1);
   return(0);
#else
   int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
   return(Mjoin(PATL,GemmWillThread)(M, N, K));
#endif
}
#else
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * RETURNS: number of threads matmul should use to paralellize the problem
 */
{
   size_t i, j, smp2, bip2, xo, xom, D;
   const int *xop;
   int k;
   if (M < 256 && N < 256 && K < 256)   /* small matrix */
   {
/*
 *    For really small problems, table lookups too expensive, so do a quick
 *    return
 */
      j = Mmax(M,N);
      i = Mmin(M,N);
      i = Mmin(i,K);
      if (j <= NB+NB || i < NB)
         return(1);    /* quick return */
/*
 *    Make choice based on most restricted dimension
 */
      if (M < N && M < K)   /* M most restricted dim */
         goto SMALLM;
      else if (K < M && K < N)  /* K most restricted dim */
         goto SMALLK;
      else if (M == N && M == K)
         goto SQUARE;
      else  /* N is most restricted dim */
         goto SMALLN;
   }
/*
 * The following three shapes model recursive factorizations where
 * two dimensions are cut during the recursion, and a third remains large
 */
   else if (N <= 256 && K <= 256)  /* recursive shape that doesn't cut M */
   {                               /* LU uses this shape */
      i = Mmin(N, K);
      j = Mmax(N, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = M;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnkLm_XO : ATL_tmmNT_SnkLm_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnkLm_XO : ATL_tmmTT_SnkLm_XO;
      #ifdef DEBUG
         printf("sNKlM_%c%c, M=%d, N=%d, K=%d rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && N <= 256)  /* recursive shape that doesn't cut K */
   {                               /* QR uses, maybe in LARFT? */
      i = Mmin(M, N);
      j = Mmax(M, N);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = K;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmnLk_XO : ATL_tmmNT_SmnLk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmnLk_XO : ATL_tmmTT_SmnLk_XO;
      #ifdef DEBUG
         printf("sMNlK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && K <= 256) /* recursive shape that doesn't cut N */
   {                              /* UNCONFIRMED: QR variant uses */
      i = Mmin(M, K);
      j = Mmax(M, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = N;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmkLn_XO : ATL_tmmNT_SmkLn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmkLn_XO : ATL_tmmTT_SmkLn_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
/*
 * The three following shapes model static blocking, where two dimensions
 * are full, and the third is blocked
 */
   else if (K <= 256)           /* K dim small, as in right-looking LU/QR */
   {
SMALLK:
      D = Mmin(M,N);
      if (D >= NB+NB)
         D = (M+N)>>1;
      i = K;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SkLmn_XO : ATL_tmmNT_SkLmn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SkLmn_XO : ATL_tmmTT_SkLmn_XO;
      #ifdef DEBUG
         printf("sKlMN_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                 T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256)          /* M dim small */
   {
SMALLM:
      D = Mmin(N,K);
      if (D >= NB+NB)
         D = (N+K)>>1;
      i = M;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmLnk_XO : ATL_tmmNT_SmLnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmLnk_XO : ATL_tmmTT_SmLnk_XO;
      #ifdef DEBUG
         printf("sMlNK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (N <= 256)          /* N dim small */
   {                           /* QR uses this */
SMALLN:
      D = Mmin(M,K);
      if (D >= NB+NB)
         D = (M+K)>>1;
      i = N;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnLmk_XO : ATL_tmmNT_SnLmk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnLmk_XO : ATL_tmmTT_SnLmk_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else                        /* all dim > 256, call it square */
   {
SQUARE:   /* near-square shape, N <= 256 if jumped here */
      D = (M+N+K+1)/3;
      j = 0;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SQmnk_XO : ATL_tmmNT_SQmnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SQmnk_XO : ATL_tmmTT_SQmnk_XO;
      #ifdef DEBUG
         printf("SQ_%c%c, M=%d, N=%d, K=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, D);
      #endif
   }

   xop += j*ATL_PDIM;
   for (k=ATL_PDIM-1; k >= 0; k--)
      if (xop[k] && D >= xop[k])
         return((k == ATL_PDIM-1) ? ATL_NTHREADS : (2<<k));
   return(1);
}
#endif
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK ATL_tgemm_bigMN_Kp
#include "atlas_misc.h"
#include "atlas_tcacheedge.h"
#ifndef CacheEdge
   #include "atlas_cacheedge.h"
   #ifndef CacheEdge
      #define CacheEdge 524288
   #endif
#endif
@ROUT ATL_tgemm_rkK ATL_tgemm_bigMN_Kp `#include "atlas_tlvl3.h"`
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np `#include "atlas_lvl3.h"`
#include "atlas_threads.h"
#include "atlas_tsumm.h"

@ROUT ATL_tgemm_MKp
static int ATL_NTHR = ATL_NTHREADS;
typedef struct
{
   void *aBcnt;           /* counter on partitions of B used in B copy */
   void *aAcnt;           /* count on the partitions of A */
   void *aCcnt;           /* count on columns of C */
   volatile int *chkin;   /* NTHR-len checkin array */
   TYPE **Aws;            /* preallocated thread copy areas */
   TYPE *Bw;              /* workspace for common B */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT Mp, nMp, mlast;
   ATL_INT M, N, K, lda, ldb, ldc, nNb;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_MKP_t;
@ROUT ATL_tgemm_rkK_Np
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_NP_t;
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/*
 * Matmul driver, loops over pre-copied A & B, operands are preblocked
 * so a column panel of B, the entire A and needed portion of C fit in the L2,
 * Therefore, use all of A against a single column panel of B, 
 * and thus do NMK loop order.
 */

#define genmm Mjoin(PATL,pKBmm)     /* cleans up any combin. of partial blks */
#define PKBmm Mjoin(PATL,pKBmm_b1)  /* cleans up full MB,NB, partial KB */
#define PNBmm Mjoin(PATL,pNBmm_b1)  /* cleans up full MB,KB, partial NB */
#define PMBmm Mjoin(PATL,pMBmm_b1)  /* cleans up full NB,KB, partial MB */

/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format and that you all you need is one traversal of the K loop.
 */
static void DoMM_K
(
   ATL_CINT mb,         /* # of rows in C <= MB */
   ATL_CINT nb,         /* # of cols in C <= NB */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr) x NB panel */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr) x NB panel */
   const SCALAR beta,   /* scale C by this value */
   TYPE *C,             /* ldcxnb array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
#ifdef TREAL
   ATL_INT k;
   ATL_CINT incA = mb*KB, incB = KB*nb;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (mb != MB || nb != NB)
      {
         if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, C, ldc);
      }
      else
      {
         if (SCALAR_IS_ONE(beta))
            Mjoin(PATL,pKBmm_b1)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,pKBmm_b0)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else
            Mjoin(PATL,pKBmm_bX)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
      }
      return;
   }
   if (mb != NB && nb != NB)
      mmk_bX = mmk = genmm;
   else if (mb != NB)
   {
      mmk = PMBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pMBmm_bX);
   }
   else if (nb != NB)
   {
      mmk = PNBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   else
   {
      mmk = NBmm;
      mmk_kr = PKBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = NBmm_b1;
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = NBmm_b0;
      else
         mmk_bX = NBmm_bX;
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, beta, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      mmk_kr(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#else  /* complex code */
   ATL_INT k;
   ATL_CINT incA = mb*KB*2, incB = 2*KB*nb;
   const TYPE one[2] = {ATL_rone, ATL_rzero};
   const TYPE *bet = beta;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (beta[1] == ATL_rzero && beta[0] != ATL_rzero)
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, *beta, C, ldc);
      else
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         else
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      }
      return;
   }
   if (mb == MB && nb == NB)
   {
      if (beta[1] == ATL_rzero)  /* real scalar */
      {
         if (*beta == ATL_rone)
         {
            NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
         }
         else if (*beta == ATL_rzero)
         {
            NBmm_b0(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rzero, C, ldc);
         }
         else
         {
            NBmm_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *beta, C, ldc);
         }
      }
      else /* must scale for complex beta */
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      }
      A += incA; B += incB;
      for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      if (kr)  /* partial remainder */
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      return;
   }
   if (mb != MB)
   {
      if (nb != NB)  /* both blocks partial */
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
         {
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
            bet = one;
         }
         else if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = mmk_bX =  Mjoin(PATL,pKBmm);
      }
      else  /* only M block is partial */
      {
         if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = Mjoin(PATL,pMBmm_b1);
         if (*bet == ATL_rone)
            mmk_bX = Mjoin(PATL,pMBmm_b1);
         else if (*bet == ATL_rzero)
            mmk_bX = Mjoin(PATL,pMBmm_b0);
         else
            mmk_bX = Mjoin(PATL,pMBmm_bX);
      }
   }
   else if (nb != MB)  /* only N block is partial */
   {
      if (beta[1] != ATL_rzero)
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         bet = one;
      }
      mmk = Mjoin(PATL,pNBmm_b1);
      if (*bet == ATL_rone)
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (*bet == ATL_rzero)
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *bet, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#endif
}

@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np
/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format.
 */
static void DoMM_NMK
(
   ATL_CINT nfMblks,    /* # of full blocks of M */
   ATL_CINT mr,         /* partial remainder block on M */
   ATL_CINT nfNblks,    /* # of full blocks of N */
   ATL_CINT nr,         /* partial remainder block on N */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr)xNB panels */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr)xNB panels */
   TYPE *C,             /* ldaxN array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
   ATL_INT j;
   ATL_CINT K = nfKblks*NB + kr;

   for (j=0; j < nfNblks; j++)
   {
      const TYPE *Bc = B + K*NB*j;
      TYPE *Cc = C + j*NB*ldc;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i;
         const TYPE *b = Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            NBmm(MB, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires PKBmm */
            PKBmm(MB, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* can use PMBmm for all full blocks of N & K */
      {
         const TYPE *b = Bc;
         const TYPE *a = A + K*NB*nfMblks;
         TYPE *c = Cc + nfMblks*NB;
         ATL_INT k;
         const int mrNB = mr*NB;

         for (k=0; k < nfKblks; k++)
         {
            PMBmm(mr, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
   if (nr) /* can use PNBmm for all full blks of M/K, must use genmm rest */
   {
      const TYPE *Bc = B + K*nfNblks*NB;
      TYPE *Cc = C + nfNblks*NB*ldc;
      ATL_CINT nrNB = nr * NB;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i, *b=Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            PNBmm(MB, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += nrNB;
         }
         if (kr)  /* partial KB&NB requires genmm */
            genmm(MB, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* must use genmm for two or more partial dim */
      {
         const TYPE *a = A + K*NB*nfMblks, *b = Bc;
         TYPE *c = Cc + nfMblks*NB;
         const int mrNB = mr*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            genmm(mr, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += nrNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
}
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/* 
 * Takes a MxN block and expands it to an ldaxM block wt zero padding in-place
 */
#ifdef TREAL
static void ExpandBlock
(
   ATL_CINT M, /* the number of rows that should be expanded to lda */
   ATL_CINT N, /* number of columns in block */
   TYPE *A,    /* in: MxN block, out: ldaxN blk, wt zero-padding in lda-M gap */
   ATL_INT lda /* desired stride between columns */
)
{
   TYPE *a, *c;
   ATL_CINT gap = lda - M;
   ATL_INT j;

   if (gap < 1)   /* already done if lda == M */
      return;

//fprintf(stderr, "ExpandBlock, M=%d, N=%d, A=%p, lda=%d\n", M, N, A, lda);
   a = A + N*M - 1;
   c = A + N*lda - 1;
   for (j=N; j; j--)
   {
      TYPE *stop = c - gap;
      do 
         *c-- = ATL_rzero;
      while (c != stop);
      stop =  c - M;
      do
         *c-- = *a--;
      while (c != stop);
   }
}
#endif

@ROUT ATL_tgemm_rkK_Np
/*
 * This routine assumes A has already been copied to block-major, row-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy an K8xNB piece of B to their cache in their workspace
 * (b) Use that copied piece against all of A to update a col-panel of C
 * (c) Repeat until all column-panels of B have been applied
 * They determine what col panel of B to operate on using the aNcnt variable.
 */
static void ATL_tloopN
(
@skip   void **aNcnts,       /* NTHR Atomic counters on col panels of C */
   void *aNcnt,         /* Atomic counter on col panels of C */
   int iam,             /* my rank; */
   ATL_CINT nfMblks,    /* # of full NB blocks in Mp */
   int mr,              /* M%NB */
   ATL_CINT nfNblks,    /* # of full NB blocks along N */
   int nr,              /* N%NB */
@skip   int nlblks,          /* nfNblks / ATL_NTHREADS */
@skip   int nrblks,          /* nfNblks % ATL_NTHREADS */
   ATL_CINT nfKblks,    /* # of full NB blocks along K8 */
   int kr,              /* K%NB */
   int krpad,           /* # of entries to add to K-dim to make mul of 8 */
   const TYPE *A,       /* A in blk-major, row-panel (K8xMB) format */
   enum ATLAS_TRANS TB, /* Transpose setting of B */
   const TYPE *B,       /* B in original column-major format */
   ATL_CINT ldb,        /* leading dim of B */
   TYPE *pB,            /* my private workspace to copy col-panels into */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = nfMblks*NB+mr, N = nfNblks*NB+nr, K = nfKblks*NB+kr, K8=K+krpad;
   ATL_CINT kr8 = kr+krpad, nnblks = nr ? nfNblks+1 : nfNblks;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblksf = (kr8 >= NB) ? nfKblks+1 : nfKblks;
   ATL_INT jblk, p;
   ATL_CINT BNOTRANS = (TB == AtlasNoTrans);
   ATL_CINT bmul = (BNOTRANS) ? ldb : 1, nNblks=nfNblks+1;

   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, nnb, nbr;
      size_t j;

      if (jblk != nNblks)
      {
         nnb = 1;
         nbr = 0;
         nb = NB;
      }
      else
      {
         nnb = 0;
         nbr = nr;
         nb = nr;
      }
      j = (jblk-1)*(NB SHIFT);
/*
 *    Copy the specified col-panel to my private workspace
 */
      if (BNOTRANS)
         Mjoin(PATL,col2blk_a1)(K, nb, B+j*ldb, ldb, pB, ATL_rone);
      else
         Mjoin(PATL,row2blkT_a1)(K, nb, B+j, ldb, pB, ATL_rone);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      if (krpad)
         ExpandBlock(kr, nb, pB+nfKblks*NB*nb, kr8);
/* 
 *    Perform the rank-K8 update of the column panel of C
 */
      DoMM_NMK(nfMblks, mr, nnb, nbr, nfKblksf, kr8f, A, pB, C+j*ldc, ldc);
   }
}

@ROUT ATL_tgemm_rkK_Np
void Mjoin(PATL,DoWork_rkK_Np)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * calls tloopN to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_NP_t *pd=lp->vp;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   SCALAR alpha = pd->alpha;
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8;
   ATL_CINT nablks = (mr) ? nMb+1 : nMb, incA = (nKb*NB + kr8)*NB;
   ATL_CINT lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   ATL_INT iblk;
   MAT2BLK A2blk;
   int k;

   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == nablks && mr) ? mr : NB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      if (kr8 != kr)
         ExpandBlock(kr, mb, Aw+iblk*incA+nKb*NB*mb, kr8);
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   chkin[iam] = 1;
   for (k=0; k < ATL_NTHREADS; k++)
      while(!chkin[k]) ATL_POLL;
/*
 * Perform the rank-K update with a fully-copied A
 */
   ATL_tloopN(pd->aNcnt, iam, nMb, mr, pd->nNb, pd->nr, 
              nKb, kr, kr8-kr, Aw, pd->TB, pd->B, pd->ldb, pd->Bws[iam], 
              pd->C, pd->ldc);
}
@ROUT ATL_tgemm_rkK
void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * loops over atomic counters on N & M to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_t *pd=lp->opstruct;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt, *aNcnt=pd->aNcnt, **aMcnts = pd->aMcnts;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   #ifdef TREAL
      TYPE alpha = pd->alpha;
      TYPE beta  = pd->beta;
   #else
      const SCALAR alpha = pd->alpha;
      const SCALAR beta  = pd->beta;
   #endif
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nNb=pd->nNb, nr=pd->nr;
   ATL_CINT nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8, krpad = kr8-kr;
   size_t incA = (nKb*KB + kr8)*(MB SHIFT), ldc = pd->ldc;
   size_t lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   size_t ldb=pd->ldb, P=ATL_NTHREADS;
   ATL_CINT tnMblks = (mr) ? nMb+1 : nMb, tnNblks = (nr) ? nNb+1 : nNb;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblks = (kr8 >= NB) ? nKb+1 : nKb;
   ATL_CINT bmul = (BNOTRANS) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT iblk, jblk;
/*   #define PRINTTOTALS */
   #ifdef PRINTTOTALS
      int myblks = 0, hisblks=0;
      static int myarr[ATL_NTHREADS], hisarr[ATL_NTHREADS];
   #endif
   MAT2BLK A2blk;
   #ifdef TREAL
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                   Mjoin(PATL,row2blkT_a1);
   #else
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                  (pd->TB == AtlasTrans) ?
                                   Mjoin(PATL,row2blkT_a1):
                                   Mjoin(PATL,row2blkC_a1);
   #endif
   int k;
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
#ifdef TREAL
   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
#else
   if (ANOTRANS)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,row2blkT_a1);
         else
            A2blk = Mjoin(PATL,row2blkT_aXi0);
      }
      else
         A2blk = Mjoin(PATL,row2blkT_aX);
   }
   else if (pd->TA == AtlasConjTrans)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blkConj_a1);
         else
            A2blk = Mjoin(PATL,col2blkConj_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blkConj_aX);
   }
   else  /* TA == AtlasTrans */
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blk_a1);
         else
            A2blk = Mjoin(PATL,col2blk_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blk_aX);
   }
#endif
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == tnMblks && mr) ? mr : MB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      #ifdef TREAL
         if (kr8 != kr)
            ExpandBlock(kr, mb, Aw+iblk*incA+nKb*KB*mb, kr8);
      #endif
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   if (iam == 0)
   {
      for (k=1; k < P; k++)
         while(!chkin[k]) 
            ATL_POLL;
      chkin[0] = 1;
   }
   else
   {
      chkin[iam] = 1;
      while (!chkin[0]);
   }
/*
 * Perform the rank-K update with a fully-copied A by looping over column-panels
 * of C in reverse order
 */
   ATL_mutex_lock(pd->Mlocks[iam]);
   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, pL;
      size_t j;
      void *aCrow;
      TYPE *c;

      nb = (jblk != tnNblks || !nr) ? NB : nr;
      pd->Js[iam] = j = (jblk-1)*NB;
      c = C + j*(ldc SHIFT);
/*
 *    Copy the specified column-panel of B to my private workspace
 */
      B2blk(K, nb, B+j*bmul, ldb, Bw, one);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      #ifdef TREAL
      if (krpad)
         ExpandBlock(kr, nb, Bw+nKb*KB*nb, kr8);
      #endif
/*
 *    Given that I'm working on col-panel j, determine the percentage of its
 *    blocks that I reserve for myself based on how many columns are left.
 *    If there are plenty of columns left, do all local counters which will
 *    reduce the counter cost by something like a factor of 10.  If we are
 *    getting close to running out of col-panels, reserve less and less of
 *    the problem for my exclusive use.
 */
      if (jblk >= P+P)
         pL =  100;
      else if (jblk <= 2)
         pL = 0;
      else
         pL = (jblk > P) ? 50 : 100/P;
      aCrow = aMcnts[iam];
      ATL_ResetGlobalAtomicCount(aCrow, tnMblks, pL);
      ATL_mutex_unlock(pd->Mlocks[iam]);
      while (iblk = ATL_DecGlobalAtomicCount(aCrow, 0))
      {
         const int mb = (!mr || iblk != 1) ? MB : mr;
         const size_t i = (tnMblks-iblk)*(NB SHIFT);
         #ifdef PRINTTOTALS
            myblks++;
         #endif

         iblk = tnMblks - iblk;
         DoMM_K(mb, nb, nfKblks, kr8f, Aw+iblk*incA, Bw, beta, c+i, ldc);
      }
      ATL_mutex_lock(pd->Mlocks[iam]);
   }
   ATL_mutex_unlock(pd->Mlocks[iam]);
   chkin[iam] = -3;  /* let everyone know I've finished my columns */
/*
 * When no more col-panels of C are available, it is time to see if I can
 * help other workers finish their columns; As long as someone hasn't
 * signaled his completion (negative # in chkin), continue trying to steal.
 */
   do
   {
/*
 *    If anyone is still working, continue looking to steal his work
 */
      for (k=0; k < P && chkin[k] <= 0; k++);
      if (k == P)
         break;    /* everyone done, quit */
      for (; k < P; k++)
      {
         const int rk = k;
         void *aCrow = aMcnts[rk];
    
         Bw = pd->Bws[rk];
         ATL_mutex_lock(pd->Mlocks[rk]);
         if (ATL_GetGlobalAtomicCount(aCrow, 1))
         {
            TYPE *c = C + pd->Js[rk]*(((size_t)ldc)SHIFT);
            int nb;
   
            nb = pd->N - pd->Js[rk];
            nb = Mmin(NB, nb);
            while (iblk = ATL_DecGlobalAtomicCount(aCrow, 1))
            {
               const int mb = (iblk != 1 || !mr) ? MB : mr;
               const size_t i = (tnMblks-iblk)*(NB SHIFT);
      
               #ifdef PRINTTOTALS
                  hisblks++;
               #endif
               iblk = tnMblks - iblk;
               DoMM_K(mb, nb, nfKblks, kr8f, Aw+(iblk*incA), Bw, beta,
                      c+i, ldc);
            }
         }
         ATL_mutex_unlock(pd->Mlocks[rk]);
      }
   }
   while(1);
/*
 * Master process is waiting on thread 0, so 0 stays here until all nodes
 * complete their operations
 */
   chkin[iam] = -2;
   #ifdef PRINTTOTALS
      myarr[iam] = myblks;
      hisarr[iam] = hisblks;
   #endif
   if (pd->Sync0 && !iam)
   {
      #ifdef PRINTTOTALS
         int lblks, rblks;
      #endif
      for (k=1; k < P; k++)
         while(chkin[k] != -2)
            ATL_POLL;
      #ifdef PRINTTOTALS
          printf(" myblks : %4d", myarr[0]);
          lblks = myarr[0];
          for (k=1; k < P; k++)
          {
             lblks += myarr[k];
             printf(",%4d", myarr[k]);
          }
          printf(" = %d\n", lblks);
          printf("hisblks : %4d", hisarr[0]);
          rblks = hisarr[0];
          for (k=1; k < P; k++)
          {
             rblks += hisarr[k];
             printf(",%4d", hisarr[k]);
          }
          printf(" = %d\n", rblks);
          printf("Total = %d, expected=%d\n", lblks+rblks, 
                 ((pd->M+NB-1)/NB)*((pd->N+NB-1)/NB));
      #endif
   }
}
@ROUT  ATL_tgemm_bigMN_Kp
void Mjoin(PATL,DoWork_bigMN_Kp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   const int iam = tp->rank, P = tp->P;
   ATL_TGEMM_RKK_t *pd = lp->opstruct;
   volatile int *chkin = pd->chkin+P, *hischk = pd->chkin;
   ATL_CINT K = pd->K, Kp = pd->kr, nkb = Kp / NB;
   ATL_CINT nnb = (pd->nr) ? pd->nNb+1 : pd->nNb;
   ATL_CINT nmb = (pd->mr) ? pd->nMb+1 : pd->nMb;
   const size_t incA = (pd->TA == AtlasNoTrans) ? (pd->lda SHIFT) : (1 SHIFT); 
   const size_t incB = (pd->TB == AtlasNoTrans) ? (1 SHIFT) : (pd->ldb SHIFT);
   ATL_INT k, kb;
   int i, n;
   const TYPE *A = pd->A, *B = pd->B;
   void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp);
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif

   for (k=0; k < K; k += Kp)
   {
      kb = K - k;
      kb = Mmin(kb, Kp);
/*
 *    To avoid race conditions, thread 0 sets everything up, and rest of
 *    threads await his signal to begin
 */
      if (iam == 0)
      {
         n = chkin[0] + 1;
         for (i=1; i < P; i++)
            while(chkin[i] < n)
               ATL_POLL;
         for (i=0; i < P; i++)
            hischk[i] = 0;
         pd->beta = (k) ? one : pd->beta;
         pd->A = A + k*incA;
         pd->B = B + k*incB;
         ATL_ResetGlobalAtomicCount(pd->aNcnt, nnb, 0);
         ATL_ResetGlobalAtomicCount(pd->aMcnt, nmb, 0);
         pd->K = kb;
         if (kb == Kp)
         {
            pd->kr = pd->kr8 = 0;
            pd->nKb = nkb;
         }
         else
         {
            pd->nKb = kb / KB;
            pd->kr = kb - pd->nKb * KB;
            #ifdef TREAL
               pd->kr = kb - pd->nKb * KB;
               pd->kr8 = ((pd->kr+7)>>3)<<3;
               if (pd->kr8 > KB)
                  pd->kr8 = KB;
            #else
               pd->kr8 = pd->kr = kb - pd->nKb * KB;
            #endif
         }
         chkin[0] = n;
      }
      else
      {
         n = ++chkin[iam];
         while (chkin[0] < n)
            ATL_POLL;
      }
      Mjoin(PATL,DoWork_rkK)(lp, vp);  /* do rank-K update */
   }
/*
 * Master process waiting on thread 0, who therefore must block until everyone
 * signals completion
 */
   n = ++chkin[iam];
   if (!iam)
      for (i=1; i < P; i++)
         while (chkin[i] < n)
            ATL_POLL;
}
#ifdef TREAL
   #undef one
#endif
   

int Mjoin(PATL,tgemm_bigMN_Kp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine handles the asymtotic case where A & B are too large to be
 * copied at once, and we loop over CacheEdge-sized partitions of K.
 * Implements very large gemm as a series of synchronized rank-Kp updates.
 */
{
   ATL_TGEMM_RKK_t pd;   /* problem definition */
   size_t sz, Kp;
   void *vp;
   int i;
   #ifdef FindingCE
      extern int FoundCE, CompCE;
      #define MY_CE FoundCE
   #else
      #define MY_CE CacheEdge
   #endif

   #ifdef TREAL
      Kp = (ATL_DivBySize(MY_CE) - MB*NB)/(MB + 2*NB);
   #else
      Kp = (ATL_DivBySize(MY_CE) - 2*MB*NB)/(2*MB + 4*NB);
   #endif
   Kp = (Kp/KB)*KB;
   #ifdef FindingCE
      if (MY_CE == 0)
         Kp = K;
      if (Kp < KB)
         Kp = KB;
      if (CompCE)
      {
         CompCE = Kp;
         return;
      }
   #else
      if (Kp < KB)
         return(1);  /* not going to be efficient */
   #endif

   pd.kr = Kp;
   pd.TA = TA;
   pd.TB = TB;
   pd.M = M;
   pd.N = N;
   pd.K = K;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.beta = beta;
   pd.C = C;
   pd.ldc = ldc;
   pd.nMb = M / MB;
   pd.mr = M - pd.nMb*MB;
   pd.nNb = N / NB;
   pd.nr = N - pd.nNb*NB;

   sz = ATL_MulBySize(Kp)*M + ATL_Cachelen;  /* sizeof A workspace */
   sz += ATL_NTHREADS*(Kp*ATL_MulBySize(NB)+ATL_Cachelen);  /* B workspaces */
   sz += ATL_NTHREADS*3*sizeof(int);  /* chkin1/2 & Js arrays */
   sz += ATL_NTHREADS*sizeof(TYPE*);  /* Bws array */
   sz += ATL_NTHREADS*2*sizeof(void*); /* aMcnts & Mlocks arrays */
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC+ATL_MaxMalloc)
      return(2);  /* can't allocate space */
   pd.Bws = malloc(sz);
   if (!pd.Bws)
      return(3);
   pd.aMcnts = (void**)(pd.Bws+ATL_NTHREADS);
   pd.Mlocks = (pd.aMcnts+ATL_NTHREADS);
   pd.chkin = (volatile int*)(pd.Mlocks+ATL_NTHREADS);
   pd.Js = (int*)(pd.chkin + 2*ATL_NTHREADS);
   pd.Aw = (TYPE *) (pd.Js + ATL_NTHREADS);
   pd.Aw = ATL_AlignPtr(pd.Aw);
   pd.Sync0 = 0;
   vp = pd.Aw + M * (Kp SHIFT); 
   pd.Bws[0] = ATL_AlignPtr(vp);
   for (i=1; i < ATL_NTHREADS; i++)
   {
      vp = pd.Bws[i-1] + (NB SHIFT)*Kp;
      pd.Bws[i] = ATL_AlignPtr(vp);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      pd.aMcnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
      pd.chkin[i] = pd.chkin[ATL_NTHREADS+i] = 0;
      pd.Js[i] = 0;
   }
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_bigMN_Kp), &pd, NULL);
/*
 * Free allocated resources
 */
   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_mutex_free(pd.Mlocks[i]);
      ATL_FreeGlobalAtomicCount(pd.aMcnts[i]);
   }
   free(pd.Bws);
   return(0);
}
@ROUT ATL_tgemm_rkK_Np 
   @define sf @_Np@
@ROUT ATL_tgemm_rkK
   @define sf @@
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
int Mjoin(PATL,tgemm_rkK@(sf))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * Does a rank-K update on dynamically scheduled column panels of C
 */
{
   ATL_TGEMM_RKK@up@(sf)_t pd;   /* problem definition */
   size_t sz;
   volatile int *chkin;
   TYPE **Bws, *Aw;
   ATL_CINT nKb = ATL_DivByNB(K), kr = K - ATL_MulByNB(nKb);
   ATL_CINT K8 = ATL_MulByNB(nKb) + (((kr+7)>>3)<<3);
   ATL_INT nlblks, nrblks;
   int i, nDb, dr;
   void **acnts;

@ROUT ATL_tgemm_rkK_Np
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+sizeof(int)+ATL_Cachelen);
@ROUT ATL_tgemm_rkK
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+2*sizeof(int)+ATL_Cachelen+2*sizeof(void*));
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC)
      return(1);
   Bws = malloc(sz);
   if (!Bws)
      return(2);
   chkin = (volatile int*) (Bws + ATL_NTHREADS);
@ROUT ATL_tgemm_rkK
   pd.Sync0 = 1;
   pd.Js = (int*) (chkin+ATL_NTHREADS);
   acnts = (void**) (pd.Js+ATL_NTHREADS);
   pd.Mlocks = acnts + ATL_NTHREADS;
   Aw = (TYPE*)(pd.Mlocks+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np
   Aw = (TYPE*) (chkin+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   Aw = ATL_AlignPtr(Aw);
   Bws[0] = Aw + K8*(M SHIFT);
   Bws[0] = ATL_AlignPtr(Bws[0]);
@ROUT ATL_tgemm_rkK `   pd.Js[0] = 0;`
   chkin[0] = 0;
   for (i=1; i < ATL_NTHREADS; i++)
   {
      Bws[i] = Bws[i-1] + K8*(NB SHIFT);
      Bws[i] = ATL_AlignPtr(Bws[i]);
      chkin[i] = 0;
@ROUT ATL_tgemm_rkK `      pd.Js[i] = 0;`
   }
   pd.chkin = chkin;
   pd.Aw = Aw;
   pd.Bws = Bws;
   pd.nMb = nDb = ATL_DivByNB(M);
   pd.mr = dr = M - ATL_MulByNB(nDb);
   nDb = (dr) ? nDb+1 : nDb;
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nDb, 0);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      acnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
   }
   pd.aMcnts = acnts;
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   pd.nNb = nDb = ATL_DivByNB(N);
   pd.nr = dr = N - ATL_MulByNB(nDb);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, dr ? nDb+1 : nDb, 0);
   pd.nKb = nKb; pd.kr = kr; 
   #ifdef TREAL
      pd.kr8 = ((kr+7)>>3)<<3;
      if (pd.kr8 > KB)
         pd.kr8 = KB;
   #else
      pd.kr8 = kr;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.M = M; pd.N = N; pd.K = K;
   pd.TA = TA; pd.TB = TB;
   pd.alpha = alpha; pd.beta = beta;

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_rkK@(sf)), &pd, NULL);

   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_FreeGlobalAtomicCount(acnts[i]);
      ATL_mutex_free(pd.Mlocks[i]);
   }
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   free(Bws);
   return(0);
}
@ROUT ATL_tgemm_MKp
/*
 * This routine assumes B has already been copied to block-major, column-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy a piece of A to their cache
 * (b) Use that copied piece against all of B to update a row-panel of C
 * (c) Repeat until all rows of A have been applied
 * They determine what row panel of A to operate on using the aAcnt variable.
 */
static void ATL_tloopA
(
   void *aAcnt,         /* Atomic counter on row panels of A */
   int iam,             /* my rank; */
   int nMp,             /* how many M partitions are there? */
   int Mp,              /* size of all but last M partition */
   int mlast,           /* how many elts in last M partition? */
   int nfMblks,         /* # of full NB blocks in Mp */
   int nfNblks,         /* # of full NB blocks along N */
   int nr,              /* N%NB */
   int nfKblks,         /* # of full NB blocks along Kp */
   int kr,              /* K%NB */
   enum ATLAS_TRANS TA,
   const TYPE *A,       /* original A matrix */
   ATL_CINT lda,        /* leading dim of A */
   TYPE *pA,            /* my private workspace to copy row-panels into */
   const TYPE *B,       /* B in blk-major, column-panel format */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = ((nMp-1)*Mp)+mlast, N = nfNblks*NB + nr, K = nfKblks*NB + kr;
   ATL_INT iblk;
   size_t i;

   while (iblk = ATL_DecGlobalAtomicCount(aAcnt, iam))
   {
      ATL_INT mb, mymr, mymblks;
      if (iblk-- == nMp)
      {
         mb = mlast;
         mymblks = (mlast/NB);
         mymr = mlast - mymblks*NB;
      }
      else
      {
         mb = Mp;
         mymr = 0;
         mymblks = nfMblks;
      }
      i = iblk*Mp;
/*
 *    Copy the specified row-panel (may be multiple MB-wide row-panels) 
 *    to my private workspace
 */
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(mb, K, A+i, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, mb, A+i*lda, lda, pA, ATL_rone);
      DoMM_NMK(mymblks, mymr, nfNblks, nr, nfKblks, kr, pA, B, C+i, ldc);
   }
}

void Mjoin(PATL,DoWork_MKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_MKP_t *pbdef=lp->opstruct;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Bw=pbdef->Bw, *Aw=pbdef->Aws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->Mp/NB, nfNblks = N/NB;
   ATL_CINT nr = pbdef->N-nfNblks*NB; 
   ATL_CINT nMp=pbdef->nMp, Mp=pbdef->Mp, mlast=pbdef->mlast;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      void *aCcnt = pbdef->aCcnt;
      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,zero)(M, C+ldc*j, 1);
      }
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ? 
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      } 
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetGlobalAtomicCount(pbdef->aBcnt, nNb, 0);/* for next k-it */
         ATL_ResetGlobalAtomicCount(pbdef->aAcnt, nMp, 0);/* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr, 
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }  
@beginskip
   if (0)
   {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
   }
@endskip
}

int Mjoin(PATL,tgemm_MKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Bw, *p;
   int *chkin;
   void *amcnt, *vp;
   TYPE **Aws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_MKP_t pbdef;

@beginskip
   if (0)
   {
      TYPE *pB, *pA;
      void *vp;

      vp = malloc(M*K*sizeof(TYPE) + N*K*sizeof(TYPE) + 2*ATL_Cachelen);
      ATL_assert(vp);
      pB = ATL_AlignPtr(vp);
      pA = pB + N*K;
      pA = ATL_AlignPtr(pA);
      if (TB == AtlasNoTrans)
      {
         if (SCALAR_IS_ONE(alpha))
            Mjoin(PATL,col2blk_a1)(K, N, B, ldb, pB, alpha);
         else
            Mjoin(PATL,col2blk_aX)(K, N, B, ldb, pB, alpha);
      }
      else if (SCALAR_IS_ONE(alpha))
         Mjoin(PATL,row2blkT2_a1)(N, K, B, ldb, pB, alpha);
      else
         Mjoin(PATL,row2blkT2_aX)(N, K, B, ldb, pB, alpha);
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(M, K, A, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, M, A, lda, pA, ATL_rone);
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      DoMM_NMK(M/NB, M-(M/NB)*NB, N/NB, N-(N/NB)*NB, K/KB, K-(K/KB)*KB,
               pA, pB, C, ldc);
      free(vp);
      return(0);
   }
@endskip
/*
 * =====================================================================
 * Compute the Kp and Mp partitionings.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    Mp*Kp + 2*(NB*Kp + Mp*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
      Mp = MB;
   }
/*
 * Otherwise, Kp = K, and we solve for Mp
 *    Mp*K + 2*(NB*K + Mp*NB) = CE  -> Mp = (CE - 2*NB*K) / (K+NB)
 *     A         B       C
 */
   else  /* Kp = K, solve for Mp */
   {
      Mp = (CacheEdge - 2*NB*K) / (K+NB);
      if (Mp > MB)
      {
         while (M/Mp < 3*ATL_NTHREADS)
            Mp >>= 1;
         Mp = (Mp > MB) ? (Mp/MB)*MB : MB;
      }
      else
         Mp = MB;
      Kp = K;
   }

/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block for both M & K
 */
   nKp = K / Kp;          /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;
      else
         nKp++;         /* nKp now includes klast */
   }
   else
      klast = Kp;  /* K is even multiple of Kp */
   nMp = M / Mp;        /* floor (K/Kp) */
   mlast = M - Mp*nMp;
   if (mlast)
   {
      if (mlast < MB)
         mlast += Mp;   /* absorb partial block into last part */
      else
         nMp++;         /* nMp now includes all partitions */
   }
   else
      mlast = Mp;    /* M even multiple of Mp */
   nNb = (N+NB-1)/NB;  /* ceil(N/NB) */
/* 
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = Mmax(Kp, klast);
   m = Mmax(Mp, mlast);
   wrksz = (2+ATL_NTHR)*ATL_Cachelen + ATL_MulBySize(NB)*nNb*k + 
           ATL_NTHR*(sizeof(int)+sizeof(TYPE*) + ATL_MulBySize(m)*k);
   if (ATL_NTHR*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Aws = vp;                            /* A work ptrs first array */
   chkin = (int*)(Aws+ATL_NTHR);        /* then checkin array */
   Bw = (TYPE*)(chkin+ATL_NTHR);        /* then workspace for B */
   Bw = ATL_AlignPtr(Bw);               /* B must be aligned */
   chkin[0] = 0;
   p = Bw + k*N;                        /* first A wrkspc after B wrkspc */
   Aws[0] = ATL_AlignPtr(p);            /* A wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      chkin[i] = 0;
      p = Aws[i-1] + m*k;
      Aws[i] = ATL_AlignPtr(p);
   }
   pbdef.chkin = chkin;
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.Mp = Mp; pbdef.mlast = mlast; pbdef.nMp = nMp;
   pbdef.nNb = nNb;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Bw = Bw; pbdef.Aws = Aws;
   pbdef.aBcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nNb, 0);
   pbdef.aAcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nMp, 0);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, N, 0);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n", 
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_MKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_tgemm2
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
void Mjoin(PATL,tgemm2)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
/*
 * See if it looks like a rank-K problem that can be split on N */
 */
   if (K <= 4*NB && N > ATL_NTHREADS*NB && M < N+N) /* rank-K split on N */
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can really only be split on K?
 */
   if ( (K > 4*ATL_NTHREADS*NB && M < 2*NB && N < 2*NB) || 
        (K >= 4*M && K >= 4*N && M*(size_t)N*K*sizeof(TYPE) < ATL_PTMAXMALLOC) )
   {
      if (!Mjoin(PATL,tgemm_Kp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on M
 */
   if (M > N && M > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Mp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on N
 */
   if (N > M && N > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * For low-rank problems, use 3-d distribution with minimal copying.  This
 * should be a specialized routine where we know we can copy the entire
 * problem up front; no-copy routs should be handled by 1-D distros
 */
/*
 * Large-rank problems using 3-D with work stealing.
 */

/*
 * Otherwise, call serial gemm; this will later be replaced by _MpNp, which
 * is not allowed to fail (return non-zero).  Can we use work-stealing?
 */
   Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
}
@ROUT ATL_tgemm_Kp
   @define D @K@
   @define I @k@
@ROUT ATL_tgemm_Np
   @define D @N@
   @define I @j@
@ROUT ATL_tgemm_Mp
   @define D @M@
   @define I @i@
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
@define d @@low@(D)@
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
typedef struct
{
@ROUT ATL_tgemm_Kp
   TYPE **Cws;            /* P-len array of C work areas, Cws[0] = C */
   void *chklck;          /* 0/1 ctr serves as lock for chkin array */
   volatile int *chkin;   /* P-len checkin array, init to 0 */
   ATL_INT ldw;           /* leading dim of workspaces */
   void *lockC;           /* lock around C combine data structs */
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   void *a@(D)cnt;           /* which @(D) blk are we doing? */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT @(D)p, n@(D)p, @(d)last;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_@(D)PART_t;

@ROUT ATL_tgemm_Kp
static void DoCombine
(
   int iam,               /* rank of this thread */
   ATL_INT M,             /* # of rows in C */
   ATL_INT N,             /* # of cols in C */
   ATL_INT ldc,           /* leading dim of C (>=M) */
   ATL_INT ldw,           /* leading dim of all workspaces */
   volatile int *chkin,   /* check-in array initialized to 0 */
   void *lockC,           /* lock protecting the chkin array */
   TYPE **Cws             /* array of C workspaces to combine */
)
{
   #ifdef TCPLX
      TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
   int i, combrank;
   ATL_CINT myldc = (iam) ? ldw : ldc;
/*
 * See if there are any C arrays I can combine with mine
 */
   do
   {
      combrank = -1;
      for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
/*
 *    If there are no arrays available, lock the structure and signal that
 *    I have left the building by storing a 1 in my structure
 */
      if (i == ATL_NTHREADS)
      {
         if (iam == 0)  /* rank=zero loops until all matrices are combined */
         {
             for (i=1; i < ATL_NTHREADS && chkin[i] == 3; i++);
             if (i != ATL_NTHREADS)
                continue;
             else
                return;
         }
/*
 *       non-zero nodes simply leave if there are no matrices to combine
 */
         ATL_mutex_lock(lockC);
         for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         if (i == ATL_NTHREADS)
         {
            chkin[iam] = 1;   /* signal that my array is available to combine */
            ATL_mutex_unlock(lockC);
            return;
         }
         else
         {
            combrank = i;
            chkin[i] = 2;
         }
         ATL_mutex_unlock(lockC);
      }
/*
 *    I found an uncombined array, see if I can take it
 */
      else
      {
         ATL_mutex_lock(lockC);
         if (chkin[i] != 1)  /* if that one is gone, see if another is there */
         {
            for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         }
         if (i < ATL_NTHREADS)
         {
            combrank = i;
            chkin[i] = 2;
         }
         else if (iam)
         {
            chkin[iam] = 1;   /* signal that my array is available to comb */
            ATL_mutex_unlock(lockC);
            return;
         }
         else   /* nothing to combine & I'm node zero */
         {
            ATL_mutex_unlock(lockC);
            continue; /* nothing more to do until someone checks in */
         }
      }
/*
 *    If I get to here, then combrank should be set, and I must do the combine
 */
      Mjoin(ATL,geadd)(M, N, one, Cws[combrank], ldw, one, Cws[iam], myldc);
      chkin[combrank] = 3;  /* safe since I can only do this after lockC */
   }                        /* and nobody writes to loc with 2 in it but me */
   while(1);
}
#ifndef TCPLX
   #undef one
#endif


@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
void Mjoin(PATL,DoWork_part@(D))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_@(D)PART_t *pd=lp->vp;
   void *a@(D)cnt = pd->a@(D)cnt;
   const TYPE *A=pd->A, *B=pd->B;
   ATL_CINT iam = tp->rank, ANOTRANS = (pd->TA == AtlasNoTrans);
   ATL_CINT M=pd->M, N=pd->N, K=pd->K;
   ATL_CINT n@(D)p = pd->n@(D)p, @(D)p = pd->@(D)p, @(d)last=pd->@(d)last;
   ATL_CINT lda=pd->lda,  ldb=pd->ldb;
   const enum ATLAS_TRANS TA=pd->TA, TB=pd->TB;
   const SCALAR alpha=pd->alpha; 
@ROUT ATL_tgemm_Mp ATL_tgemm_Np
   TYPE *C=pd->C;
   ATL_CINT ldc = pd->ldc;
   const SCALAR beta=pd->beta;
@ROUT ATL_tgemm_Kp
   ATL_CINT ldc = (iam) ? pd->ldw : pd->ldc;
   TYPE *C = pd->Cws[iam];
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
      const TYPE zero[2] = {ATL_rzero, ATL_rzero};
      const TYPE *beta= (iam) ? zero : pd->beta;
   #else
      TYPE beta = (iam) ? ATL_rzero : pd->beta;
   #endif
   volatile int *chkin=pd->chkin;
   ATL_INT kblk;
   ATL_CINT amul = ((TA == AtlasNoTrans || TA == AtlasConj) ? lda : 1)SHIFT;
   ATL_CINT bmul = ((TB == AtlasNoTrans || TB == AtlasConj) ? 1 : ldb)SHIFT;

   while (kblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      size_t ka, kb;
      ATL_INT i, k;

      if (kblk-- == nKp)
      {
         k = klast;
         i = pd->K - k;
      }
      else
      {
         i = kblk*Kp;
         k = Kp;
      }
      ka = i*amul;
      kb = i*bmul;
      Mjoin(PATL,gemm)(TA, TB, M, N, k, alpha, A+ka, lda, B+ka, ldb, beta, 
                       C, ldc);
      #ifdef TCPLX
         beta = one;
      #else
         beta = ATL_rone;
      #endif
   }
   DoCombine(iam, M, N, pd->ldc, pd->ldw, pd->chkin, pd->lockC, pd->Cws);
@ROUT ATL_tgemm_Np
   ATL_CINT bmul = (TB == AtlasNoTrans) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT jblk;

   while (jblk = ATL_DecAtomicCount(aNcnt))
   {
      ATL_INT n;
      size_t jb, jc;

      if (jblk-- == nNp)
      {
         n = nlast;
         jc = pd->N - n;
      }
      else
      {
         jc = jblk*Np;
         n = Np;
      }
      jb = jc*bmul;
      jc *= (ldc SHIFT);
      Mjoin(PATL,gemm)(TA, TB, M, n, K, alpha, A, lda, B+jb, ldb, beta, 
                       C+jc, ldc);
   }
@ROUT ATL_tgemm_Mp
   ATL_INT iblk;

   while (iblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      ATL_INT i, m;

      if (iblk-- == nMp)
      {
         m = mlast;
         i = pd->M - m;
      }
      else
      {
         i = iblk*Mp;
         m = Mp;
      }
      a = (ANOTRANS) ? A+(i SHIFT) : A+i*(lda SHIFT);
      Mjoin(PATL,gemm)(TA, TB, m, N, K, alpha, a, lda, B, ldb, beta, 
                       C+(i SHIFT), ldc);
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
}

int Mjoin(PATL,tgemm_@(D)p)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine divides @(D) into roughly 4*P chunks, and calls serial GEMM.
 */
{
   ATL_INT n@(D)p, @(D)p, @(d)last;
   ATL_TGEMM_@(D)PART_t pd;  /* problem definition */
@ROUT ATL_tgemm_Kp 
   ATL_INT ldw, i;
   size_t totsz;
   void *vp=NULL;
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp

   @(D)p = @(D) / (2*ATL_NTHREADS);
   if (@(D)p < NB)
      return(0);
   @(D)p = (@(D)p >= NB+NB) ? @(D)p>>1 : @(D)p;
   n@(D)p = @(D) / @(D)p;
   @(d)last = @(D) - n@(D)p*@(D)p;
   if (@(d)last)
   {
      if (@(d)last < @(D)B)
         @(d)last += @(D)p;   /* absorb partial block into last full partition */
      else
         n@(D)p++;
   }
   else
      @(d)last = @(D)p;       /* last part same size as all parts */
   pd.M = M; pd.N = N; pd.K = K;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.TA = TA; pd.TB = TB;
   #ifdef TCPLX
      pd.alpha = (TYPE*)alpha; pd.beta = (TYPE*)beta;
   #else
      pd.alpha = alpha; pd.beta = beta;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.@(D)p = @(D)p; pd.n@(D)p = n@(D)p; pd.@(d)last = @(d)last;
   pd.a@(D)cnt = ATL_SetAtomicCount(n@(D)p);
@ROUT ATL_tgemm_Kp
  
   ldw = ((M+7)>>8)<<8;   /* make ldw a multiple of 8 */
   if (!(ldw & (ldw-1)))  /* if ldw is a power of 2 */
      ldw += 8;           /* change it so it is not */
   totsz = (ATL_MulBySize(ldw)*N+ATL_Cachelen) * (ATL_NTHREADS-1) + 
           ATL_NTHREADS*sizeof(TYPE*);
   if (totsz > ATL_PTMAXMALLOC*ATL_NTHREADS)
      return(1);
   pd.Cws = malloc(totsz);
   pd.Cws[0] = C;
   pd.ldw = ldw;
   pd.Cws[1] = (TYPE*) (pd.Cws + ATL_NTHREADS);
   pd.Cws[1] = ATL_AlignPtr(pd.Cws[1]);
   pd.chkin[0] = pd.chkin[1] = 0;  /* at least 2 threads, so safe */
   for (i=2; i < ATL_NTHREADS; i++)
   {
      void *vp;
      const size_t inc=ldw*N;
      vp = pd.Cws[i-1] + inc;
      pd.Cws[i] = ATL_AlignPtr(vp);
      pd.chkin[i] = 0;
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_part@(D)), &pd, NULL);
   return(0);
}
@ROUT ATL_tgemm_MpNpKp
int Mjoin(PATL,tgemm_Mp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT nmblks, nnblks, nkblks, nblks, m, n, k;
   int cutKOK;

   nmblks = nnblks = nkblks = 1;
   m = M; n = N; k = K;
   do
   {
/*
 *    cost of operation : (2*M*N*K)/P
 *    worst case combine: (P*M*N)
 *    If this ratio is large, then we can afford to cut K
 */
      cutKOK = (k+k)/((nkblks+1)*(nkblks+1)) >= 200;
      if (cutKOK && k > m && k > n)
      {
         nkblks++;
         k = K / nkblks;
         if (k < KB)
         {
            nkblks--;
            break;
         }
      }
      else if (n > m)
      {
         nnblks++;
         n = N / nnblks;
         if (n < NB)
         {
            nnblks--;
            break;
         }
      }
      else /* M largest dim, cut it */
      {
         nmblks++;
         m = M / nmblks;
         if (m < MB)
         {
            nmblks--;
            break;
         }
      }
      nblks = nmblks * nnblks * nkblks;
   }
   while (nblks < 2*ATL_NTHREADS);
}
@ROUT ATL_tgemm_rKp
typedef struct
{
   void **aNcnts;         /* P counters on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void *donecnt;         /* Counter used to detect if op is complete */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   volatile int Riam;     /* 1st arriver on cooperative operation */
   volatile int DONE;     /* set by Riam when cooperative op is complete */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKP_t;

void Mjoin(PATL,DoWork_NKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKP_t *pbdef=lp->vp;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Aw=pbdef->Bw, *Bw=pbdef->Bws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->nMb, nfNblks = nNb;
   ATL_CINT nr = pbdef->nr;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      int Ciam = iam+1;
      void *aCcnt = pbdef->aCcnt;

      j = ATL_DecAtomicCount(aCcnt);
      if (j == N)
      {
         pbdef.Riam  = iam;  /* I'm responsible for certifying C is finished */
      }
      if (j)
      {
         Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
         while (j = ATL_DecAtomicCount(aCcnt))
            Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
      }
      if (iam == pbdef.Riam)
      {
         pbdef.Riam = -1;
      }
      else
         while (!pbdef->DONE);
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecAtomicCount(aCcnt))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecAtomicCount(pbdef->aBcnt) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ?
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecAtomicCount(pbdef->aBcnt))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      }
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetAtomicCount(pbdef->aBcnt, nNb);  /* reset for next k-it */
         ATL_ResetAtomicCount(pbdef->aAcnt, nMp);  /* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr,
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }
}
int Mjoin(PATL,tgemm_rKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Aw, *p;
   void **acnts;
   void *amcnt, *vp;
   TYPE **Bws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_RKP_t pbdef;

/*
 * =====================================================================
 * Compute the Kp partitioning.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    M*Kp + 2*(NB*Kp + M*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
   }
   else  /* otherwise, its a rank-K update, call special routine */
      return(Mjoin(PATL,tgemm_rkK_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                      beta, C, ldc));
/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block 
 */
   nKp = K / Kp;        /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;   /* absorb partial block into last K block */
      else
         nKp++;         /* nKp now includes klast, which is > NB */
   }
   else
      klast = Kp;       /* K is even multiple of Kp */
   nMb = M/NB;          /* floor(M/NB) */
   nNb = N/NB;          /* floor(N/NB) */
   if (nNb < ATL_NTHREADS*2)
      return(3);
/*
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = ((klast+NB-1)/NB)*NB;
   k = Mmax(Kp, k);
   wrksz = (2+ATL_NTHREADS)*ATL_Cachelen + ATL_MulBySize(MB)*nMb*k +
           ATL_NTHREADS*(sizeof(void*)+sizeof(TYPE*) + ATL_MulBySize(NB)*k);
   if (ATL_NTHREADS*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Bws = vp;                            /* B work ptrs first array */
   acnts = (void*)(Bws+ATL_NTHREADS);   /* then atomic count array */
   Aw = (TYPE*)(acnts+ATL_NTHREADS);    /* then workspace for common A */
   Aw = ATL_AlignPtr(Aw);               /* A must be aligned */
   p = Aw + M*k;                        /* first B wrkspc after A wrkspc */
   Bws[0] = ATL_AlignPtr(p);            /* B wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      p = Bws[i-1] + N*k;
      Bws[i] = ATL_AlignPtr(p);
   }
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.nMb = nMb; pbdef.nNb = nNb;
   pbdef.mr = M - nMb * NB; pbdef.nr = N - nNb * NB;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Aw = Aw; pbdef.Bws = Bws;
   i = nr ? nNb+1 : nNb;
   pbdef.aBcnt = ATL_SetAtomicCount(i);
   pbdef.donecnt = ATL_SetAtomicCount(i);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetAtomicCount(N);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n",
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_NKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_gccstackfix
#include "atlas_asm.h"
/*
 * This routine necessary to fix gcc's 32-bit ABI violation:
 *    http://math-atlas.sourceforge.net/errata.html#gccCrazy
 * This assembly is only called on 32-bit Windows, where all windows calls
 * call this wrapper function, which takes a pointer to a void pointer array;
 * the first entry is a function pointer to the function usually threaded,
 * while the second is a pointer to its argument.  This assembly routine
 * manually aligns the stack so that gcc's ABI violation doesn't kill
 * the spawned thread.
 */

/*
 *                       4(%esp) 
 * void *ATL_gccstackfix(void *vp)
 */
        .text
.globl ATL_asmdecor(ATL_gccstackfix)
ALIGN16
ATL_asmdecor(ATL_gccstackfix):
#define FSIZE 12                /* (SP0, arg, func) */
ATL_gccstackfix:
   mov %esp, %eax               /* get unaligned stack ptr */
   and $0xF0, %al               /* align to 32 bytes */
   mov %esp, %edx               /* save original value of SP */
   lea -FSIZE(%eax), %esp       /* SP is aligned and has room for FSIZE */
   movl %edx, 4(%esp)           /* stack has original value of SP */
   movl 4(%edx), %eax           /* get ptr to func and arg ptrs */
   movl 4(%eax), %ecx           /* get arg ptr */
   movl %ecx, (%esp)            /* put arg as parameter for called func */
   movl 0(%eax), %ecx           /* get address of func to call */
   call *%ecx                   /* call func, pushing ret @ to stack */
   mov 4(%esp), %esp            /* restore stack ptr */
   ret
@ROUT ATL_tDistMemTouch
#include "atlas_misc.h"
#include "atlas_threads.h"

typedef struct
{
   int *mem;   /* ptr to memory location */
   size_t N;   /* length of mem in sizeof(int) chunks */
} ATL_dmt_t;

/*
 * This function will make it so that a first-touch allocation policy
 * still results in the pages of mem being allocated in cyclic fashion
 * amongst the nodes.  Without calling this function, all allocated memory
 * that is serially initialized will be owned by only one processor, which
 * can cause slowdown on parallel machines.  For AMD machines with HT-assist
 * putting all pages in one local memory essentially reduces the global
 * cache size to the local cache size, with a corresponding disastrous
 * drop in performance.
 * The first page is always assigned to rank=0; if you allocate a bunch of
 * small spaces, they will all be owned by rank=0!
 */
void ATL_dmt_DOWORK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   const int iam = tp->rank;
   ATl_dmt_t *pd = lp->opstruct;
   const size_t pgsz = 4096 / sizeof(int), stride=pgsz * ATL_NTHREADS;
   const size_t N = pd->N;
   int *mp = pd->mem, *end = pd->mem + N;

   mp += iam*pgsz;
   while (mp < end)
   {
      *mp = iam+1;
      mp += stride;
   }
}

void ATL_tDistMemTouch  /* use first-touch policy to force cyclic page distro */
(
   size_t N,   /* length of allocated memory */
   void *vp    /* allocated memory */
)
{
   ATL_dmt_t pd;
   pd.N = N / sizeof(int);
   pd.mem = vp;
   ATL_goparallel(ATL_NTHREADS, ATL_dmt_DOWORK, &pd, NULL);
}
@ROUT amm
struct amm
{
   int ntexp;     /* # of threads who must at least call before return */
   int ntactive;  /* # of threads who've checked in for work */
   int ntcomp;    /* # of threads presently doing computations */
   int ntdone;    /* # of threads idling for finish of computation */
   int ngcntK;    /* # of glocal counters used for K dims */
   int mb, nb, kb;/* blocking for kernel */
   int mr, nr, kr;/* size of partial last block.  if 0, size is full block */
   int nfmblks, nfnblks, nfkblks;  /* CEIL(N/NB) for each dim */
   int nmblks, nnblks, nkblks;  /* FLOOR(N/NB) for each dim */
   int nAcp, nBcp;             /* # of panels of A/B copied so far */
   int *bvA, *bvB; /* bitvecs for tracking copy of A/B, one bit per K-panel */
   int *bvC;  /* bitvec for blocks of C, which are our compute tasks */
   void **Mstart;  /* nmblks different counters for perf copy of panels of A */
   void **Nstart;  /* nnblks different counters for perf copy of panels of B */
   void **Mdone;   /* nmblks different counters for finishing copy of A */
   void **Ndone;   /* nmblks different counters for finishing copy of B */
}

void setuptAMM()
{
   int int ncnt, i, nfmblks, nfnblks, nfkblks;
   nfmblks = ap->nfmblks = M/mb;
   nfnblks = ap->nfnblks = N/nb;
   nfkblks = ap->nfkblks = K/kb;
   ap->mr = M - nfmblks*mb;
   ap->nr = N - nfnblks*nb;
   ap->kr = K - nfkblks*kb;
   ap->nmblks = ap->mr ? nfmblks+1 : nfmblks;
   ap->nnblks = ap->nr ? nfnblks+1 : nfnblks;
   ap->nkblks = ap->kr ? nfkblks+1 : nfkblks;
   ap->Mstart = malloc(sizeof(void*)*2*(nfnblks+nfmblks));
   ATL_assert(ap->Mstart);
   ap->Mdone = ap->Mstart + nfmblks;
   ap->Nstart = ap->Mdone + nfmblks;
   ap->Ndone = ap->Nstart + nfnblks;
   ncnt = (nfkblks > 64) ? nfkblks >> 5 : 1;
   #if ATL_NTHREADS > 8
      if (ncnt > (ATL_NTHREADS>>3))
         ncnt = ATL_NTHREADS>>3;
      ncnt = (ncnt > 8) ? 8 : ncnt;
   #else
      ncnt = 1;
   #endif
   ap->ngcntK = ncnt;
   for (i=0; i < nfmblks; i++)
   {
      ap->Mstart[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
      ap->Mdone[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
   }
   for (i=0; i < nfnblks; i++)
   {
      ap->Nstart[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
      ap->Ndone[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
   }
}
void FreetAMM()
{
   for (i=0; i < nfmblks; i++)
   {
      ATL_FreeGlobalAtomicCount(ap->Mstart[i]);
      ATL_FreeGlobalAtomicCount(ap->Mdone[i]);
   }
   for (i=0; i < nfnblks; i++)
   {
      ATL_FreeGlobalAtomicCount(ap->Nstart[i]);
      ATL_FreeGlobalAtomicCount(ap->Ndone[i]);
   }
   free(ap->Mstart);
}

{
   SELECTOR:
      TRY_TO_COMPUTE:
/*
 *       If the number computing equals the number of available jobs,
 *       I cannot do any computation yet
 */
         if (tp->njobs == tp->ncomp)
         {
            if (tp->njobs < tp->ntasks)  /* if possible, go enable more jobs */
               goto TRY_TO_COPY;
            else                         /* otherwise await completion */
               goto DONE;
         }
         else /* Looks like there's a job I can grab */
         {
            LOCK 
            if njobs == ncomp
            {
               UNLOCK
               bail as before;
            }
            get job, update data
            UNLOCK
            call job
         }
      TRY_TO_COPY:
/*
 *       If we've copied everything, can only do computation or finish
 */
         if (tp->nAcp == tp->nmblks && tp->nBcp == tp->nnblks)
            goto TRY_TO_COMPUTE;
   goto SELECTOR
DONE:
   LOCK
     tp->ntdone++;
   UNLOCK
   while(tp->ntdone < tp->ntexp)
      ATL_thread_yield();
}
@ROUT ATL_tammm_tMN
#define ATL_GLOBIDX  1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_level1.h"
#include "atlas_tlvl3.h"

#ifdef TCPLX
static void INLINE Mjoin(PATL,amm_tMN_k)
   (ipinfo_t *ip, size_t k, size_t bet, const TYPE *A, const TYPE *B, 
    TYPE *rA, TYPE *iA, TYPE *rB, TYPE *iB, TYPE *rC, TYPE *iC)
{
   const size_t nfkblks = ip->nfkblks;
   ATL_CUINT nmu=ip->nmuF, nnu=ip->nnuF;
   ATL_UINT kb = ip->kb;
   if (k != nfkblks || kb == ip->kb0)  /* normal kb size */
   {
      ammkern_t amm_b1=ip->amm_b1, amm_bn=ip->amm_bn;
      ip->b2blk(kb, ip->nF, ip->ONE, B+ip->incBk*k, ip->ldb, rB, iB);
      ip->a2blk(kb, ip->mF, ip->ONE, A+ip->incAk*k, ip->lda, rA, iA);
      if (bet)
      {
         amm_bn(nmu, nnu, kb, iA, iB, rC, rA, iB, iC);
         amm_b1(nmu, nnu, kb, rA, iB, iC, rA, rB, rC);
      }
      else
      {
         ammkern_t amm=ip->amm_b0;
         amm(nmu, nnu, kb, iA, iB, rC, rA, iB, iC);
         amm(nmu, nnu, kb, rA, iB, iC, rA, rB, rC);
      }
      amm_bn(nmu, nnu, kb, rA, rB, rC, iA, rB, iC);
      amm_b1(nmu, nnu, kb, iA, rB, iC, iA, iB, rC);
   }
   else /* KB0/kb0 size */
   {
      ammkern_t amm_b1=ip->ammK1_b1, amm_bn=ip->ammK1_bn;
      kb = ip->kb0;
      ip->b2blk(kb, ip->nF, ip->ONE, B+ip->incBk*nfkblks, ip->ldb, rB, iB);
      ip->a2blk(kb, ip->mF, ip->ONE, A+ip->incAk*nfkblks, ip->lda, rA, iA);
      kb = ip->KB0;
      if (bet)
      {
         amm_bn(nmu, nnu, kb, iA, iB, rC, rA, iB, iC);
         amm_b1(nmu, nnu, kb, rA, iB, iC, rA, rB, rC);
      }
      else
      {
         ammkern_t amm=ip->ammK1_b0;
         amm(nmu, nnu, kb, iA, iB, rC, rA, iB, iC);
         amm(nmu, nnu, kb, rA, iB, iC, rA, rB, rC);
      }
      amm_bn(nmu, nnu, kb, rA, rB, rC, iA, rB, iC);
      amm_b1(nmu, nnu, kb, iA, rB, iC, iA, iB, rC);
   }
}
#else
static void INLINE Mjoin(PATL,amm_tMN_k)
   (ipinfo_t *ip, size_t k, size_t bet, const TYPE *A, const TYPE *B, 
    TYPE *pA, TYPE *pB, TYPE *pC)
{
   const size_t nfkblks = ip->nfkblks;
   ATL_UINT kb = ip->kb;
   if (k != nfkblks || kb == ip->kb0)  /* normal kb size */
   {
      ip->b2blk(kb, ip->nF, ATL_rone, B+ip->incBk*k, ip->ldb, pB);
      ip->a2blk(kb, ip->mF, ATL_rone, A+ip->incAk*k, ip->lda, pA);
      if (bet)
         ip->amm_b1(ip->nmuF, ip->nnuF, kb, pA, pB, pC, pA, pB, pC);
      else
         ip->amm_b0(ip->nmuF, ip->nnuF, kb, pA, pB, pC, pA, pB, pC);
   }
   else /* KB0/kb0 size */
   {
      ip->b2blk(ip->kb0, ip->nF, ATL_rone, B+ip->incBk*nfkblks, ip->ldb, pB);
      ip->a2blk(ip->kb0, ip->mF, ATL_rone, A+ip->incAk*nfkblks, ip->lda, pA);
      if (bet)
         ip->ammK1_b1(ip->nmuF, ip->nnuF, ip->KB0, pA, pB, pC, pA, pB, pC);
      else
         ip->ammK1_b0(ip->nmuF, ip->nnuF, ip->KB0, pA, pB, pC, pA, pB, pC);
   }
}
#endif

void Mjoin(PATL,DoWork_amm_tMN)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tMN_t *pd = pp->PD;
   ipinfo_t *ip = pd->ip;
   const size_t nUblks=pd->nUblks, nblksKp=pd->nblksKp, nKp=pd->nKp;
   const size_t nPblks=nKp*nblksKp;
   size_t namm=0;
   const TYPE *A=pd->A, *B=pd->B;
   #ifdef TCPLX
      TYPE *iA = pd->w + vrank*pd->wsz, *rA, *iB, *rB, *iC, *rC;
   #else
      TYPE *pA = pd->w + vrank*pd->wsz, *pB, *pC;
   #endif

   #ifdef TCPLX
      rA = iA + ip->szA;
      iB = rA + ip->szA;
      iB = ATL_AlignPtr(iB);
      rB = iB + ip->szB;
      iC = rB + ip->szB;
      iC = ATL_AlignPtr(iC);
      rC = iC + ip->szC;
   #else
      pB = pA + ip->szA;
      pB = ATL_AlignPtr(pB);
      pC = pB + ip->szB;
      pC = ATL_AlignPtr(pC);
   #endif
   if (nPblks)
   {
      size_t kctr;
      while ( (kctr = ATL_DecGlobalAtomicCount(pd->PartKctr, vrank)) )
      {
         size_t k = (nKp - kctr)*nblksKp, kmax;
         for (kmax=k+nblksKp; k < kmax; k++)
         {
            #ifdef TCPLX
               Mjoin(PATL,amm_tMN_k)(ip, k, namm, A, B, rA, iA, rB, iB, rC, iC);
            #else
               Mjoin(PATL,amm_tMN_k)(ip, k, namm, A, B, pA, pB, pC);
            #endif
            namm++;
         }
      }
   }
   if (nUblks)
   {
      size_t kctr, kk = nPblks + nUblks;
      while ( (kctr = ATL_DecGlobalAtomicCount(pd->blkUctr, vrank)) )
      {
         size_t k = kk - kctr;
         #ifdef TCPLX
            Mjoin(PATL,amm_tMN_k)(ip, k, namm, A, B, rA, iA, rB, iB, rC, iC);
         #else
            Mjoin(PATL,amm_tMN_k)(ip, k, namm, A, B, pA, pB, pC);
         #endif
         namm++;
      }
   }
   if (namm)
   {
      ATL_mutex_lock(pd->Cmut);
      if (pd->ndone)             /* beta already applied */
      #ifdef TCPLX
         ip->blk2c_b1(ip->mF, ip->nF, ip->alpC, rC, iC, ip->ONE, pd->C,ip->ldc);
      #else
         ip->blk2c_b1(ip->mF, ip->nF, ip->alpC, pC, ATL_rone, pd->C, ip->ldc);
      #endif
      else                        /* I'm first, apply real beta */
      #ifdef TCPLX
         ip->blk2c(ip->mF, ip->nF, ip->alpC, rC, iC, pd->beta, pd->C, ip->ldc);
      #else
         ip->blk2c(ip->mF, ip->nF, ip->alpC, pC, pd->beta, pd->C, ip->ldc);
      #endif
      (pd->ndone)++;
      ATL_mutex_unlock(pd->Cmut);
   }
//   printf("%d: namm=%lu, wrk=[%p,%p]\n", vrank, (unsigned long) namm,
//          pA, (pA+pd->wsz));
}
/*
 * This routine should be used only when M <= LASTMB && N <= LASTNB, and it
 * therefore handles a simple inner-product to update a single block of C.
 * It requires minimal workspace of (szA+szB+szC)*P.
 */
int Mjoin(PATL,tammm_tMN)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   void *vp;
   size_t sz;
   ATL_UINT P, p;
   ATL_tamm_tMN_t pd;
   ipinfo_t ip;

   if (M > ATL_geAMM_LASTMB || N > ATL_geAMM_LASTNB ||
       K < Mmin(ATL_NTHREADS,8)*ATL_geAMM_LASTKB)
      return(2);  /* not correct shape! */

   P = Mjoin(PATL,tGetIPInfo_tMN)(&ip, ATL_NTHREADS, TA, TB, M, N, K, alpha,
                                  lda, ldb, beta, ldc);
   #if 0
      pd.nKp = 1;
      pd.nblksKp = ip.nfkblks;
   #elif 0
      pd.nKp = pd.nblksKp = 0;
   #else
      sz = (ip.nfkblks + 1)>>1;
      if (sz < P)
         pd.nKp = pd.nblksKp = 0;
      else if (sz >= (P<<4))
      {
         pd.nKp = 8;
         pd.nblksKp = sz>>3;
      }
      else if (sz >= (P<<3))
      {
         pd.nKp = 4;
         pd.nblksKp = sz>>2;
      }
      else
      {
         pd.nblksKp = 2;
         pd.nKp = sz>>1;
      }
   #endif
   pd.nUblks = (ip.nfkblks+1) - pd.nKp*pd.nblksKp;
   p = pd.nKp + pd.nUblks;
   P = Mmin(P,p);
//printf("     P=%d, nPblks=%d, nKp=%d, nblksP=%d, nUblks=%d\n", P, 
//       (int)(pd.nKp*pd.nblksKp), (int)pd.nKp, (int)pd.nblksKp,(int)(pd.nUblks));
   pd.wsz = ATL_MulBySize(ip.szA + ip.szB + ip.szC) + 3*ATL_Cachelen-1;
   pd.wsz = ATL_MulByCachelen(ATL_DivByCachelen(pd.wsz));
   vp = malloc(ATL_Cachelen + P*pd.wsz + ATL_MulBySize((ip.mu+ip.mu)*ip.nu));
   if (!vp)
      return(1);
   pd.wsz = ATL_DivBySize((pd.wsz SHIFT));
   pd.w = ATL_AlignPtr(vp);
   pd.ip = &ip;
   pd.beta = beta;
   pd.A = A;
   pd.B = B;
   pd.C = C;
   pd.Cmut = ATL_mutex_init();
   pd.ndone = 0;
   if (pd.nKp)
      pd.PartKctr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nKp, P), pd.nKp, 0);
   else
      pd.PartKctr = NULL;
   if (pd.nUblks)
      pd.blkUctr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nUblks, P), 
                                            pd.nUblks, 0);
   else
      pd.blkUctr = NULL;

   ATL_goParallel(P, Mjoin(PATL,DoWork_amm_tMN), NULL, &pd, NULL);
   if (pd.PartKctr)
      ATL_FreeGlobalAtomicCount(pd.PartKctr);
   if (pd.blkUctr)
      ATL_FreeGlobalAtomicCount(pd.blkUctr);
   ATL_mutex_free(pd.Cmut);
   free(vp);
   return(0);
}
@beginskip
void Mjoin(PATL,DoWork_amm_tMN)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tMN_t *pd = pp->PD;
   cm2am_t a2blk = pd->a2blk, b2blk = pd->b2blk;
   ammkern_t amm = pd->amm_b0;
   TYPE *wA = pd->w + vrank*pd->szW;
   TYPE *wB;
   TYPE *wC;
   const int nkblks = pd->nkblks, kb=pd->kb, KB0=pd->KB0, kb0=pd->kb0,
             M=pd->M, N=pd->N;
   const size_t incA = (pd->TA) ? 1:pd->lda;
   const size_t incB = (pd->TB) ? pd->ldb:1;
   int kctr;

   wA = ATL_AlignPtr(wA);
   wB = wA + pd->szA;
   wC = wB + pd->szB;
   while ((kctr = ATL_DecGlobalAtomicCount(pd->KbCtr, vrank)))
   {
      const int kblk = nkblks - kctr;
      size_t k;
@skip printf("%d,%d: nkblks=%d, kblk=%d\n", vrank,rank, nkblks, kblk);
/*
 *    Normal case, doing full K blocks
 */
      if (kblk)
      {
         size_t k = kb0 + kb*(kblk-1);
         const TYPE *a = pd->A + k*incA, *b = pd->B + k*incB;
         a2blk(kb, M, ATL_rone, a, pd->lda, wA);
         b2blk(kb, N, ATL_rone, b, pd->ldb, wB);
         amm(pd->nmu, pd->nnu, kb, wA, wB, wC, wA, wB, wC);
      }
/*
 *    Doing possibly partial kb0 at beginning
 */
      else
      {
         amm = (amm == pd->amm_b1) ? pd->ammK_b1 : pd->ammK_b0;
         a2blk(kb0, M, ATL_rone, pd->A, pd->lda, wA);
         b2blk(kb0, N, ATL_rone, pd->B, pd->ldb, wB);
         amm(pd->nmu, pd->nnu, KB0, wA, wB, wC, wA, wB, wC);
      }
      amm = pd->amm_b1;
   }
/*
 * If I did no work, zero my wC so combine isn't messed up!
 */
   if (amm == pd->amm_b0)
      Mjoin(PATL,zero)(pd->nC, wC, 1);
}

void Mjoin(PATL,DoComb_amm_tMN)
   (void *vpp, int rank, int vrank, int hisvrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tMN_t *pd = pp->PD;
   TYPE *mC, *hC;

   mC = pd->w + vrank*pd->szW;
   mC = ATL_AlignPtr(mC);
   mC += pd->szA + pd->szB;

   hC = pd->w + hisvrank*pd->szW;
   hC = ATL_AlignPtr(hC);
   hC += pd->szA + pd->szB;
   Mjoin(PATL,axpy)(pd->nC, ATL_rone, hC, 1, mC, 1);
}

/*
 * This routine handles the case where M <= maxMB && N <= maxNB so C is
 * only one block.  It gets its own case because it requires minimal
 * workspace.  K will need to fairly long, and due to streaming A & B
 * through the cache with only intra-kernel reuse, its parallel efficiency
 * tends to be limited by bus speed.
 */
int Mjoin(PATL,tammm_tMN)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   ATL_SZT nmblks;
   amminfo_t mminfo;
   unsigned int i, mb, nb, kb, kb0, KB0, nmu, nnu, mu, nu, ku, P, mr;
   ATL_INT nkblks;
   ATL_tamm_tMN_t pd;  /* problem definition structure */
   void *vp;
   TYPE *wC;

/*
 * Special case for tiny N&K, and large M
 */
   if (N > ATL_geAMM_LASTNB || M > ATL_geAMM_LASTMB ||
       K < Mmin(8,ATL_NTHREADS)*ATL_geAMM_LASTKB)
      return(1);
/*
 * Not uncommon to recur and hit this degenerate case that's a dot product
 */
   if (M == 1 && N == 1)
   {
      TYPE dot;
      dot = Mjoin(PATL,dot)(K, A, (TA == AtlasNoTrans)?lda:1, 
                            B, (TB == AtlasNoTrans) ? 1:ldb);
      if (SCALAR_IS_ZERO(beta))
         *C = alpha * dot;
      else
         *C = alpha * dot + beta * (*C);
      return(0);
   }
   nkblks = K / ATL_sqAMM_66KB;
   mb = Mjoin(PATL,tGetAmmmInfo)(&mminfo, Mmin(nkblks, ATL_NTHREADS), TA,
                                 TB, M, N, K, alpha, beta);

   if (!SCALAR_IS_ONE(alpha)) { ATL_assert(mb == 2); }
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   pd.amm_b0 = mminfo.amm_b0;
   pd.amm_b1 = mminfo.amm_b1;
   pd.TA = (TA == AtlasTrans);
   pd.TB = (TB == AtlasTrans);
   pd.M = M;
   pd.N = N;
   pd.K = K;
   pd.A = A;
   pd.B = B;
   pd.C = C;
   pd.lda = lda;
   pd.ldb = ldb;
   pd.ldc = ldc;
   mu = mminfo.mu;
   nu = mminfo.nu;
   ku = mminfo.ku;
   pd.nmu = nmu = (M+mu-1)/mu;
   pd.nnu = nnu = (N+nu-1)/nu;
   kb = pd.kb = mminfo.kb;
   nkblks = K / kb;
   KB0 = kb0 = K - nkblks*kb;
   if (!kb0)
   {
      kb0 = KB0 = kb;
      pd.ammK_b0 = mminfo.amm_b0;
      pd.ammK_b1 = mminfo.amm_b1;
   }
   else
   {
      nkblks++;
      #if ATL_geAMM_MAXKVEC > 1
         if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         {
            KB0 = ((kb0+ku-1)/ku)*ku;
            if (ATL_AMMFLG_KRUNTIME(mminfo.flag))
            {
               pd.ammK_b0 = mminfo.amm_b0;
               pd.ammK_b1 = mminfo.amm_b1;
            }
            else
            {
               pd.ammK_b0 = mminfo.amm_k1_b0;
               pd.ammK_b1 = mminfo.amm_k1_b1;
            }
         }
         else
      #endif
      {
         if (ATL_AMMFLG_KRUNTIME(mminfo.flag) && kb0 == (kb0/ku)*ku &&
             kb0 > mminfo.kbmin)
         {
            pd.ammK_b0 = mminfo.amm_b0;
            pd.ammK_b1 = mminfo.amm_b1;
         }
         else
         {
            pd.ammK_b0 = mminfo.amm_k1_b0;
            pd.ammK_b1 = mminfo.amm_k1_b1;
         }
      }
   }
   pd.nkblks = nkblks;
   pd.KB0 = KB0;
   pd.kb0 = kb0;
/*
 * Maximum scale is limited by NTHREADS or max number of K-blocks
 */
   P = (ATL_NTHREADS <= nkblks) ? ATL_NTHREADS : nkblks;
   mb = nmu * mu;
   nb = nnu * nu;
   pd.nC = mb*nb;
   pd.szA = mb*kb;
   pd.szB = nb*kb;
   pd.szW = pd.szA + pd.szB + pd.nC + ATL_Cachelen;

   vp = malloc(ATL_MulBySize(pd.szW*P + mu*nu*ku));
   if (!vp)
      return(2);
   pd.w = vp;
   pd.KbCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nkblks, P), nkblks, 0);

/*   #define DEBUG1 */
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoWork_amm_tMN)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoWork_amm_tMN), Mjoin(PATL,DoComb_amm_tMN),
                     &pd, NULL);
   #endif
   ATL_FreeGlobalAtomicCount(pd.KbCtr);
/*
 * Copy answer back out while scaling by alpha and beta
 */
   wC = ATL_AlignPtr(pd.w);
   wC += pd.szA + pd.szB;
   mminfo.Cblk2cm(M, N, alpha, wC, beta, C, ldc);

   free(vp);
   return(0);
}
@endskip
@ROUT ATL_ttrsm_amm
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"

static void cpyAblk(ATL_ttrsm_amm_t *pd)
{
   const int mb=pd->mb;
   int actr, ablk, kb;
   size_t i, j;
   const TYPE *a=pd->A;
   TYPE *wA;

   actr = ATL_DecAtomicCount(pd->AblkCtr);
   if (!actr)
      return;
   ablk = pd->nablks - actr;
   wA = pd->wA + ablk*pd->blkszA;
   Mtblk2coord(pd->nmblks, ablk, i, j);
   i = pd->mb0 + (i-1)*mb;
   if (j)
   {
      j = pd->mb0 + (j-1)*mb;
      kb = mb;
   }
   else
      kb = pd->mb0;
   if (pd->uplo == AtlasLower)
      a += j*pd->lda + i;
   else
      a += i*pd->lda + j;
   pd->a2blk(kb, mb, ATL_rnone, a, pd->lda, wA);
   ATL_mutex_lock(pd->Acpymut);
   ATL_SetBitBV(pd->AcpyBV, ablk);
   if (ATL_FindFirstUnsetBitBV(pd->AcpyBV, 0) == -1)
      pd->AcpyDone = 1;
   ATL_mutex_unlock(pd->Acpymut);
}

#define trsmK Mjoin(PATL,trsm)
void Mjoin(PATL,DoTrsm_amm)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_amm_t *pd = pp->PD;
   TYPE *wrk;
   int rhs;
   void *rhsCtr = pd->rhsCtr;

   wrk = pd->w + vrank*pd->wsL;
   if ((rhs = ATL_DecGlobalAtomicCount(rhsCtr, vrank)))
   {
      TYPE *wB=wrk, *X = pd->X;
      TYPE alpha = pd->alpha;
      const size_t ldx=pd->ldx, lda=pd->lda;
      const int mb=pd->mb, mb0=pd->mb0, MB0=pd->MB0, nmu=pd->nmu;
      const int nmblks=pd->nmblks, nnblks=pd->nnblks;
      const int blkszC = pd->blkszB, blkszA = pd->blkszA;
      const enum ATLAS_DIAG diag = pd->diag;
      const enum ATLAS_DIAG uplo = pd->uplo;
      const enum ATLAS_TRANS TA=pd->TA;
      cm2am_t b2blk = pd->b2blk;
      ablk2cmat_t blk2c=pd->blk2c;
      ammkern_t amm_b0=pd->amm_b0, amm = pd->amm_b1;

      do
      {
         const int rblk = nnblks - rhs;
         const int nb = (rblk == nnblks-1) ? pd->nbf : pd->nb;
         const int nnu = (rblk == nnblks-1) ? pd->nnuf : pd->nnu;
         TYPE *x = X + ldx*rblk*pd->nb;
         TYPE *wA=pd->wA, *wC=wB+blkszC;
         const TYPE *A = pd->A;
/*
 *       Solve top block using first diag blk of A (no need to copy)
 */
         trsmK(AtlasLeft, uplo, TA, diag, mb0, nb, alpha, A, lda, x, ldx);
/*
 *       If there are more RHS blocks, subtract solved X from them
 */
         if (nmblks > 1)
         {
            int i;
            TYPE *wc=wC, *a = wA;
            int ba=0;
/*
 *          Copy solved part of X, and subtract it from C, which will later be
 *          used to update the B values before solving them to X
 */
            b2blk(mb0, nb, ATL_rone, x, ldx, wB);
            for (i=1; i < nmblks; i++, ba++)
            {
               TYPE *cn = (i != nmblks-1) ? wc + blkszC:wC, *an = a + blkszA;
/*
 *             If the block of A we need hasn't been copied, copy A until it is.
 */
               if (!pd->AcpyDone)
               {
                  while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
                     cpyAblk(pd);
               }
               amm_b0(nmu, nnu, MB0, a, wB, wc, an, wB, cn);
               wc = cn;
               a = an;
            }
/*
 *          For each remaining RHS block, we solve it, then subtract its
 *          part of the equation from all the blocks below it, until alg
 *          finishes.
 */
            A += mb0*(lda+1);
            x += mb0;
            for (i=1; i < nmblks; i++)
            {
/*
 *             Apply alpha to X, and then subtract of solved equations, before
 *             solving using the current mbxmb diagonal block of A to form X
 */
               blk2c(mb, nb, ATL_rone, wC, alpha, x, pd->ldx);
               wC += blkszC;
               trsmK(AtlasLeft, uplo, TA, diag, mb, nb, ATL_rone, A, lda, 
                     x, ldx);
/*
 *             Subtract solved equations from remaining RHS blocks
 */
               if (i != nmblks-1)
               {
                  int k;

                  b2blk(mb, nb, ATL_rone, x, ldx, wB);
                  wc = wC;
                  for (k=i+1; k < nmblks; k++, ba++)
                  {
                     TYPE *cn = (k != nmblks-1) ? wc+blkszC : wC; 
                     TYPE *an = a + blkszA;
                     if (!pd->AcpyDone)
                     {
                        while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
                           cpyAblk(pd);
                     }
                     amm(nmu, nnu, mb, a, wB, wc, an, wB, cn);
                     wc = cn;
                     a = an;
                  }
               }
               A += mb*(lda+1);
               x += mb;
            }
         }
      }
      while ((rhs = ATL_DecGlobalAtomicCount(rhsCtr, vrank)));
   }
/*
 * See if any A remains to be copying before finishing; only if I never got
 * any work is it possible to reach here before A is completely copied.
 */
   else if (!pd->AcpyDone)
      while (ATL_GetAtomicCount(pd->AblkCtr))
         cpyAblk(pd);
}

/*
 * This routine handles case where A is on left of B, A X = B
 */
static int ttrsm_ammL
   (const enum ATLAS_UPLO uplo, const enum ATLAS_TRANS TA, 
    const enum ATLAS_DIAG diag, ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_amm_t pd;
   int P = (ATL_TP_PTR) ? ATL_TP_PTR->nthr : ATL_NTHREADS;
   int i;
   size_t sz, szA, szL;
   void *vp;

   P = Mjoin(PATL,tGetTrsmInfo)(&pd, P, TA, M, N, alpha);
   if (P < 2)
   {
      Mjoin(PATL,trsm)(AtlasLeft, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return(0);
   }
// printf("M=%d, N=%d, P=%d, mb=%d, nb=%d\n", M, N, P, pd.mb, pd.nb);
   pd.nablks = sz = ((pd.nmblks-1)*pd.nmblks)>>1;
   if (sz != pd.nablks)    /* if # of blocks overflow ints */
      return(1);              /* tell caller to recur until it doesn't */
   if (((size_t)pd.nmblks)*pd.nnblks != pd.nxblks)
      return(1);
   pd.TA = TA;
   pd.uplo = uplo;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.lda = lda;
   pd.ldx = ldb;
   pd.A = A;
   pd.X = B;
   pd.blkszB = pd.mb * pd.nb;
   pd.blkszA = pd.mb * pd.mb;
   pd.panszC = (pd.nmblks-1) * pd.blkszB;
   szA = pd.nablks * pd.blkszA;
   pd.wsL = szL = pd.blkszB + pd.panszC;
   sz = szA + P*szL + pd.mu*pd.nu*pd.ku;
   sz = ATL_MulBySize(sz) + ATL_Cachelen;
   if (sz > ATL_PTMAXMALLOC)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   pd.AcpyDone = (pd.mb >= M);
   pd.wA = ATL_AlignPtr(vp);
   pd.w = pd.wA + szA;
/*
 * Select the number of cores to perform A copy: to many and they just
 * fight for the bus, to few and algorithm starts slower
 */
   if (P >= 8)
   {
      i = P>>2;
      i = (i > 4) ? 4 : i;
   }
   else if (P >= 4)
      i = P>>1;
   else
      i = P;
   pd.AblkCtr = ATL_SetAtomicCount(pd.nablks);  /* w/o glob, is in-order */
   pd.rhsCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nnblks,P), pd.nnblks, 0);
   pd.AcpyBV = ATL_NewBV(pd.nablks);
   pd.Acpymut = ATL_mutex_init();
/*   #define DEBUG1  */
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoTrsm_amm)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoTrsm_amm), NULL, &pd, NULL);
   #endif
   ATL_FreeAtomicCount(pd.AblkCtr);
   ATL_FreeGlobalAtomicCount(pd.rhsCtr);
   ATL_FreeBV(pd.AcpyBV);
   ATL_mutex_free(pd.Acpymut);
   free(vp);
   return(0);
}

/*
 * Simple recursion for Left, Lower, Notrans case (used in LU)
 */
#include "atlas_tcacheedge.h"
#if CacheEdge > ATL_PTMAXMALLOC_MB*1024*1024
   #undef CacheEdge
#endif
#ifndef CacheEdge
   #define CacheEdge 524288
#endif

static void trsmREC_LLN
   (const int STOP_EARLY, const enum ATLAS_DIAG diag, ATL_CSZT M, ATL_CSZT N,
    const SCALAR alpha, const TYPE *A, ATL_CSZT lda, TYPE *B, ATL_CSZT ldb)
{
   ATL_CSZT Ml=M>>1, Mr = M-Ml;
   TYPE *B1=B+(Ml SHIFT);
   const TYPE *A10=A+(Ml SHIFT), *L11=A10+Ml*(lda SHIFT);
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
      TYPE NONE[2] = {ATL_rnone, ATL_rzero};
   #else
      #define ONE ATL_rone
      #define NONE ATL_rnone
   #endif
   size_t sz;  /* size(A) + 2*pan(B) */
/*
 * Stop recursion when A & panel of B fit in cache
 */
   sz = ((M*M)>>1) + M*(ATL_sqAMM_98KB<<1);
   sz = ATL_MulBySize(sz);
   if (STOP_EARLY || sz < CacheEdge)
   {
      if (!ttrsm_ammL(AtlasLower, AtlasNoTrans, diag, M, N, alpha, 
                      A, lda, B, ldb))
         return;
/*
 *    If we can't allocate any space, call serial & hope it can work
 */
      if (M < 80)
      {
         Mjoin(PATL,trsm)(AtlasLeft, AtlasLower, AtlasNoTrans, diag, M, N, 
                          alpha, A, lda, B, ldb);
         return;
      }
   }
   trsmREC_LLN(STOP_EARLY, diag, Ml, N, alpha, A, lda, B, ldb);
   Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, Mr, N, Ml, NONE, A10, lda,
                     B, ldb, alpha, B1, ldb);
   trsmREC_LLN(STOP_EARLY, diag, Mr, N, ONE, L11, lda, B1, ldb);
}

static void trsmREC_LUT
   (const int STOP_EARLY, const enum ATLAS_DIAG diag, ATL_CSZT M, ATL_CSZT N,
    const SCALAR alpha, const TYPE *A, ATL_CSZT lda, TYPE *B, ATL_CSZT ldb)
{
   ATL_CSZT Ml=M>>1, Mr = M-Ml;
   TYPE *B1=B+(Ml SHIFT);
   const TYPE *A10=A+(Ml*lda SHIFT), *L11=A10+(Ml SHIFT);
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
      TYPE NONE[2] = {ATL_rnone, ATL_rzero};
   #else
      #define ONE ATL_rone
      #define NONE ATL_rnone
   #endif
   size_t sz;  /* size(A) + 2*pan(B) */
/*
 * Stop recursion when A & panel of B fit in cache
 */
   sz = ((M*M)>>1) + M*(ATL_sqAMM_98KB<<1);
   sz = ATL_MulBySize(sz);
   if (STOP_EARLY || sz < CacheEdge)
   {
      if (!ttrsm_ammL(AtlasUpper, AtlasTrans, diag, M, N, alpha, 
                      A, lda, B, ldb))
         return;
/*
 *    If we can't allocate any space, call serial & hope it can work
 */
      if (M < 80)
      {
         Mjoin(PATL,trsm)(AtlasLeft, AtlasUpper, AtlasTrans, diag, M, N, 
                          alpha, A, lda, B, ldb);
         return;
      }
   }
   trsmREC_LUT(STOP_EARLY, diag, Ml, N, alpha, A, lda, B, ldb);
   Mjoin(PATL,tgemm)(AtlasTrans, AtlasNoTrans, Mr, N, Ml, NONE, A10, lda,
                     B, ldb, alpha, B1, ldb);
   trsmREC_LUT(STOP_EARLY, diag, Mr, N, ONE, L11, lda, B1, ldb);
}

#ifndef TCPLX
   #undef ONE
   #undef NONE
#endif

#ifdef ATL_ARCH_XeonPHI
  #define ATL_SE 1
#else
  #define ATL_SE 0
#endif
int Mjoin(PATL,ttrsm_amm)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   if (side == AtlasLeft)
   {
      if (uplo == AtlasLower && TA == AtlasNoTrans)
      {
         trsmREC_LLN(ATL_SE, diag, M, N, alpha, A, lda, B, ldb);
         return(0);
      }
      if (uplo == AtlasUpper && TA == AtlasTrans)
      {
         trsmREC_LUT(ATL_SE, diag, M, N, alpha, A, lda, B, ldb);
         return(0);
      }
   }
   return(1);
}
@ROUT bad_ttrsm
static void SolveAllRemainingRows(ATL_ttrsm_amm_t *pd, int vrank, int xctr)
{
   const int mb=pd->mb, mb0=pd->mb0, nnblks=pd->nnblks, nxblks=pd->nxblks,
      nmu=pd->nmu, blkszA=pd->blkszA, blkszB=pd->blkszC,
      panszB=pd->panszB, nmblks=pd->nmblks, Bn=pd->nb;
   TYPE *X, *C = pd->wCs + vrank*pd->blkszC;
   ammkern_t amm=pd->amm_b1;
   ablk2cmat_t blk2c=pd->blk2c;

   do
   {
      const int xblk = nxblks - xctr, i = xblk / nnblks, j = xblk - i*nnblks;
      const int nb  =(j < nnblks-1) ? Bn : pd->nbf, 
                nnu =(j < nnblks-1) ? pd->nnu : pd->nnuf;
      int k, bb;
      const TYPE *a;
/*
 *    Loop over all row panels I'm waiting on to come into B
 */
      for (bb=k=0; k < i; k++, bb += nnblks)
      {
         int ba, ibn = bb+j+nnblks;
/*
 *       Wait until B(k,j) is available, and copy A while waiting
 */
         while(!ATL_IsBitSetBV(pd->BcpyBV, bb+j))
         {
            if (!pd->AcpyDone)
               cpyAblk(pd);
            else
               ATL_thread_yield();
         }
/*
 *       If the block of A I need hasn't been copied, copy until it is
 *       We get the lock to force memory barrier on weakly-ordered caches,
 *       so that we don't waste time copying A while waiting for the BV
 *       to be updated
 */
         ba = Mcoord2tblk(nmblks, i, k);
         if (!pd->AcpyDone)
         {
            while(!pd->AcpyDone && !ATL_IsBitSetBV(pd->AcpyBV, ba))
               cpyAblk(pd);
@beginskip
            ATL_mutex_lock(pd->Acpymut);
            while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
            {
               ATL_mutex_unlock(pd->Acpymut);
               cpyAblk(pd);
               ATL_mutex_lock(pd->Acpymut);
            }
            ATL_mutex_unlock(pd->Acpymut);
@endskip
         }
/*
 *       I've got both A & B I need, so time to put the solved equation
 *       subtractions into C for later updating of my RHS
 */
         if (k)
         {
            TYPE *a = pd->wA + ba*blkszA;
            TYPE *b = pd->wB + j*panszB + k*blkszB, *bn=b;
            if (ibn < nxblks-nnblks && ATL_IsBitSetBV(pd->BcpyBV, ibn))
               bn = b + blkszB;
            amm(nmu, nnu, mb, a, b, C, a+blkszA, bn, C);
         }
         else /* first row-panel of size mb0, may require special kernel */
         {
            TYPE *a = pd->wA + ba*blkszA;
            TYPE *b = pd->wB + j*panszB, *bn=b;
            if (ibn < nxblks-nnblks && ATL_IsBitSetBV(pd->BcpyBV, bb+j+nnblks))
               bn = b + blkszB;
            pd->amm_b0(nmu, nnu, pd->MB0, a, b, C, a+blkszA, bn, C);
         }
      }  /* end loop over B blocks I need to subtract from my RHS */
/* 
 *    OK, I've got all updates in C, apply them and original alpha to my RHS,
 *    then do solve with diagonal block to produce final X
 */
      X = pd->X + pd->mb0 + (i-1)*mb + pd->ldx*Bn*j;
      a = pd->A + (pd->lda+1)*(pd->mb0 + (i-1)*mb);
      blk2c(mb, nb, ATL_rone, C, pd->alpha, X, pd->ldx);
      trsmK(AtlasLeft, pd->uplo, pd->TA, pd->diag, mb, nb, ATL_rone, 
            a, pd->lda, X, pd->ldx);
/*
 *    If I'm not the last row panel, copy my final X to C for updates
 */
      if (i != nmblks-1)
      {
         TYPE *b = pd->wB + j*panszB + i*blkszB;
         pd->b2blk(mb, nb, ATL_rone, X, pd->ldx, b);
         ATL_mutex_lock(pd->Bcpymut);
         ATL_SetBitBV(pd->BcpyBV, bb+j);
         ATL_mutex_unlock(pd->Bcpymut);
      }
   }
   while ((xctr = ATL_DecAtomicCount(pd->XblkCtr)));
}

void Mjoin(PATL,DoTrsm_amm)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_amm_t *pd = pp->PD;
   int xctr;
   const int nnblks=pd->nnblks, nxblks=pd->nxblks;

   xctr = ATL_DecAtomicCount(pd->XblkCtr);
   if (xctr)
   {
      unsigned int xblk = nxblks - xctr;
      unsigned int i = xblk / nnblks;
/*
 *    Doing first mb0 row panel! 
 */
      while (!i)
      {
         const unsigned int j = xblk - i*nnblks;
         const int nb = (j == nnblks-1) ? pd->nbf : pd->nb, mb0 = pd->mb0;
         TYPE *x, *b;
/*
 *       Solve X0 = inv(L00) * B0
 */
         x = pd->X + pd->ldx*j*pd->nb;
         trsmK(AtlasLeft, pd->uplo, pd->TA, pd->diag, mb0, nb, pd->alpha, 
               pd->A, pd->lda, x, pd->ldx);
/*
 *       Move X into GEMM B storage for updates, and report ready to update
 */
         if (pd->nmblks > 1)
         {
            b = pd->wB + xblk*pd->panszB;
            pd->b2blk(mb0, nb, ATL_rone, x, pd->ldx, b);
            ATL_mutex_lock(pd->Bcpymut);
            ATL_SetBitBV(pd->BcpyBV, xblk);
            ATL_mutex_unlock(pd->Bcpymut);
         }
         xctr = ATL_DecAtomicCount(pd->XblkCtr);
         xblk = nxblks - xctr;
         i = xblk / nnblks;
      }   /* end while(working on 1st row panel) */
      if (xctr)
         SolveAllRemainingRows(pd, vrank, xctr);
   }     /* end if (xctr) */
/*
 * If all work on RHS has been claimed, finish up any A copying
 */
   while (!pd->AcpyDone)
      cpyAblk(pd);
}

/*
 * This routine handles case where A is on left of B, A X = B
 */
static int ttrsm_ammL
   (const enum ATLAS_UPLO uplo, const enum ATLAS_TRANS TA, 
    const enum ATLAS_DIAG diag, ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_amm_t pd;
   int P;
   size_t sz, szA, szB;
   void *vp;

   P = Mjoin(PATL,tGetTrsmInfo)(&pd, TA, M, N, M, alpha);
   pd.actpan = 0;
   pd.nablks = sz = ((pd.nmblks-1)*pd.nmblks)>>1;
   if (sz != pd.nablks)    /* if # of blocks overflow ints */
      return(1);              /* tell caller to recur until it doesn't */
   if (((size_t)pd.nmblks)*pd.nnblks != pd.nxblks)
      return(1);
   pd.TA = TA;
   pd.uplo = uplo;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.lda = lda;
   pd.ldx = ldb;
   pd.A = A;
   pd.X = B;
   pd.blkszC = pd.mb * pd.nb;
   pd.blkszA = pd.mb * pd.mb;
   pd.panszB = (pd.nmblks-1) * pd.blkszC;
   szA = pd.nablks * pd.blkszA;
   szB = pd.panszB * pd.nnblks;
   sz = szA + szB;
   sz += P * pd.blkszC + pd.mu*pd.nu*pd.ku;  /* 1 C block per thread */
   sz = ATL_MulBySize(sz) + ATL_Cachelen;
   if (sz > ATL_PTMAXMALLOC)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   pd.AcpyDone = (pd.mb >= M);
   pd.wA = ATL_AlignPtr(vp);
   pd.wB = pd.wA + szA;
   pd.wCs = pd.wB + szB;
   pd.XblkCtr = ATL_SetAtomicCount(pd.nxblks);  /* w/o glob, is in-order */
   pd.AblkCtr = ATL_SetAtomicCount(pd.nablks); 
   pd.BcpyBV = ATL_NewBV(pd.nxblks-pd.nnblks);
   pd.AcpyBV = ATL_NewBV(pd.nablks);
   pd.Bcpymut = ATL_mutex_init();
   pd.Acpymut = ATL_mutex_init();
//   #define DEBUG1 
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoTrsm_amm)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoTrsm_amm), NULL, &pd, NULL);
   #endif
   ATL_FreeAtomicCount(pd.XblkCtr);
   ATL_FreeAtomicCount(pd.AblkCtr);
   ATL_FreeBV(pd.BcpyBV);
   ATL_FreeBV(pd.AcpyBV);
   ATL_mutex_free(pd.Bcpymut);
   ATL_mutex_free(pd.Acpymut);
   free(vp);
   return(0);
}


@beginskip
@endskip

int Mjoin(PATL,ttrsm_amm)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   if (side == AtlasLeft && uplo == AtlasLower)
      return(ttrsm_ammL(uplo, TA, diag, M, N, alpha, A, lda, B, ldb));
   return(1);
}
@ROUT ATL_tgemm_amm ATL_tammm_tNK ATL_tammm_G ATL_tammm_tK ATL_tammm_sK
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"
#include "atlas_cbc.h"

@ROUT ATL_tammm_tNK
void Mjoin(PATL,DoWork_tamm_tNK)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_tamm_tNK_t *pd = lp->opstruct;  /* parallel prob def struct */
   rkinfo_t *rp = pd->rp;
   size_t ldc=rp->ldc, lda=rp->lda;
   ATL_CUINT rank = tp->rank, nfmblks=rp->nfmblks, npmblks=rp->npmblks;
   ATL_CUINT nmblks=nfmblks+npmblks, kb = rp->KB, K=rp->kb;
   TYPE *pB = pd->wB, *pA = pd->w + rank*pd->wsz, *pC, *C=pd->C;
   const TYPE *A=pd->A;
   #ifdef TCPLX
      TYPE *rC;
   #else
      #define rC pC
   #endif

   ATL_UINT BCOPIED=0, mb, nb, nmu, nnu;
   ATL_CUINT N = (rp->nfnblks) ? rp->nb : rp->nF;
   ablk2cmat_t blk2c = rp->blk2C;
   cm2am_t a2blk = rp->a2blk;
   int ictr;
   #ifdef TCPLX
      ammkern_t amm_b0=rp->amm_b0, amm_b1=rp->amm_b1, amm_bn=rp->amm_bn;
   #else
      ammkern_t amm=rp->amm_b0;
   #endif

   pC = pA + ((rp->szA)SHIFT);
   pC = ATL_AlignPtr(pC);
   #ifdef TCPLX
      rC = pC + rp->szC;
   #endif
/*
 * First guy here starts to copy B
 */
   if (ATL_DecAtomicCount(pd->BassgCtr))
   {
/*
 *    Copy B, which is known to be only one block in this algorithm
 */
      Mjoin(PATL,opblk)(rp, 0, 0, NULL, pd->B, NULL, NULL, NULL, pB, NULL, 
                        NULL, NULL);
/*
 *    Let waiting threads know B is ready for use, memsync for weakly-ordered
 */
      #if ATL_CBC_STRONG
         BCOPIED = ATL_DecAtomicCount(pd->BdoneCtr);
         ATL_assert(BCOPIED);
      #else
         ATL_cbc_barrier(pd->P, rank, NULL);
         BCOPIED = 1;
      #endif
   }
/*
 * For first A block I work on, I must await completion of B copy 
 */
   if (!BCOPIED)
   {

      ictr = ATL_DecGlobalAtomicCount(pd->MbCtr, rank);
      if (ictr)
      {
         int iblk = nmblks - ictr;
/*
 *       Copy A, then await B done ACK
 */
         Mjoin(PATL,opblk)(rp, iblk, 0, A, NULL, NULL, pA, NULL, NULL, NULL, 
                           NULL, NULL);
         #if ATL_CBC_STRONG
            while (ATL_GetAtomicCount(pd->BdoneCtr))  /* await B cpy finish */
               ATL_thread_yield();
         #else
            ATL_cbc_barrier(pd->P, rank, NULL);
         #endif
/*
 *       Now multiply global B * local A, and write to my piece of C
 */
         Mjoin(PATL,opblk)(rp, iblk, 0, NULL, NULL, C, pA, pA, pB, pB, rC, pC);
      }
   }
/*
 * Now, B is ready, so just go to town on remaining blocks
 */
   while ((ictr = ATL_DecGlobalAtomicCount(pd->MbCtr, rank)))
   {
      Mjoin(PATL,opblk)(rp, nmblks-ictr, 0, A, NULL, C, pA, pA, pB, pB, rC, pC);
   }
}
#ifndef TCPLX
   #undef rC
#endif
/*
 * This routine handles the case where N <= maxNB && K <= maxKB, so B is
 * only one block.  It is particularly important for the panel factorizations
 * of both LU and QR.
 */
int Mjoin(PATL,tammm_tNK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   rkinfo_t rki;
   ATL_tamm_tNK_t tnk;
   size_t nmblks;
   void *vp;

   if (N >= ATL_rkAMM_LASTNB || K >= ATL_rkAMM_LASTKB || M < ATL_rkAMM_LASTMB ||
       M < Mmin(8,ATL_NTHREADS)*ATL_rkAMM_LASTMB)
      return(1);
   tnk.P = Mjoin(PATL,tGetRKInfo_tNK)(&rki, ATL_NTHREADS, TA, TB, M, N, K, 
                                  lda, ldb, ldc, alpha, beta);
   if (tnk.P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
//   printf("P=%d\n", tnk.P);
   tnk.rp = &rki;
   tnk.A = A;
   tnk.B = B;
   tnk.C = C;
   tnk.wsz = ATL_MulBySize(rki.szA + rki.szC) + ATL_Cachelen;
   tnk.wsz = ATL_MulByCachelen(ATL_DivByCachelen(tnk.wsz + ATL_Cachelen-1));
   vp = malloc(ATL_MulBySize(rki.szB) + tnk.P*tnk.wsz + 2*ATL_Cachelen);
   if (!vp)
      return(1);
   tnk.wsz = ATL_DivBySize(tnk.wsz)SHIFT;
   tnk.wB = ATL_AlignPtr(vp);
   tnk.w = tnk.wB + (rki.szB SHIFT);
   tnk.w = ATL_AlignPtr(tnk.w);
   nmblks = rki.nfmblks + rki.npmblks;
   tnk.MbCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks, tnk.P), nmblks, 0);
   tnk.BassgCtr = ATL_SetAtomicCount(1);
   #if ATL_CBC_STRONG
      tnk.BdoneCtr = ATL_SetAtomicCount(1);
   #endif
   ATL_goparallel(tnk.P, Mjoin(PATL,DoWork_tamm_tNK), &tnk, NULL);

   #if ATL_CBC_STRONG
   ATL_FreeAtomicCount(tnk.BdoneCtr);
   #endif
   ATL_FreeAtomicCount(tnk.BassgCtr);
   ATL_FreeGlobalAtomicCount(tnk.MbCtr);
   free(vp);
   return(0);
}
@ROUT ATL_tammm_tK
void Mjoin(PATL,DoWork_amm_tK)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tK_t *pd = pp->PD;
   rkinfo_t *op = pd->rp;
   const TYPE *B = pd->B, *A = pd->A;
   TYPE *pA = pd->wA, *pB = pd->w + vrank*pd->wsz, *pC, *C=pd->C;
   #ifdef TCPLX
      TYPE *rC;
   #else
      #define rC pC
   #endif
   ATL_CUINT nByBlks=pd->nByBlks, nByCols=pd->nByCols;
   const size_t nmblks=op->nfmblks+op->npmblks, nnblks=op->nfnblks+op->npnblks;
   int ictr, ict2;
/*
 * First, copy last nByCols B operands
 */
   pC = pB + ((op->szB)SHIFT);
   pC = ATL_AlignPtr(pC);
   #ifdef TCPLX
      rC = pC + op->szC;
   #endif
   if (nByBlks)
   {
      ATL_UINT szB=((op->szB)SHIFT);
      while ( (ictr = ATL_DecAtomicCount(pd->begBBlksCtr)) )
      {
          Mjoin(PATL,opblk)(op, 0, nnblks-ictr, NULL, B, NULL, NULL, NULL,
                            pd->wBb+(nByBlks-ictr)*szB, NULL, NULL, NULL);
          #if ATL_CBC_STRONG
             ATL_DecAtomicCount(pd->donBBlksCtr);
          #endif
      }
   }
/* 
 * Now, compute 1st colpan of C while copying global A; 
 * strongly-ordered cachces uses shared B, while weak duplicate B
 */
   ictr = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
   if (ictr)
   {
      int BCOPIED = 0;
      int i = nmblks - ictr;
      TYPE *b, *a, *an;
      #if ATL_CBC_STRONG
         b = pd->wBb + (nByBlks)*((op->szB)SHIFT);
         if (ATL_DecAtomicCount(pd->begBCtr))
         {
            Mjoin(PATL,opblk)(op, 0, 0, NULL, B, NULL, NULL, NULL, b, NULL,
                              NULL, NULL);
            BCOPIED = ATL_DecAtomicCount(pd->donBCtr);
            ATL_assert(BCOPIED);
         }
/*
 *       Copy A, then await B done ACK
 */
         a = IdxAw_rkK(op, pA, i);
         Mjoin(PATL,opblk)(op, i, 0, A, NULL, NULL, a, NULL, NULL, NULL, 
                           NULL, NULL);
         ATL_DecGlobalAtomicCount(pd->donACtr, vrank);
         if (!BCOPIED)
         {
            while (ATL_GetAtomicCount(pd->donBCtr))  /* await B cpy finish */
               ATL_thread_yield();
         }
         ict2 = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
         if (ict2)
            an = IdxAw_rkK(op, pA, nmblks-ict2);
         else
            an = pB;
/*
 *       Now multiply local(weak)/glob B * local A, and write to my piece of C
 */
         Mjoin(PATL,opblk)(op, i, 0, NULL, NULL, C, a, an, b, b, rC, pC);
      #else
         b = pd->w + vrank*pd->wsz;
/*
 *       All threads make local copy of B to avoid costly sync
 */
         Mjoin(PATL,opblk)(op, 0, 0, NULL, B, NULL, NULL, NULL, b, NULL, 
                           NULL, NULL);
/*
 *       Copy chosen block to correct place in global A workspace & do multiply
 */
         a = IdxAw_rkK(op, pA, i);
         ict2 = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
         if (ict2)
            an = IdxAw_rkK(op, pA, nmblks-ict2);
         else
            an = pB;
         Mjoin(PATL,opblk)(op, i, 0, A, NULL, C, a, an, b, b, rC, pC);
      #endif
/*
 *    Now loop over any remaining A blks with known-good b
 */
      while (ict2)
      {
         ictr = ict2;
         i = nmblks - ictr;
         a = an;
         #if ATL_CBC_STRONG
            Mjoin(PATL,opblk)(op, i, 0, A, NULL, NULL, a, NULL, NULL, NULL, 
                              NULL, NULL);
            ATL_DecGlobalAtomicCount(pd->donACtr, vrank);
            ict2 = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
            if (an)
               an = IdxAw_rkK(op, pA, nmblks-ict2);
            else
               an = pB;
            Mjoin(PATL,opblk)(op, i, 0, NULL, NULL, C, a, an, b, b, rC, pC);
         #else
            ict2 = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
            if (an)
               an = IdxAw_rkK(op, pA, nmblks-ict2);
            else
               an = pB;
            Mjoin(PATL,opblk)(op, i, 0, A, NULL, C, a, an, b, b, rC, pC);
         #endif
      }
   }
/*
 * We now have global A copied for everyone's use.  For weakly-ordered caches,
 * we sync all thr to to make sure we can all see each others' copies;
 * Strongly-ordered caches need to hang-fire until A copy is complete.
 */
   #if ATL_CBC_STRONG
      while (ATL_GetGlobalAtomicCount(pd->donACtr, vrank))
         ATL_thread_yield();      /* await A cpy finish */
   #else
      ATL_cbc_barrier(pp->nworkers, vrank, NULL);  /* barrier & memory fence */
   #endif
/*
 * Now loop over colpans from [1,nByCols+1], wt thread doing entire col,
 * and global A copy known to be ready to use.
 */
   if (nByCols)
   {
      ATL_CUINT nfnblks = op->nfnblks;
      while ( (ictr = ATL_DecGlobalAtomicCount(pd->ColCtr, vrank)) )
      {
         int i, j = nByCols - ictr + 1;
         TYPE *an;
/*
 *       For top block, use copied A, and copy this colblk of B
 */
         an = IdxAw_rkK(op, pA, 1);
         Mjoin(PATL,opblk)(op, 0, j, NULL, B, C, pA, an, pB, pB, rC, pC);
/*
 *       Remaining blocks don't need to copy A or B
 */
         for (i=1; i < nmblks; i++)
         {
            TYPE *a=an;
            an = IdxAw_rkK(op, pA, i+1);
            Mjoin(PATL,opblk)(op, i, j, NULL, NULL, C, a, an, pB, pB, rC, pC);
         }
      }
   }
/*
 * Finally, load balance end of computation by using block-level scheduling
 * for last nByBlks cols
 *
 * Currently, I have this designed to take any block in the last nByBlks cols.
 * However, this would result in a given thread moving pB unnecessarily.
 * A better idea is probably to nByBlks-len array of nmblks ctrs, and guys
 * can start at col+rank to spread around usage.  This will allow us to reuse
 * a given pB maximally.
 */
   if (nByBlks)
   {
      #if ATL_CBC_STRONG
         while (ATL_GetAtomicCount(pd->donBBlksCtr))  /* await B cpy finish */
            ATL_thread_yield();
      #endif
      while (1)
      {
         ATL_UINT max, j, jmax=0;
         TYPE *wb;
/*
 *       Find column with maximum remaining blocks, and work on that one
 */
         max = ATL_GetGlobalAtomicCount(pd->BlkCtrs[0], vrank);
         for (j=1; j < nByBlks; j++)
         {
            int k;
            k = ATL_GetGlobalAtomicCount(pd->BlkCtrs[j], vrank);
            if (k >= max)
            {
               max = k;
               jmax = j;
            }
         }
         if (!max)   /* if no blocks are left in any of the nByBlks colpans */
            break;   /* we are done */
/*
 *       For chosen colpan, work on individual blks of C with other threads.
 *       This is the last nByBlks columns, so add 1st and nByCols to get glob j
 *       Both the shared B and the global A have been copied at start of alg.
 */
         j = jmax + 1 + nByCols;
         wb = pd->wBb + jmax*((op->szB)SHIFT);
         ictr = ATL_DecGlobalAtomicCount(pd->BlkCtrs[jmax], vrank);
         if (ictr)
         {
            TYPE *a, *an;
            int i = nmblks - ictr;
            ict2 = ATL_DecGlobalAtomicCount(pd->BlkCtrs[jmax], vrank);
            a  = IdxAw_rkK(op, pA, i);
            an = IdxAw_rkK(op, pA, nmblks-ict2);
            Mjoin(PATL,opblk)(op, i, j, NULL, NULL, C, a, an, wb, wb, rC, pC);
            while (ict2)
            {
               i = nmblks-ict2;
               a = an;
               ict2 = ATL_DecGlobalAtomicCount(pd->BlkCtrs[jmax], vrank);
               an = IdxAw_rkK(op, pA, nmblks-ict2);
               Mjoin(PATL,opblk)(op, i, j, NULL, NULL, C, a, an, wb, wb, rC,pC);
            }
         }
@beginskip
         while ( (ictr = ATL_DecGlobalAtomicCount(pd->BlkCtrs[jmax], vrank)) )
         {
            int i = nmblks - ictr;
            TYPE *a, *an;
            a = an = IdxAw_rkK(op, pA, i);
            Mjoin(PATL,opblk)(op, i, j, NULL, NULL, C, a, an, wb, wb, rC, pC);
         }
@endskip
      }
   }
}
#ifndef TCPLX
   #undef rC
#endif

/*
 * This routine handles the case where K <= maxKB, and N is reasonably large
 * so that the it can spend most of its time giving out entire colpans of C,
 * and only go to block-level syncs for a few colpans at end.  It requires
 * enouch workspace to allocate common A (whole matrix), and P*(szB+szC).
 * It is particularly important for the statically blocked right-looking LU
 * or QR factorizations.
 */
int Mjoin(PATL,tammm_tK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   rkinfo_t rki;
   ATL_tamm_tK_t tnk;
   size_t nmblks, nnblks, szAp, szBb, sz;
   void *vp=NULL;

   tnk.P = Mjoin(PATL,tGetRKInfo_tK)(&rki, ATL_NTHREADS, TA, TB, M, N, K, 
                                     lda, ldb, ldc, alpha, beta);
@skip   tnk.P = 1;
   if (tnk.P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
//   printf("P=%d\n", tnk.P);
   tnk.rp = &rki;
   tnk.A = A;
   tnk.B = B;
   tnk.C = C;
/*
 * Compute how many columns to handle wt block-level and colpan-lvl sheduling
 */
   nmblks = rki.nfmblks + rki.npmblks;
   nnblks = rki.nfnblks + rki.npnblks;
   #if 1
   tnk.nByBlks = tnk.P - 1;
   tnk.nByBlks = Mmin(tnk.nByBlks, 32); /* allowing huge causes too many Ctrs */
   tnk.nByBlks = Mmin(tnk.nByBlks, nnblks-1);
   #elif 0
      tnk.nByBlks = 0;         // just for testing, bad load balance
   #else
      tnk.nByBlks = nnblks-1;  // just for testing, bad parallel overhead
   #endif
   tnk.nByCols = nnblks - tnk.nByBlks - 1;
/*
 * Get wrkspc
 */
   szAp = rki.szA*rki.nfmblks + rki.pszA*rki.npmblks;
   #if ATL_CBC_STRONG
      szBb = rki.szB*(1+tnk.nByBlks);
   #else
      szBb = rki.szB*tnk.nByBlks;
   #endif
   tnk.wsz = ATL_MulBySize(rki.szB + rki.szC) + ATL_Cachelen;
   tnk.wsz = ATL_MulByCachelen(ATL_DivByCachelen(tnk.wsz + ATL_Cachelen-1));
   sz = ATL_MulBySize(szAp+szBb) + tnk.P*tnk.wsz + 3*ATL_Cachelen;
   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)
      return(1);
   tnk.wsz = ATL_DivBySize(tnk.wsz)SHIFT;
   tnk.wA = ATL_AlignPtr(vp);
   tnk.wBb = tnk.wA + (szAp SHIFT);
   tnk.wBb = ATL_AlignPtr(tnk.wBb);
   tnk.w = tnk.wBb + (szBb SHIFT);
   tnk.w = ATL_AlignPtr(tnk.w);
   if (tnk.nByCols)
      tnk.ColCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(tnk.nByCols, tnk.P), 
                                            tnk.nByCols, 0);
   else
      tnk.ColCtr = NULL;
   tnk.begACtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,tnk.P), nmblks, 0);
   if (tnk.nByBlks)
      tnk.begBBlksCtr = ATL_SetAtomicCount(tnk.nByBlks);
   else
      tnk.begBBlksCtr = NULL;
   #if ATL_CBC_STRONG
      tnk.donACtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,tnk.P), 
                                             nmblks, 0);
      if (tnk.nByBlks)
         tnk.donBBlksCtr = ATL_SetAtomicCount(tnk.nByBlks);
      else
         tnk.donBBlksCtr = NULL;
      tnk.begBCtr = ATL_SetAtomicCount(1);
      tnk.donBCtr = ATL_SetAtomicCount(1);
   #endif
   if (tnk.nByBlks)
   {
      int nat, i;
      tnk.BlkCtrs = malloc(tnk.nByBlks * sizeof(void*));
      ATL_assert(tnk.BlkCtrs);
      nat = ATL_EstNctr(nmblks,tnk.P);
      for (i=0; i < tnk.nByBlks; i++)
         tnk.BlkCtrs[i] = ATL_SetGlobalAtomicCount(nat, nmblks, 0);
   }
   else
      tnk.BlkCtrs = NULL;
   ATL_goParallel(tnk.P, Mjoin(PATL,DoWork_amm_tK), NULL, &tnk, NULL);

   if (tnk.nByBlks)
   {
      int i;
      for (i=0; i < tnk.nByBlks; i++)
         ATL_FreeGlobalAtomicCount(tnk.BlkCtrs[i]);
      free(tnk.BlkCtrs);
   }
   ATL_FreeGlobalAtomicCount(tnk.begACtr);
   if (tnk.nByBlks)
      ATL_FreeAtomicCount(tnk.begBBlksCtr);
   #if ATL_CBC_STRONG
      ATL_FreeAtomicCount(tnk.begBCtr);
      ATL_FreeAtomicCount(tnk.donBCtr);
      ATL_FreeGlobalAtomicCount(tnk.donACtr);
      if (tnk.nByBlks)
         ATL_FreeAtomicCount(tnk.donBBlksCtr);
   #endif
   if (tnk.nByCols)
      ATL_FreeGlobalAtomicCount(tnk.ColCtr);
   free(vp);
   return(0);
}
@ROUT ATL_tammm_G
void Mjoin(PATL,ipCompBlk)
   (ipinfo_t *ip, size_t i, size_t j, size_t k, int be,
    TYPE *wA, TYPE *wB, TYPE *wC, TYPE *nA, TYPE *nB, TYPE *nC)
{
   #ifdef TCPLX
      TYPE *iC=wC, *rC=wC+ip->szC, *iA=wA, *iB=wB, *rA, *rB;
      ammkern_t amm_b1, amm_bn, amm0_b1, amm0_bn;
   #else
      ammkern_t amm;
   #endif
   ATL_CUINT kb = (k != ip->nfkblks) ? ip->kb : ip->KB0;
   const size_t nfmblks = ip->nfmblks, npmblks = ip->npmblks;
   const size_t nfnblks = ip->nfnblks, npnblks = ip->npnblks;
   unsigned int nmu, nnu;

   if (i+1 == nfmblks + npmblks)
      nmu = ip->nmuF;
   else
      nmu = (i < nfmblks) ? ip->nmu : ip->pnmu;
   if (j+1 == nfnblks + npnblks)
      nnu = ip->nnuF;
   else
      nnu = (j < nfnblks) ? ip->nnu : ip->pnnu;

   #ifdef TCPLX
      rA = iA + ((i < nfmblks) ? ip->szA : ip->pszA);
      rB = iB + ((j < nfnblks) ? ip->szB : ip->pszB);
      if (kb == ip->kb)  /* can use normal amm */
      {
         if (be)
         {
            amm0_b1 = amm_b1 = ip->amm_b1;
            amm0_bn = amm_bn = ip->amm_bn;
         }
         else
         {
            amm0_b1 = amm0_bn = ip->amm_b0;
            amm_b1 = ip->amm_b1;
            amm_bn = ip->amm_bn;
         }
      }
      else /* need K1 versions of everything */
      {
         if (be)
         {
            amm0_b1 = amm_b1 = ip->ammK1_b1;
            amm0_bn = amm_bn = ip->ammK1_bn;
         }
         else
         {
            amm0_b1 = amm0_bn = ip->ammK1_b0;
            amm_b1 = ip->ammK1_b1;
            amm_bn = ip->ammK1_bn;
         }
      }
      amm0_bn(nmu, nnu, kb, iA, iB, rC, rA, iB, iC);
      amm0_b1(nmu, nnu, kb, rA, iB, iC, rA, rB, rC);
      amm_bn(nmu, nnu, kb, rA, rB, rC, iA, rB, iC);
      amm_b1(nmu, nnu, kb, iA, rB, iC, nA, nB, rC);
   #else
      if (kb == ip->kb)  /* can use normal amm */
         amm = (be) ? ip->amm_b1 : ip->amm_b0;
      else               /* must use ammK1 for K-cleanup */
         amm = (be) ? ip->ammK1_b1 : ip->ammK1_b0;
      amm(nmu, nnu, kb, wA, wB, wC, nA, nB, nC);
   #endif
}
void Mjoin(PATL,ipwriteC)(ipinfo_t *ip, size_t i, size_t j, const TYPE *beta, 
                          TYPE *wC, TYPE *C)
{
   #ifdef TCPLX
      TYPE *rC = wC + ip->szC;
   #endif
   const size_t nfmblks=ip->nfmblks, nfnblks=ip->nfnblks;
   int mb, nb;

   C = IdxC_ip(ip, C, i, j);
   if (i+1 != nfmblks + ip->npmblks)
      mb = (i < nfmblks) ? ip->mb : ip->pmb;
   else
      mb = ip->mF;
   if (j+1 != nfnblks + ip->npnblks)
      nb = (j < nfnblks) ? ip->nb : ip->pnb;
   else
      nb = ip->nF;
   #ifdef TCPLX
      if (!beta)
         ip->blk2c_b1(mb, nb, ip->alpC, rC, wC, ip->ONE, C, ip->ldc);
      else
         ip->blk2c(mb, nb, ip->alpC, rC, wC, beta, C, ip->ldc);
   #else
      if (!beta)
         ip->blk2c_b1(mb, nb, ip->alpC, wC, ATL_rone, C, ip->ldc);
      else
         ip->blk2c(mb, nb, ip->alpC, wC, *beta, C, ip->ldc);
   #endif
}
/*
 * This routine computes one block C += A * B step
 */
static void DoBlkWtCpy
(
   ATL_tamm_gOOO_t *pd,              /* problem definition */
   ATL_CUINT rank,                   /* my (virtual) rank */
   size_t dblk,                      /* CpyBlk of C to compute */
   size_t k,                         /* which K-dim blk to do */
   int ibeta,                        /* 0: assume beta=0, else beta=1 */
   TYPE *wC                          /* private C workspace */
)
{
   ipinfo_t *ip=pd->ip;
   const size_t nmblks=pd->nmblks, nnblks=pd->nnblks, nkblks=ip->nfkblks+1; 
   size_t i=dblk, j=dblk;
   TYPE *wA = pd->wA, *wB = pd->wB;

   if (dblk < nmblks)  /* need to copy A blk */
      wA = Mjoin(PATL,ipcopyA)(ip, pd->A, dblk, k, wA);
   else
   {
      i = nmblks-1;
      wA = IdxAw_ip(pd->ip, wA, i, k);
   }
   if (dblk < nnblks)  /* need to copy B blk */
      wB = Mjoin(PATL,ipcopyB)(ip, pd->B, k, dblk, wB);
   else
   {
      j = nnblks-1;
      wB = IdxBw_ip(pd->ip, wB, k, j);
   }
/*
 * If I just finished copying dblk K-panel for A & B, let folks know
 */
   if (ATL_DecGlobalAtomicCountDown((pd->KdonCtr)[dblk], rank) == 1)
   {
      ATL_mutex_lock(pd->cpmut);
      if (dblk < nmblks)
         ATL_SetBitBV(pd->cpyAdBV, dblk);
      if (!pd->cpyAdone)
         if (ATL_FindFirstUnsetBitBV(pd->cpyAdBV, 0) == -1)
            pd->cpyAdone = 1;

      if (dblk < nnblks)
         ATL_SetBitBV(pd->cpyBdBV, dblk);
      if (!pd->cpyBdone)
         if (ATL_FindFirstUnsetBitBV(pd->cpyBdBV, 0) == -1)
            pd->cpyBdone = 1;
      ATL_mutex_unlock(pd->cpmut);
   }
   if (dblk < Mmin(nmblks, nnblks))
      Mjoin(PATL,ipCompBlk)(ip, i, j, k, ibeta, wA, wB, wC, wA, wB, wC);
}
static void CompDiagC                /* old DoBlksWtCpy */
(
   ATL_tamm_gOOO_t *pd,              /* problem definition */
   unsigned const int rank,          /* my (virtual) rank */
   size_t dblk,                      /* CpyBlk of C to compute */
   size_t kctr,                      /* non-zero KbegCtr */
   TYPE *wC                          /* my private C workspace */
)
/*
 * This routine is called only at the beginning of the algorithm, and its
 * main purpose is to perform the data copy of both A & B.  In the process,
 * we compute the diagonal blocks of C, so that the thread doing the block
 * gets to use the data it copies at least once, and so that we cut bus
 * traffic at least a little bit by doing computation.  Large-scale systems
 * are likely to slow down to speed of memory during this step regardless.
 */
{
   const size_t nkblks = pd->ip->nfkblks + 1;
   const size_t minblks = Mmin(pd->nmblks, pd->nnblks);

   DoBlkWtCpy(pd, rank, dblk, nkblks-kctr, 0, wC);
   while (kctr = ATL_DecGlobalAtomicCount((pd->KbegCtr)[dblk], rank))
   {
      DoBlkWtCpy(pd, rank, dblk, nkblks-kctr, 1, wC);
   }
/*
 * If we computed a block of C (rather than copying A or B past C's diagonals),
 * seize mutex for block of original C, and add my part to it
 */
   if (dblk < minblks)
   {
      int COPIED=0;
      ATL_mutex_lock(pd->Cmuts[dblk]);
      if (!ATL_IsBitSetBV(pd->cbetaBV, dblk))
      {
         ATL_mutex_lock(pd->cbetamut);
         if (!ATL_IsBitSetBV(pd->cbetaBV, dblk))
         {
            ATL_SetBitBV(pd->cbetaBV, dblk);
            ATL_mutex_unlock(pd->cbetamut);
            Mjoin(PATL,ipwriteC)(pd->ip, dblk, dblk, SADD(pd->beta), wC, pd->C);
            COPIED=1;
         }
         else
            ATL_mutex_unlock(pd->cbetamut);
      }
      if (!COPIED)
         Mjoin(PATL,ipwriteC)(pd->ip, dblk, dblk, NULL, wC, pd->C);
      ATL_mutex_unlock(pd->Cmuts[dblk]);
   }
}

static void DoCompWithCopy(ATL_tamm_gOOO_t *pd, int rank, TYPE *wC)
/*
 * This routine loops over the diagonal blocks of C in order to copy A & B.
 * All threads first try to find a new diag blk to work on alone.  If no
 * diagonal blocks are left, then they find a diagonal block with work still
 * to be done and steal work from the current diagonal block owner.
 */
{
   int NEWBLK=1;
   const size_t ND = pd->nMNblks;  /* max K-panels in A or B for copying */
/*
 * Keep going as long as there is copy work to be done
 */
   while (!pd->NOCPWORK)
   {
       size_t d=0, k;
/*
 *     Find which diagonal block to work on, and then which k blk to use
 */
       if (NEWBLK)
       {
          d = ATL_DecGlobalAtomicCount(pd->ccCtr, rank);
          if (d)
          {
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[ND-d], rank);
             if (!k)     /* if no more K work to do */
                d = 0;   /* can't work on this diag after all */
          }
       }
/*
 *     If all diagonal blocks currently being worked on by threads, find
 *     one that I can help with.
 */
       if (!d)
       {
          size_t i;
          NEWBLK = 0;
          for (i=0; i < ND; i++)
          {
             size_t j = (i+rank)%ND;
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[j], rank);
             d = ND-j;
             if (k)
                goto FOUNDDK;
          }
          pd->NOCPWORK = 1;   /* no work left to assign */
          return;             /* so done with copy of A&B and this func */
       }
/*
 *     If I reach here, I've got a valid d & k;  and I'll call a routine
 *     that continues to grab blocks from this diag along K until all K
 *     is done; it will then write the answer back to the original C, and
 *     return to this loop to see if it can help with another diag.
 */
       FOUNDDK:
          CompDiagC(pd, rank, ND-d, k, wC);
   }
}

static long FindCblk(const int rank, ATL_tamm_gOOO_t *pd)
/*
 * RETURNS: undone C block that has had its A and B k-panels copied or:
 *          -1: no work left to do;     -2: no work available
 * NOTE: this routine only called when A/B copying is still ongoing!
 */
{
   unsigned const int ncblks=pd->nCblks, nmblks=pd->nmblks, nnblks=pd->nnblks;
   int bb=0, ib;
   do
   {
      int i, j;
      ib = ATL_FindFirstUnsetBitBV(pd->cCblkBV, bb);
      if (ib == -1)
         return(-1);
      j = ib / nmblks;
      i = ib - j*nmblks;
      if (pd->cpyAdone || ATL_IsBitSetBV(pd->cpyAdBV, i))
      {
         if (pd->cpyBdone || ATL_IsBitSetBV(pd->cpyBdBV, j))
            return(ib);
      }
      bb = ib+1;
   }
   while(bb < ncblks);
   return(-2);
}
/*
 * Returns a number between [0,pd->ncK-1].  This is the Kctr to use for
 * work on this C, and it is the distance-1 from the last diagonal block in C.
 * Computes the (i,j) block coordinate of C block to use.
 * RETURNS: which KlastCtr to use.
 */
static INLINE int GetResCblk
   (ATL_CUINT rank, ATL_tamm_gOOO_t *pd, size_t *I, size_t *J)
{
   ipinfo_t *ip=pd->ip;
   const unsigned int nres = pd->ncK;  /* # of reserved C blks */
   const size_t nmblks=ip->nfmblks+ip->npmblks, nnblks=ip->nfnblks+ip->npnblks;
   unsigned int ctr, k;
/*
 * If copy still ongoing, assign C work based on dep info in cpy[A,B]dBV
 */
   while (!(pd->cpyAdone | pd->cpyBdone))
   {
      size_t i, j;
      for (k=0; k < nres; k++)
      {
         if (!ATL_GetGlobalAtomicCount(pd->KlastCtr[k], rank))
            continue;
         if (nmblks > nnblks) /* last row (non-diag) is reserved */
         {
            i = nmblks-1;
            j = k;
         }
         else               /* nnblks >= nmblks */
         {                  /* last col (non-diag) is reserved */
            i = k;
            j = nnblks-1;
         }
         if (!pd->cpyAdone && !ATL_IsBitSetBV(pd->cpyAdBV, i))
            continue;
         if (!pd->cpyBdone && !ATL_IsBitSetBV(pd->cpyBdBV, j))
            continue;
/*
 *       For weakly ordered caches, seize mutex to force sync
 */
         #if ATL_CBC_STRONG == 0
            ATL_mutex_lock(pd->cpmut);
            ATL_mutex_unlock(pd->cpmut);
         #endif
         *I = i;
         *J = j;
         return(k);
      }
   }
/*
 * If we reach here, done copying, so select first reserved C blk wt work
 */
   for (k=0; k < nres; k++)
   {
      ctr = (k + rank)%nres;
      if (ATL_GetGlobalAtomicCount(pd->KlastCtr[ctr], rank))
      {
         if (nmblks > nnblks) /* last row (non-diag) is reserved */
         {
            *I = nmblks-1;
            *J = ctr;
         }
         else               /* nnblks >= nmblks */
         {                  /* last col (non-diag) is reserved */
            *I = ctr;
            *J = nnblks-1;
         }
         return(ctr);
      }
   }
   return(-1);  /* no work left! */
}
/*
 * RETURNS: C block that has had its A and B k-panels copied
 */
static INLINE long GetCblk(ATL_CUINT rank, ATL_tamm_gOOO_t *pd)
{
   const int ncblks = pd->nCblks;
   int ib;
/*
 * If we are done copying, then use the counter to get the C block to work on
 * NOTE: thread that sets cpy[A,B]done must have cpmut to avoid race cond!
 */
   if (pd->cpyAdone & pd->cpyBdone)
   {
/*
 *    On weakly-ordered caches, checking cpyA/cpyB insufficient to ensure
 *    copied data is visible, so lock a BS mutex to force a memory barrier
 *    (the producer thread will have locked a mutex to update the copy bitvec)
 *    Use diagonal C mutex, which should have nobody using it.
 */
      #if !ATL_CBC_STRONG
         static char FIRST=1;
      if (FIRST)
      {
         FIRST=0;
         ib = (rank < pd->nMNblks)  ? rank : 0; /* spread across dead mutexes */
         ATL_mutex_lock(pd->Cmuts[ib]);
         ATL_mutex_unlock(pd->Cmuts[ib]);
      }
      #endif
      do
      {
         ib = ATL_DecGlobalAtomicCount(pd->cCtr, rank);
         if (!ib)
            return(-1);
         ib = ncblks - ib;
      }
      while (ATL_IsBitSetBV(pd->cCblkBV, ib));
      return(ib);
   }
/*
 * If we reach here, we must assign C work based on dep info in cpy[A,B]dBV
 */
   else
   {
      while(1)
      {
         ib = FindCblk(rank, pd);
/*
 *       If we have a candidate block, grab the mutex and make sure one is
 *       still there
 */
         if (ib >= 0)
         {
            ATL_mutex_lock(pd->cpmut);
/*
 *          If copy finished while locking, call ourselves to get fast answer
 */
            if (pd->cpyAdone & pd->cpyBdone)
            {
               ATL_mutex_unlock(pd->cpmut);
               return(GetCblk(rank, pd));
            }
/*
 *          Otherwise, see if we've still got a candidate block
 */
            ib = FindCblk(rank, pd);
            if (ib >= 0)  /* we've got a block! */
            {
               ATL_SetBitBV(pd->cCblkBV, ib);  /* claim the block */
               ATL_mutex_unlock(pd->cpmut);
               return(ib);
            }
            ATL_mutex_unlock(pd->cpmut);
         }
         if (ib == -1)
            return(-1);
/*
 *       If no work available, pause before trying again
 */
         if (ib == -2)
            ATL_thread_yield();
      }
   }
}

static void DoCompNoCopy(ATL_tamm_gOOO_t *pd, ATL_UINT rank, TYPE *wC)
{
   ipinfo_t *ip=pd->ip;
   TYPE *C=pd->C, *wA=pd->wA, *wB=pd->wB;
   #ifdef TCPLX
      const TYPE *beta=pd->beta;
      TYPE *rC = wC + ip->szC;
   #else
      const TYPE beta=pd->beta;
      #define rC wC
   #endif
   size_t nmblks = pd->nmblks;
   size_t ic;
   const ablk2cmat_t blk2c=ip->blk2c;

   while ((ic = GetCblk(rank, pd)) != -1)
   {
      int i, j;
      j = ic / nmblks;
      i = ic - j*nmblks;
      Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, IdxC_ip(ip, C, i, j), 3, 
                           IdxAw_ip(ip, wA, i, 0), IdxBw_ip(ip, wB, 0, j),
                           rC, wC, beta, blk2c);
   }
}
#ifndef TCPLX
   #undef rC
#endif
void DoLastComp(ATL_tamm_gOOO_t *pd, ATL_UINT rank, TYPE *wC)
{
   ipinfo_t *ip=pd->ip;
   TYPE *wA=pd->wA, *wB=pd->wB, *C=pd->C;
   void **Cmuts = pd->Cmuts + pd->nMNblks;
   const size_t nkblks = ip->nfkblks+1, nmblks=ip->nfmblks+ip->npmblks;
   size_t i, j;
   int cctr;

/*
 * As long as I can find C blks using the KlastCtr
 */
   cctr = GetResCblk(rank, pd, &i, &j);
   while (cctr >= 0)
   {
      size_t k;
      int WORKED=0;
/*
 *    Keep working on same C blk until we run out of A/B blocks in K-panel
 */
      while ( (k = ATL_DecGlobalAtomicCount(pd->KlastCtr[cctr], rank)) )
      {
         TYPE *a, *b;
         k = nkblks - k;
         a = IdxAw_ip(ip, wA, i, k);
         b = IdxBw_ip(ip, wB, k, j);
         Mjoin(PATL,ipCompBlk)(ip, i, j, k, WORKED, a, b, wC, a, b, wC);
         WORKED = 1;
      }
      if (WORKED)  /* if I did computation, will need to write C out */
      {
         int COPIED=0;
         ATL_mutex_lock(Cmuts[cctr]);
         if (!ATL_IsBitSetBV(pd->lbetaBV, cctr))
         {
            ATL_mutex_lock(pd->lbetamut);
            if (!ATL_IsBitSetBV(pd->lbetaBV, cctr))
            {
               ATL_SetBitBV(pd->lbetaBV, cctr);
               ATL_mutex_unlock(pd->lbetamut);
               Mjoin(PATL,ipwriteC)(ip, i, j, SADD(pd->beta), wC, C);
               COPIED=1;
            }
            else
               ATL_mutex_unlock(pd->lbetamut);
         }
         if (!COPIED)
            Mjoin(PATL,ipwriteC)(pd->ip, i, j, NULL, wC, C);
         ATL_mutex_unlock(Cmuts[cctr]);
//printf("%d:wrote C(%d,%d), CPY=%d, ctr=%d\n", rank, i, j, COPIED, cctr);
      }
      cctr = GetResCblk(rank, pd, &i, &j);
   }
}

void Mjoin(PATL,DoWork_OOO)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_gOOO_t *pd = pp->PD;
   TYPE *myC = pd->wC + vrank*((pd->ip->szC)SHIFT);
   if (!pd->NOCPWORK)
      DoCompWithCopy(pd, vrank, myC);
   DoCompNoCopy(pd, vrank, myC);
   if (pd->ncK)
      DoLastComp(pd, vrank, myC);
}
/*
 * This algorithm can be used for any parallelizable case wt K>LASTKB.  Usually,
 * it deals out blocks of C, with K-dim divided only for initial copy of A & B
 * and some load balance at the end.  However, if C is small and K is large,
 * it can switch and do parallalize along K (with fairly high overhead).
 * It tries to copy all of A & B up front, so recursion may be needed to
 * use it for large matrices.
 * RETURNS: 0 if it did the operation, non-zero if it did not
 */

int Mjoin(PATL,tammm_G)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   ATL_tamm_gOOO_t pd;
   ipinfo_t ip;
   int P, i, j, k, idx, mb, pmb, nb, pnb;
   size_t nkcnt, nkblks, nKpar, nMNblks, sz, szA, szB;
   amminfo_t mminfo;
   void *vp;
   #ifdef ATL_PHI_SORTED
      const unsigned int nthr = ATL_TP_PTR ? ATL_TP_PTR->nthr : ATL_NTHREADS,
                         p4 = nthr>>2, p4_2 = nthr+nthr, p4_3 = p4_2+p4;
   #endif

   idx = Mjoin(PATL,tGetParCIndx)(&ip, ATL_NTHREADS, M, N, K);
   Mjoin(PATL,geFillInIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc,
                              alpha, beta, 
                              ip.nfmblks, ip.npmblks, ip.mb, ip.pmb, 
                              ip.nfnblks, ip.npnblks, ip.nb, ip.pnb);

//   if (ip.kb > K)
//      return(1);

   pd.ip = &ip;
   pd.nmblks = ip.nfmblks + ip.npmblks;
   pd.nnblks = ip.nfnblks + ip.npnblks;
   pd.nCblks = sz = ((size_t)pd.nmblks)*pd.nnblks;
   if (pd.nCblks != sz)
      return(1);
   nkblks = ip.nfkblks + 1;
   pd.ncK = 0;
   if (pd.nmblks >= pd.nnblks)
   {
      pd.nMNblks = nMNblks = pd.nmblks;
      nKpar = pd.nnblks;
   }
   else
   {
      pd.nMNblks = nMNblks = pd.nnblks;
      nKpar = pd.nmblks;
   }
   pd.beta = beta;
/*
 * If K is large, dealing out C blks alone for parallelism can lead to idle
 * time at end of algorithm, as a long K-loop is given out as the last task.
 * Therefore, K is decent size, reserve roughly P-1 C blks that can parallize
 * along the K dim, providing for reduced idle time at the cost of extra
 * parallel idle time.
 */
   P = nkcnt = pd.nCblks - nKpar;  /* # of C blks given out as private work */
   P += nkblks * nKpar;            /* # of C blks parallelized along K */
   P = Mmin(ATL_NTHREADS,P);
   if (nkcnt > 1 && ip.nfkblks > 8)
   {        /* do we want to split K dim of some of these C blks? */
      size_t imin;
/*
 *    Our algorithm can only give out the non-diagonal elements of last row/col
 *    so we must limit ncK (# of non-diag c parallelized along K) to last 
 *    row/col block count
 */
      if (pd.nmblks > pd.nnblks)
         imin = pd.nnblks;
      else
         imin = (pd.nmblks != pd.nnblks) ? pd.nmblks : pd.nmblks-1;
      if (pd.nCblks-nKpar < P)
         pd.ncK = imin;
      else
      {
         pd.ncK = (P > 2) ? P-2 : 1;
         pd.ncK = Mmin(pd.ncK,imin);
      }
      P += pd.ncK*nkblks;
      P = Mmin(ATL_NTHREADS,P);
   }
/*
 * Quick exit for problems too small to thread
 */
   if (P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
#if 0
printf("P=%d, ncK=%d, M=(%d,%d; %d,%d) N=(%d,%d; %d,%d)\n", 
       P, (int)pd.ncK, (int)ip.nfmblks, (int)ip.npmblks, ip.mb, ip.pmb, 
       (int)ip.nfnblks, (int)ip.npnblks, ip.nb, ip.pnb);
#endif
   szA = nkblks*(ip.nfmblks*ip.szA + ip.npmblks*ip.pszA);
   szB = nkblks*(ip.nfnblks*ip.szB + ip.npnblks*ip.pszB);
   sz = (nMNblks*3+2*pd.ncK)*sizeof(void*);/* K[beg,don,last]Ctr & Cmuts arrs */
   sz += ATL_MulBySize(ip.szC*P + szA + szB + (ip.mu<<1)*ip.nu);
   sz += 3*ATL_Cachelen;                  /* room for alignment */
   #ifdef ATL_PHI_SORTED
      pd.ncntxts = 1;
      pd.ncores = p4;
      if (P == p4_2)
         pd.ncntxts = 2;
      else if (P == p4_3)
         pd.ncntxts = 3;
      else if (P == nthr)
         pd.ncntxts = 4;
      else
         pd.ncores = P;
      if (pd.ncntxts > 1)
         sz += ATL_Cachelen*(pd.ncores+1);
   #endif
   if (sz > ATL_PTMAXMALLOC)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   #ifdef ATL_PHI_SORTED
   if (pd.ncntxts > 1)
   {
      pd.chkin = (volatile int*)ATL_AlignPtr(vp);
      pd.KbegCtr = (void*)(((char*)pd.chkin)+pd.ncores*ATL_Cachelen);
      for (i=0; i < pd.ncores; i++)
      {
         int *ip;
         char *cp = (char*)pd.chkin;

         cp += i*ATL_Cachelen;
         ip = (int*) cp;
         *ip = ip[1] = ip[2] = ip[3] = -2;
      }
   }
   else
   #endif
   pd.KbegCtr = vp;
   pd.cpyAdone = pd.cpyBdone = pd.NOCPWORK = 0;
   pd.ccCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nMNblks, P),nMNblks, 0);
   pd.cCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nCblks, P),pd.nCblks, 0);
   nkcnt = ATL_EstNctr(nkblks, P);
   pd.KdonCtr = pd.KbegCtr + nMNblks;
   pd.Cmuts = pd.KdonCtr + nMNblks;
   if (pd.ncK)
   {
      pd.KlastCtr = pd.Cmuts+nMNblks+pd.ncK;
      pd.wA = (TYPE*)(pd.KlastCtr+pd.ncK);
   }
   else
   {
      pd.KlastCtr = NULL;
      pd.wA = (TYPE*)(pd.Cmuts+nMNblks);
   }
   pd.wA = ATL_AlignPtr(pd.wA);
   pd.wB = pd.wA + (szA SHIFT);
   pd.wB = ATL_AlignPtr(pd.wB);
   pd.wC = pd.wB + (szB SHIFT);
   pd.wC = ATL_AlignPtr(pd.wC);
   pd.A = A;
   pd.B = B;
   pd.C = C;
   for (i=0; i < nMNblks; i++)
   {
      pd.KbegCtr[i] = ATL_SetGlobalAtomicCount(nkcnt, nkblks, 0);
      pd.KdonCtr[i] = ATL_SetGlobalAtomicCountDown(nkcnt, nkblks);
      pd.Cmuts[i] = ATL_mutex_init();
   }
   for (i=0; i < pd.ncK; i++)
   {
      pd.KlastCtr[i] = ATL_SetGlobalAtomicCount(nkcnt, nkblks, 0);
      pd.Cmuts[i+nMNblks] = ATL_mutex_init();
   }
   pd.cpmut = ATL_mutex_init();
   pd.cbetamut = ATL_mutex_init();
   if (pd.ncK)
      pd.lbetamut = ATL_mutex_init();
   else
      pd.lbetamut = NULL;
   pd.cpyAdBV = ATL_NewBV(pd.nmblks);
   pd.cpyBdBV = ATL_NewBV(pd.nnblks);
   pd.cCblkBV = ATL_NewBV(pd.nCblks);
   pd.cbetaBV = ATL_NewBV(nMNblks);
   if (pd.ncK)
      pd.lbetaBV = ATL_NewBV(pd.ncK);
   else
      pd.lbetaBV = NULL;
/*
 * Initialize cCblkBV so that all diagonal blocks are shown as already complete
 * This BV is used for assigning work to non-copy blocks.
 */
   k = Mmin(pd.nmblks, pd.nnblks);
   for (i=0; i < k; i++)
      ATL_SetBitBV(pd.cCblkBV, i*(pd.nmblks+1));
/*
 * Reserve blks for load balancing at end; take largest of last row/col-panel 
 */
   if (pd.nmblks > pd.nnblks) /* reserve last row panel of C */
   {
      size_t k = pd.nmblks-1;
      for (i=0; i < pd.ncK; i++, k += pd.nmblks)
         ATL_SetBitBV(pd.cCblkBV, k);
   }
   else                      /* nnblks >= nmblks */
   {                         /* reserve last col panel of C */
      size_t k = pd.nCblks-pd.nmblks;
      for (i=0; i < pd.ncK; i++)
         ATL_SetBitBV(pd.cCblkBV, k+i);
   }
   ATL_goParallel(P, Mjoin(PATL,DoWork_OOO), NULL, &pd, NULL);
/*
 * Free allocated structures and return;
 */
   ATL_FreeBV(pd.cpyAdBV);
   ATL_FreeBV(pd.cpyBdBV);
   ATL_FreeBV(pd.cCblkBV);
   ATL_FreeBV(pd.cbetaBV);
   ATL_mutex_free(pd.cpmut);
   ATL_mutex_free(pd.cbetamut);
   ATL_FreeGlobalAtomicCount(pd.cCtr);
   ATL_FreeGlobalAtomicCount(pd.ccCtr);
   for (i=0; i < nMNblks; i++)
   {
      ATL_FreeGlobalAtomicCount(pd.KbegCtr[i]);
      ATL_FreeGlobalAtomicCountDown(pd.KdonCtr[i]);
      ATL_mutex_free(pd.Cmuts[i]);
   }
   for (i=0; i < pd.ncK; i++)
   {
      ATL_FreeGlobalAtomicCount(pd.KlastCtr[i]);
      ATL_FreeGlobalAtomicCount(pd.Cmuts[i+nMNblks]);
   }
   free(vp);
   return(0);
}
@ROUT ATL_tammm_smNK
typedef struct ATL_tamm_smNK ATL_tamm_smNK_t;
struct ATL_tamm_smNK
{
   ammkern_t amm_k1;   /* kernel to use on kb0 */
   ammkern_t amm;      /* kernel to use remaining kb */
   cm2am_t a2blk;      /* block copy for A */
   cm2am_t b2blk;      /* block copy for B, applies alpha */
   ablk2cmat_t blk2c;   /* copy that applies beta  */
   const TYPE *A;       /* input A matrix */
   const TYPE *B;       /* input A matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* ATL_NTHREADS wsz-len thread-local workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *BcpCtr;        /* which blk of B should I copy? */
   void *BdnCtr;        /* while != 0, B is still being copied */
   void *MbCtr;         /* which M block can I work on? */
   size_t wsz;          /* size of local workspace */
   int bsz;             /* size of common workspace for B */
   int TA;              /* 0: noTrans; 1: Trans */
   int TB;              /* 0: noTrans; 1: Trans */
   int nkblks;          /* (K-1)/kb */
   int kb;
   int kb0;             /* K%ku;  if 0, kb */
   int KB0;             /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int nmblks;          /* CEIL(M/mb) */
   int mb;              /* block factor along M */
   int mbL;             /* M - (nmblks-1)*mb */
   int nmu;             /* mb / mu */
   int nmuL;            /* CEIL(mbL/mu) */
   int nb;              /* block factor along N */
   int nbL;             /* N - (nnblks-1)*nb */
   int nnu;             /* nb / nu */
   int nnuL;            /* CEIL(nbL/nu) */
   int lda;             /* leading dim of A */
   int ldb;             /* leading dim of B */
   int ldc;             /* leading dim of C */
};

/*
 * This routine is specialized for the case where both N & K are relatively
 * small, and M is at least large enough to provide full parallelism
 */
int Mjoin(PATL,tammm_smNK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   amminfo_t mmi;
   unsigned int i, mb, nb, kb, nmblks, nnblks, nkblks;
/*
 * Smallest operand should always be B; if it is not, tell caller we decline
 * to work on this problem shape
 */
   if(Mjoin(PATL,tGetAmmmInfo)(&mmi, ATL_NTHREADS, TA, TB, M, N, K, alpha, beta)
      != 1)
      return(10);
   mb = mmi.mb;
   nmblks = M / mb;
   mbL = M - nmblks*mb;
   if (mbL)
   {
      nmblks++;
      pd.nmuL = (mbL+mu-1) / mu;
      pd.mbL = mbL;
   }
   else
   {
      pd.nmuL = nmu;
      pd.mbL = mb;
   }
   if (nmb < ATL_NTHREADS+ATL_NTHREADS)
      return(11);

}
@ROUT ATL_tgemm_amm
/*
 * recurs on any standard GEMM interface routine, passed as a function
 * pointer in amm.  flg is a bitflag, meaning if set:
 * 0    1 : Do divide M
 * 1    2 : Do divide N
 * 2    4 : Do divide K
 * 3    8 : stop dividing M when it is < 4*MAXMB
 * 4   16 : stop dividing N when it is < 4*MAXNB
 * 5   32 : stop dividing K when it is < 3*MAXKB
 */
int Mjoin(PATL,ammm_REC)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc,
   ATL_UINT flg,
   int (*amm)(enum ATLAS_TRANS,enum ATLAS_TRANS, size_t, size_t, size_t,
              const SCALAR, const TYPE*, size_t,  const TYPE*, size_t, 
              const SCALAR, TYPE*, size_t)
)
{
   size_t Dk, Dm;
   if (amm(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc))
   {
      if ((flg&8) && M <= (ATL_geAMM_LASTMB<<2))
         flg &= ~1;
      if ((flg&16) && N <= (ATL_geAMM_LASTNB<<2))
         flg &= ~2;
      if ((flg&32) && K <= 3*ATL_geAMM_LASTKB)
         flg &= ~4;
/*
 *    Stopping criteria in case something is horribly wrong
 */
      if (K <= ATL_geAMM_LASTKB && M <= ATL_geAMM_LASTMB && 
          N <= ATL_geAMM_LASTNB || !(flg&7))
      {
         printf("ATLAS warning: out-of-workspace causes serial execution!\n");
         Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
         return(0);
      }
/*
 *    Divide K first: it cuts space from both A & B
 */
      if ((flg | 3) == flg)
         Dk = Mmax(M,N);
      else if ((flg&3) == 0)
         Dk = 0;
      else
         Dk = (flg&1) ? M : N;
      Dm = (flg&2) ? N : 0;
      if (K+K >= Dk && (flg&4))
      {
         const int KL = K>>1, KR = K-KL;
         #ifdef TCPLX
            TYPE ONE[2] = {ATL_rone, ATL_rzero};
         #else
            #define ONE ATL_rone
         #endif
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, M, N, KL, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, flg, amm));
         A += ((TA == AtlasNoTrans) ? KL*lda : KL)SHIFT;
         B += ((TB == AtlasNoTrans) ? KL : KL*ldb)SHIFT;
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, KR, alpha, A, lda, 
                                     B, ldb, ONE, C, ldc, flg, amm));
         #ifndef TCPLX
            #undef ONE
         #endif
      }
/*
 *    If M largest dim (twice K), cut it instead
 */
      else if (M >= Dm && (flg&1))
      {
         const int ML = M>>1, MR = M-ML;
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, ML, N, K, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, flg, amm));
         A += ((TA == AtlasNoTrans) ? ML : ML*lda)SHIFT;
         return(Mjoin(PATL,ammm_REC)(TA, TB, MR, N, K, alpha, A, lda, 
                                     B, ldb, beta, C+(ML SHIFT), ldc, flg,amm));
      }
/*
 *    Otherwise, cut N
 */
      else if (flg&2)
      {
         const int NL = N>>1, NR = N-NL;
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, M, NL, K, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, flg, amm));
         B += ((TB == AtlasNoTrans) ? NL*ldb : NL)SHIFT;
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, NR, K, alpha, A, lda, B, ldb, 
                                     beta, C+NL*(ldc SHIFT), ldc, flg, amm));
      }
      else  /* can't cut anymore, give up and use serial */
      {
         printf("ATLAS warning: out-of-workspace causes serial execution!\n");
         Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
         return(0);
      }
   }
   return(0);
}

int Mjoin(PATL,tammm)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CSZT M,
   ATL_CSZT N,
   ATL_CSZT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CSZT lda,
   const TYPE *B,
   ATL_CSZT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CSZT ldc
)
{
   size_t nblks;
/*
 * No threading for Level-2 operations, or support for K=1,2
 */
   if (K <= 2 || M < 2 || N < 2)
      return(1);  /* signal failure */
   if (M <= ATL_geAMM_LASTMB && N <= ATL_geAMM_LASTNB &&
       K <= ATL_geAMM_LASTKB)
      return(1);
/*
 * _tNK & _tMN require minimal wrkspc, so just return if they fail
 */
   if (N <= ATL_rkAMM_LASTNB && K <= ATL_rkAMM_LASTKB)
      return(Mjoin(PATL,tammm_tNK)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                   beta, C, ldc));
   if (N <= ATL_geAMM_LASTNB && M <= ATL_geAMM_LASTMB &&
       K > (ATL_geAMM_LASTKB<<2))
      return(Mjoin(PATL,tammm_tMN)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                   beta, C, ldc));
/*
 * Routines that copy one entire matrix may need to cut varying dims in order
 * to fit within workspace requirements.  Use ammm_REC for this.
 */
   if (K <= ATL_rkAMM_LASTKB)  /* outer product / rank-K with N > LASTNB */
   {
@skip      if (((double)M)*N*K < 
@skip          2.0*ATL_rkAMM_LASTMB*ATL_rkAMM_LASTNB*ATL_rkAMM_LASTKB)
@skip         return(1);
      return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                  beta, C, ldc, 1, Mjoin(PATL,tammm_tK)));
   }

   if (N < (ATL_geAMM_LASTNB<<2) && K < (ATL_geAMM_LASTKB<<2) &&  
       (M>>ATL_NTHRPOW2) > ATL_geAMM_LASTMB)
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                     beta, C, ldc, 6, Mjoin(PATL,tammm_sNK)));
   if (M < (ATL_geAMM_LASTMB<<2) && K < (ATL_geAMM_LASTKB<<2) &&  
       (N>>ATL_NTHRPOW2) > ATL_geAMM_LASTNB)
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                     beta, C, ldc, 5, Mjoin(PATL,tammm_sMK)));
   if (K >= ATL_geAMM_LASTKB && M >= 12 && N >= 12)
      return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                  beta, C, ldc, 63, Mjoin(PATL,tammm_G)));
   return(1);
}
@ROUT ATL_tsyrk_amm
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_tbitvec.h"
#include "atlas_gatmctr.h"
#include "atlas_tperf.h"
#include Mstr(Mjoin(ATLAS_PRE,amm_syrk.h))
#ifdef Conj_
   #define syrkBlk Mjoin(PATL,herkBlk)
   #define syrk_K Mjoin(PATL,herk_KLoop)
#else
   #define syrkBlk Mjoin(PATL,syrkBlk)
   #define syrk_K  Mjoin(PATL,syrk_KLoop)
#endif
#define DEBUG 0
#ifndef DEBUG
   #define DEBUG 0
#endif
void syrkBlk(ipinfo_t*,int,size_t,size_t,const TYPE *,cm2am_t,ablk2cmat_t,
             const SCALAR,TYPE*,TYPE*,TYPE*,TYPE*,TYPE*,TYPE*,TYPE*,TYPE*,
             TYPE*,TYPE*);
void syrk_K(ipinfo_t*, int flg, size_t d, const TYPE*A, cm2am_t,
            ablk2cmat_t, const SCALAR, TYPE *C, TYPE *rS, TYPE *wS, TYPE *wA,
            TYPE *wB, TYPE *rC, TYPE *wC, TYPE *wU);
/* 
 * Translates C coordinate in lower triangular matrix to Cblk #.
 * nm is the number of Mblocks, 
 * (i,j) is the coordinate of the block
 */
#define Mcoord2cblk(i_, j_, nm_) ((((nm_)+(nm_)-j-1)*j)>>1 - (j_) + (i_) - 1)
/*
 * Translates block number b_ to (i,j) coordinates assuming first panel
 * has nm_ blocks in it (including diagonal block)
 */
#define Mcblk2coord(NM_, B_, I_, J_) \
{ \
   register int n_ = (NM_)-1, b_=(B_), j_; \
   for (j_=0; b_ >= n_; j_++) \
   { \
      b_ -= n_; \
      n_--; \
   } \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}

static void DoBlkWtCpy
(
   unsigned const int rank,   /* my thread rank */
   ATL_tsyrk_ammN_t *pd,      /* problem def structure */
   unsigned const long dblk,  /* diagonal block of C to compute */
   unsigned long kblk,        /* kblk to compute */
   unsigned const int flg,    /* 0: amm_b0: else: amm_b1 */
   TYPE *rS,                  /* real wrk for SYRK's A (unused for real) */
   TYPE *iS,                  /* imag wrk for SYRK's A (real for real */
   TYPE *rC,                  /* real wrk for C (unused for real) */
   TYPE *iC                   /* imag wrk for C (real for real) */
)
{
   ipinfo_t *ip=pd->ip;
   const TYPE *A;             /* ptr to original matrix */
   TYPE *wA, *wB;             /* ptr to copy space */
   #ifdef TCPLX
      TYPE *rA, *rB;
      ammkern_t amm_bn, amm_b1, ammR, ammC;
   #else
      ammkern_t amm;          /* amm kern to use  */
   #endif
   void *KdonCtr = ATL_AddBytesPtr(pd->KdonCtr, dblk*pd->KDCgap);
   const size_t skp=pd->pansz0, ndiag=pd->ndiag, lda=ip->lda;
   size_t i, k;
   const int UPPER=flg&1;
   const int kb0=ip->kb;
   int kb = kb0, KB=kb0, b=ip->nb;
   int kk, nmu=ip->nmu, nnu=ip->nnu;
/*
 * K cleanup needs to use special kern
 */
   if (!kblk)
   {
      #ifdef TCPLX
         if (flg&4)
            ammR = ammC = ip->ammK1_b0;
         else
         {
            ammR = ip->ammK1_bn;
            ammC = ip->ammK1_b1;
         }
         amm_b1 = ip->ammK1_b1;
         amm_bn = ip->ammK1_bn;
      #else
         amm = (flg&4) ? ip->ammK1_b0 : ip->ammK1_b1;
      #endif
      KB = ip->KB0;
      kb = ip->kb0;
   }
   else
   {
      #ifdef TCPLX
         if (flg&4)
            ammR = ammC = ip->amm_b0;
         else
         {
            ammR = ip->amm_bn;
            ammC = ip->amm_b1;
         }
         amm_b1 = ip->amm_b1;
         amm_bn = ip->amm_bn;
      #else
         amm = (flg&4) ? ip->amm_b0 : ip->amm_b1;
      #endif
   }
   if (dblk == ndiag-1)
   {
      nmu = ip->nmuF;
      nnu = ip->nnuF;
      b = ip->nF;
   }
/*
 * If last K block of different size, take it from end of vector to avoid
 * screwing up alignment
 */
   if (kb0 != ip->kb0)
      kblk = (kblk) ? kblk-1 : ip->nfkblks;
// fprintf(stderr, "%d: dblk=%d, kblk=%d\n", rank, dblk, kblk);
   A = IdxA_ip(ip, pd->A, dblk, kblk);
   if (UPPER)
   {
      if (dblk != ndiag-1)
         wA = IdxAw_ip(ip, pd->wA, dblk, kblk);
      else
         wA = iS;
      if (dblk)
         wB = IdxBw_ip(ip, pd->wAt, kblk, dblk) - skp;
      else
         wB = iS;
   }
   else         /* Lower */
   {
      if (dblk != ndiag-1)
         wB = IdxBw_ip(ip, pd->wAt, kblk, dblk);
      else
         wB = iS;
      if (dblk)
          wA = IdxAw_ip(ip, pd->wA, dblk, kblk) - skp;
      else
         wA = iS;
   }
   #if DEBUG > 1
      fprintf(stderr, "%d: DIAMM(%ld,%ld), wA=%p, wB=%p\n", rank, dblk, kblk, 
              wA, wB);
   #endif

   #ifdef TCPLX
      if (dblk < ip->nfnblks)
      {
         rA = wA + ip->szA;
         rB = wB + ip->szB;
      }
      else
      {
         rA = wA + ip->pszA;
         rB = wB + ip->pszB;
      }
      ip->a2blk(kb, b, ip->alpA, A, lda, rA, wA);
      ip->b2blk(kb, b, ip->alpB, A, lda, rA, wB);
   #else
      ip->a2blk(kb, b, ip->alpA, A, lda, wA);
      ip->b2blk(kb, b, ip->alpB, A, lda, wB);
   #endif
/*
 * Signal copy of this K-panel of A/A' is complete iff I just copied the 
 * last block
 */
   if (ATL_gatmctr_dec(KdonCtr, rank) == 1)
      ATL_tSetBitBV(pd->cpydonBV, dblk);
   #ifdef TCPLX
      ammR(nmu, nnu, KB, wA, wB, rC, rA, wB, iC);
      ammC(nmu, nnu, KB, rA, wB, iC, rA, rB, iC);
      amm_bn(nmu, nnu, KB, rA, rB, rC, wA, rB, iC);
      amm_b1(nmu, nnu, KB, wA, rB, iC, wA, rB, iC);
   #else
      amm(nmu, nnu, KB, wA, wB, iC, wA, wB, iC);
   #endif
}

static void WriteAmmDiag
   (ATL_tsyrk_ammN_t *pd, const long d, const int B, const SCALAR beta, 
    const TYPE *W)
/*
 * Copies from BxB block-major W to upper/lower portion of pd->C
 */
{
   ipinfo_t *ip=pd->ip;
   TYPE *c;
   #ifdef TCPLX
      const SCALAR ONE=ip->ONE;
   #else
      #define ONE ATL_rone
   #endif
   c = IdxC_ip(ip, pd->C, d, d);
   if (pd->flg & 1)  /* Upper */
   {
      int j;
      const size_t ldc=(ip->ldc)SHIFT;

      for (j=0; j < B; j++, c += ldc, W += (B SHIFT))
      {
         Mjoin(PATL,axpby)(j+1, ONE, W, 1, beta, c, 1);
         #ifdef Conj_
            c[j+j+1] = ATL_rzero;
         #endif
      }
   }
   else              /* Lower */
   {
      const size_t ldp1=(ip->ldc+1)SHIFT;
      int j;

      for (j=0; j < B; j++, c += ldp1, W += (B+1)SHIFT)
      {
         Mjoin(PATL,axpby)(B-j, ONE, W, 1, beta, c, 1);
         #ifdef Conj_
            c[1] = ATL_rzero;
         #endif
      }
   }
}
#ifndef TCPLX
   #undef ONE
#endif

static void DoBlksWtCpy
(
   unsigned const int rank,   /* my thread rank */
   ATL_tsyrk_ammN_t *pd,      /* problem def structure */
   unsigned const long dblk,  /* diagonal block of C to compute */
   unsigned long kctr,        /* non-zero Kbeg ctr */
   TYPE *rS,                  /* real wrk for SYRK's A (unused for real) */
   TYPE *iS,                  /* imag wrk for SYRK's A (real for real */
   TYPE *rC,                  /* real wrk for C (unused for real) */
   TYPE *iC,                  /* imag wrk for C (real for real) */
   TYPE *U                    /* Upper reflection space */
)
{
   ipinfo_t *ip=pd->ip;
   ablk2cmat_t syblk2c=pd->syblk2c_b1;
   cm2am_t sya2blk = pd->sya2blk;
   const size_t ldc = ip->ldc;
   size_t skpA, skpB;
   #ifdef TCPLX
      const TYPE *beta=ip->ONE;
      #define ONE ip->ONE
      const TYPE ZERO[2] = {ATL_rzero, ATL_rzero};
   #else
      #define ONE ATL_rone
      #define ZERO ATL_rzero
      TYPE beta = ONE;
   #endif
   ATL_UINT szC=ip->szC;
   const unsigned int flg = pd->flg;
   unsigned int nb;
   const size_t nkblks=pd->nkblks, ndiag=pd->ndiag;
   long kblk = nkblks - kctr, b;
   void *KbegCtr = ATL_AddBytesPtr(pd->KbegCtr, dblk*pd->KBCgap);
   void *KdonCtr = ATL_AddBytesPtr(pd->KdonCtr, dblk*pd->KDCgap);
   void *cpydonBV = pd->cpydonBV;
   TYPE *wA0=pd->wA, *wB0=pd->wAt, *wA, *wB;
   const TYPE *A = pd->A;
   void *Cdmut;

   if (sya2blk)
   {
      if (flg & 1) /* Upper */
      {
         skpA = 0;
         skpB = pd->pansz0;
         if (dblk == ndiag-1)
            wA0 = NULL;
         if (!dblk)
            wB0 = NULL;
      }
      else         /* Lower */
      {
         skpA = pd->pansz0;
         skpB = 0;
         if (dblk == ndiag-1)
            wB0 = NULL;
         if (!dblk)
            wA0 = NULL;
      }
      wA = (wA0) ? (IdxAw_ip(ip, wA0, dblk, kblk) - skpA) : NULL;
      wB = (wB0) ? (IdxBw_ip(ip, wB0, kblk, dblk) - skpB) : NULL;
      syrkBlk(ip, flg|4, dblk, kblk, A, sya2blk, NULL, ZERO, NULL, 
              rS, iS, wA, wA, wB, wB, rC, iC, U);
      if (ATL_gatmctr_dec(KdonCtr, rank) == 1)
      {
         ATL_tSetBitBV(cpydonBV, dblk);
         #if DEBUG > 1
            ATL_tPrintBV(stderr, "00", cpydonBV);
         #endif
      }
   }
   else
      DoBlkWtCpy(rank, pd, dblk, kblk, flg|4, rS, iS, rC, iC);
   while ((kctr = ATL_gatmctr_dec(KbegCtr, rank)))
   {
      long kk;
      kblk = nkblks - kctr;

      if (sya2blk)
      {
         wA = (wA0) ? (IdxAw_ip(ip, wA0, dblk, kblk) - skpA) : NULL;
         wB = (wB0) ? (IdxBw_ip(ip, wB0, kblk, dblk) - skpB) : NULL;

         syrkBlk(ip, flg, dblk, kblk, A, sya2blk, NULL, ZERO, NULL, 
                 rS, iS, wA, wA, wB, wB, rC, iC, U);
         kk = ATL_gatmctr_dec(KdonCtr, rank);
         if (kk == 1)
            ATL_tSetBitBV(cpydonBV, dblk);
      }
      else
         DoBlkWtCpy(rank, pd, dblk, kblk, flg, rS, iS, rC, iC);
   }
/*
 * For upper or amm, write to U ptr before seizing mutex.  After getting mutex,
 * we will reflect/copy this matrix to Upper one
 */
   if (!sya2blk)  /* copy from access-major iC to block-major U */
   {
      nb = (dblk < ip->nfnblks) ? ip->nb : ip->pnb;
      #ifdef TCPLX
         pd->syblk2c(nb, nb, ip->alpC, rC, iC, ZERO, U, nb);
      #else
         pd->syblk2c(nb, nb, ip->alpC, iC, ATL_rzero, U, nb);
      #endif
   }
   else if (flg&1)  /* Upper must use workspace */
      syrkBlk(ip, flg, dblk, 0, NULL, NULL, pd->syblk2c, ZERO, NULL, 
              NULL, NULL, NULL, NULL, NULL, NULL, rC, iC, U);
/*
 * Now, seize mutex for diagonal block of original C, and copy back out
 * only above/below diagonal
 * NOTE: because I'm using bitvecs, have to use tSetBitBV, which contains
 *       another mutex lock.  If I kept betaBV in a char array instead,
 *       could reduce to only needing Cdmut here.
 */
@skip   c = IdxC_ip(ip, pd->C, dblk, dblk);
@skip fprintf(stderr, "%u: get lock diag %lu\n", rank, dblk);
   Cdmut = ATL_AddBytesPtr(pd->Cdmuts, dblk*pd->dCinc);
   ATL_lock(Cdmut);
   if (!ATL_SetBitBV(pd->dbetaBV, dblk))   /* I'm first, so tell rest of  */
   {                                       /* thrs don't apply beta */
      beta = pd->beta;                     /* because I'm going to do it */
      syblk2c = pd->syblk2c;
   }
   if (sya2blk)
   {
      if (flg&1)  /* upper just needs to reflect from U to C */
         syrkBlk(ip, flg, dblk, 0, NULL, NULL, NULL, beta, pd->C,
                 NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, U);
      else /* Lower writes directly to C from rC,iC */
         syrkBlk(ip, flg, dblk, 0, NULL, NULL, syblk2c, beta, pd->C,
                 NULL, NULL, NULL, NULL, NULL, NULL, rC, iC, NULL);
   }
   else
      WriteAmmDiag(pd, dblk, nb, beta, U);
   ATL_unlock(Cdmut);
@skip fprintf(stdout, "%u: unlock diag %lu\n", rank, dblk);
}
#undef ONE

/*
 * In this phase, we work only on nshar diagonal blocks, while copying both
 * A & A'.  For init diag work, we parallelize both N & K dims so that the copy
 * is done as quickly as possible.  Threads coming in first choose differing
 * diag blks; diagonal blocks are dealt out using the diCtr local
 * counter (which starts at nshar).
 * Once all shared diagonal blocks are dealt out, new threads will start using
 * the atomic ctr array KbegCtr array to share K work for each diagonal.
 * both KbegCtr & KdonCtr are nshar-len arrays of atomic counters.  Each
 * counter starts at nkblks.  Once the block pointed to by KbegCtr is
 * completely copied, the copying array increments the KdonCtr.  Only one
 * core per diag will get KdonCtr == 1 after doing his copy, and this
 * core will to set the appropriate bit in cpydonBV, which is a nnblks-length 
 * global threaded bit vector.  If the kth bit is set, that means the 
 * kth row of A & kth col of A' has been copied.
 * Once a thread gets KbegCtr for a particular diag of 0, it means there's
 * no more work for this block of C, and so it will seize the appropriate
 * Cdmuts mutex which protects each diagonal block of C, and write its
 * finished contribution out to C.  The first such thread to ever seize
 * the mutex will scope dbetaBV to find if this diagonal block needs beta
 * applied, while later threads will use beta=1.
 * Eventually, all init diagonal work is finished, and the first processor to
 * get 0 for all dCtr & KbegCtr requests will set NOINCPY=1, so later
 * threads don't have to query all the counters to know they should proceed
 * to working on non-init diagonals or non-diagonal block.
 */
static void DoSharDiag(const int P, const int rank, ATL_tsyrk_ammN_t *pd, 
                       TYPE *rS, TYPE *iS, TYPE *rC, TYPE *iC, TYPE *U)
{
   const size_t KBinc = pd->KBCgap;
   void *KbegCtr;
   const unsigned long nshar=pd->nshar;
   unsigned long jstart, k;
   int DIAG=1;

   if (nshar >= P)
      jstart = rank * (nshar/P);
   else
      jstart = rank - (rank/nshar)*nshar;

   #if DEBUG > 1
      fprintf(stderr, "%d: start DoSharDiag\n", rank);
   #endif
   while (!(pd->NOINICPY))
   {
      long d=0;
/*
 *    Find which diagonal block to work on, and then which k blk to use
 */
      if (DIAG)
      {
         d = ATL_atmctr_dec(pd->diCtr);
         if (d)
         {
            k = nshar - d;
            KbegCtr = ATL_AddBytesPtr(pd->KbegCtr, k*KBinc);
            k = ATL_gatmctr_dec(KbegCtr, rank);
            if (!k)     /* if no more K work to do */
               d = 0;   /* can't work on this diag after all */
         }
      }
/*
 *    If all diagonal blocks currently being worked on by threads, find
 *    one that I can help with.
 */
      if (!d)
      {
         unsigned long i;
         DIAG = 0;
         for (i=0; i < nshar; i++)
         {
            unsigned long j = i+jstart;
            j = (j < nshar) ? j : j-nshar;
            KbegCtr = ATL_AddBytesPtr(pd->KbegCtr, j*KBinc);
            k = ATL_gatmctr_dec(KbegCtr, rank);
            d = nshar-j;
            if (k)
              goto FOUNDDK;
         }
         pd->NOINICPY = 1;    /* no initial work left to assign */
         #if DEBUG > 1
            fprintf(stderr, "%d: DONE  DoSharDiag\n", rank);
         #endif
         return;             /* so done with copy of A/A' & this func */
      }
/*
 *    If I reach here, I've got a valid d & k;  and I'll call a routine
 *    that continues to grab blocks from this diag along K until all K
 *    is done; it will then write the answer back to the original C, and
 *    return to this loop to see if it can help with another diag.
 */
      FOUNDDK:
         DoBlksWtCpy(rank, pd, nshar-d, k, rS, iS, rC, iC, U);
   }
   #if DEBUG > 1
      fprintf(stderr, "%d: DONE  DoSharDiag\n", rank);
   #endif
}

static void Do1Diag(const int rank, ATL_tsyrk_ammN_t *pd, long d,
                    TYPE *rS, TYPE *iS, TYPE *rC, TYPE *iC, TYPE *U)
/*
 * This routine computes 1 diagonal block using SYRK
 */
{
   ipinfo_t *ip=pd->ip;
   TYPE *wA=pd->wA, *wB=pd->wAt;
   const size_t skp=pd->pansz0;
   const unsigned long ndiag=pd->ndiag;
   const unsigned int flg=pd->flg;
   #if DEBUG > 1
      fprintf(stderr, "%d: start Do1Diag %ld\n", rank, d);
   #endif
   if (flg&1)
   {
      if (d != ndiag-1)
         wA = IdxAw_ip(ip, wA, d, 0);
      else
         wA = NULL;
      if (d)
         wB = IdxBw_ip(ip, wB, 0, d) - skp;
      else
         wB = NULL;
   }
   else         /* Lower */
   {
      if (d != ndiag-1)
         wB = IdxBw_ip(ip, wB, 0, d);
      else
         wB = NULL;
      if (d)
          wA = IdxAw_ip(ip, wA, d, 0) - skp;
      else
         wA = NULL;
   }
   #if DEBUG > 1
      fprintf(stderr, "%d:C(%ld,%ld), wA=%p, wB=%p\n", rank, d, d, wA, wB);
   #endif
   if (pd->sya2blk)
      syrk_K(ip, flg, d, pd->A, pd->sya2blk, pd->syblk2c, pd->beta, pd->C, 
             rS, iS, wA, wB, rC, iC, U);
   else  /* using gemm instead of SYRK! */
   {
      const int B0=ip->nb, B = (d < ip->nfnblks) ? B0 : ip->nF;
      const SCALAR beta=pd->beta;
      const TYPE *lA;
      #ifdef TCPLX
         const TYPE ONE[2]={ATL_rone,ATL_rzero}, ZERO[2]={ATL_rzero,ATL_rzero};
      #else
         #define ONE ATL_rone
         #define ZERO ATL_rzero
      #endif
      TYPE *c;
      int MV=3;

      if (!wA)
      {
         wA = iS;
         MV = 2;
      }
      else if (!wB)
      {
         MV = 1;
         wB = iS;
      }
      lA = IdxA_ip(ip, pd->A, d, 0);
      Mjoin(PATL,iploopsK)(ip, d, d, lA, lA, NULL, MV, wA, wB, rC, iC, 
                           pd->beta, NULL);
/*
 *    Copy from access-major rC to block-major U, then update Upper/lower Cblk
 */
      #ifdef TCPLX
         pd->syblk2c(B, B, ip->alpC, rC, iC, ZERO, U, B);
      #else
         pd->syblk2c(B, B, ip->alpC, iC, ZERO, U, B);
      #endif
      c = IdxC_ip(ip, pd->C, d, d);
      if (pd->flg & 1)  /* Upper */
      {
         int j;
         const size_t ldc=(ip->ldc)SHIFT;

         for (j=0; j < B; j++, c += ldc, U += (B SHIFT)) 
         {
            Mjoin(PATL,axpby)(j+1, ONE, U, 1, beta, c, 1);
            #ifdef Conj_
               c[j+j+1] = ATL_rzero;
            #endif
         }
      }
      else              /* Lower */
      {
         const size_t ldp1=(ip->ldc+1)SHIFT;
         int j;

         for (j=0; j < B; j++, c += ldp1, U += (B+1)SHIFT)
         {
            Mjoin(PATL,axpby)(B-j, ONE, U, 1, beta, c, 1);
            #ifdef Conj_
               c[1] = ATL_rzero;
            #endif
         }
      }
   }
   ATL_tSetBitBV(pd->cpydonBV, d);
   if (!ATL_tInfoBV(pd->cpydonBV, ATL_TBV_NUNSET))
      pd->cpydone = 1;
   #if DEBUG > 1
      fprintf(stderr, "%d: DONE  Do1Diag %ld\n", rank, d);
   #endif
}
#ifndef TCPLX
   #undef ONE
   #undef ZERO
#endif

@beginskip
/*
 * This routine does non-initial diag blocks by accessing dCtr, and then
 * each core will copy the entire K-panel
 */
static int DoDiag(const int rank, ATL_tsyrk_ammN_t *pd, 
                  TYPE *rS, TYPE *iS, TYPE *rC, TYPE *iC, TYPE *U)
{
   const unsigned long nshar=pd->nshar, ndiag=pd->ndiag, nhere=ndiag-nshar;
   void *dCtr=pd->dCtr, *cpydonBV=pd->cpydonBV;
   unsigned long mycnt=0;
   #if DEBUG > 1
      fprintf(stderr, "%d: start DoDiag\n", rank);
   #endif
   while (!(pd->NODWORK))
   {
      long d=0;
/*
 *    Find which diagonal block to work on, and then which k blk to use
 */
      d = ATL_gatmctr_dec(pd->dCtr, rank);
      if (!d)
      {
         pd->NODWORK = 0;
         break;
      }
      d = nhere - d + nshar;
      Do1Diag(rank, pd, d, rS, iS, rC, iC, U);
      mycnt++;
   }
   #if DEBUG > 1
      fprintf(stderr, "%d: DONE  DoDiag\n", rank);
   #endif
   return(mycnt);
}

@endskip
static int compPossCols
(
   void *gcpanDonBV,
   ATL_BV_t *lcpyDonBV, /* set bits are ready to to be computed */
   ATL_BV_t *lColBV     /* OUTPUT: ready cols still to be computed */
)
/*
 * Computes all column coordinates that can legally be computed given
 * cpyDonBV ready input blks, subtracting out any already-finished colpans
 * indicated by gcpanDonBV
 * RETURNS: nunset in gcpanDonBV
 * 
 */
{
   long nunset, N, Ne, Nr, i;

   nunset = ATL_tGlb2locBV(lColBV, gcpanDonBV, 0);
   N = lColBV[0];
   Ne = N >> shBV;
/*
 * We can only compute those columns that are: (1) not already computed &
 * (2) have the Kpanel copied.
 */
   for (i=0; i < Ne; i++)
      lColBV[i+1] = (~(lColBV[i+1])) & lcpyDonBV[i+1];
   Nr = N & modmskBV;
   if (Nr)
      lColBV[i+1] = ((~(lColBV[i+1])) & lcpyDonBV[i+1]) & ((1L<<Nr)-1);

   return(nunset);
}

static int compPossRows
(
   const int UPPER,
   long ndiag,
   long J,              /* colpan to compute rows of */
   void *gcblkBV,
   ATL_BV_t *lcpyDonBV, /* set bits are ready to to be computed */
   ATL_BV_t *lrowBV     /* OUTPUT: rows in col J that can be computed */
)
/*
 * RETURNS: 0 if no rows possible, else 1.
 */
{
   long ret=0;
   if (UPPER)  /* lower blks don't exist, j'th blk diag */
   {
      ATL_BV_t N=(ndiag+bpiBV-1)>>shBV, k;
      if (!J)        /* no non-diag blks in 1st Upper colpan */
         return(0);
      k = J>>shBV;
      lrowBV[k+1] = allsetBV;
      if (!ATL_tGlb2locBV(lrowBV, gcblkBV, 0))  /* if no unset bits in cblkBV */
         return(0);                   /* col J has no computable blocks left! */
      for (k++; k < N; k++)
         lrowBV[k+1] = allsetBV;
   }
   else /* Lower, upper j blks don't exist, j'th blk diag */
   {
      ATL_BV_t *lp = lrowBV+1, N=(J+bpiBV)>>shBV, k;
      if (J == ndiag-1)  /* no non-diag blks in last colpan */
         return(0);
      for (k=0; k < N; k++)
         lp[k] = allsetBV;
      k = ATL_tGlb2locBV(lrowBV, gcblkBV, J+1);
      if (!k)            /* if no unset bits in cblkBV */
         return(0);      /* col J has no computable blocks left! */
@skip      ATL_tPrintBV(stderr, "H", gcblkBV);
   }
/*
 * To be a computable row block, cblkBV must not be set (not already done)
 * and lcpyDone must be set (or input K-panel not yet copied)
 */
   {
      const ATL_BV_t n=ndiag>>shBV, nr=ndiag&modmskBV;
      ATL_BV_t i;

      for (i=1; i <= n; i++)
      {
         ATL_BV_t tmp;
         tmp = (~lrowBV[i]) & lcpyDonBV[i];
         ret |= tmp;
         lrowBV[i] = tmp;
      }
      if (nr)
      {
         ATL_BV_t tmp;
         tmp = ((~lrowBV[i]) & lcpyDonBV[i])&((1L<<nr)-1);
         ret |= tmp;
         lrowBV[i] = tmp;
      }
   }
   return(ret ? 1:0);
}

static long FindColWithCount
   (const long ndiag, const int UPPER, long p0, const long maxpos, 
    void *gCpanDonBV, void *gcblkBV0, size_t strd, ATL_BV_t *lcpyDonBV, 
    ATL_BV_t *lcolsBV, ATL_BV_t *lrowsBV)
{
   long J, p=p0-1, KEEPON = 0;

   SEARCH_JS:
      if (++p == maxpos)
         return(-1);
      J = ATL_FindFirstSetBitBV(lcolsBV, p);
      J = (J < maxpos) ? J : -1;
      if (J != -1)   /* possible candidate */
      {
         if (ATL_tIsBitSetBV(gCpanDonBV, J))  /* if this J already done */
         {
            p = J;                            /* goto next */
            goto SEARCH_JS;                   /* and search again */
         }
      }
/*
 *    If we've got a candidate col, compute lrowsBV and see if we can do work!
 */
      if (J != -1)
      {
         void *gcblkBV;
         gcblkBV = ATL_AddBytesPtr(gcblkBV0,J*strd);
         if (!compPossRows(UPPER, ndiag, J, gcblkBV, lcpyDonBV, lrowsBV))
         {
            if (J != maxpos-1)  /* if more blks to search */
            {
               p = J;           /* start search from following J */
               goto SEARCH_JS;  /* and keep looking */
            }
            else                /* no blocks left */
               J = -1;          /* stop search with failure */
         }
      }
/* 
 * If there are unsearched bits prior to p0, and we have so far not found
 * a candidate column, try again starting from 0
 */
   if (p0 && J == -1) /* if unsearched bits prior to p0, and we failed */
      return(FindColWithCount(ndiag, UPPER, 0, p0, gCpanDonBV, gcblkBV0, 
                              strd, lcpyDonBV, lcolsBV, lrowsBV));
   return(J);
}
/*
 * This function is does non-diagonal work when A/A^T have been copied,
 * and switches to diagonal work/copying when it can't find ready work
 * to do.  The idea is to reduce max bus load by intermixing copy and
 * no-copy code.  It returns once the copy is complete, so we can go
 * to lower-overhead code that doesn't need to check copy dependencies.
 */
static int DoDepBlks(const int rank, ATL_tsyrk_ammN_t *pd, 
                     TYPE *rS, TYPE *iS, TYPE *rC, TYPE *iC, TYPE *U)
{
   ipinfo_t *ip=pd->ip;
   const size_t stride = pd->LOCgap, Cstride=pd->Cgap;
   size_t skpA, skpB;
   ATL_BV_t *lcolsBV=ATL_AddBytesPtr(pd->locBVs,(rank+(rank<<1))*stride);
   ATL_BV_t *lrowsBV=ATL_AddBytesPtr(lcolsBV, stride);
   ATL_BV_t *lcpyDonBV=ATL_AddBytesPtr(lrowsBV, stride);
   void *gcpyDonBV = pd->cpydonBV, *gCpanDonBV=pd->cpanDonBV; 
   void *gcblkBV0=pd->cblkBV;
   TYPE *wA = pd->wA, *wB = pd->wAt, *C = pd->C;
   const unsigned long ndiag=pd->ndiag;
   long nun, p0, J, nunCpy=(-2), nunCpan=(-2);
   #ifdef TCPLX
      const TYPE *beta=pd->beta;
   #else
      const TYPE beta=pd->beta;
   #endif
   unsigned long mycnt=0;
   const ablk2cmat_t blk2c=ip->blk2c;
   const int UPPER = pd->flg & 1;
   int RECALC;

   lcolsBV[0] = lrowsBV[0] = lcpyDonBV[0] = ndiag;
   p0 = ATL_tGetLocalBoundsBV(gcpyDonBV, rank, NULL);
   if (UPPER)  /* Upper skips 1st colpan B */
   {
      skpA = 0;
      skpB = pd->pansz0;
   }
   else        /* Lower skips 1st rowpan A */
   {
      skpA = pd->pansz0;
      skpB = 0;
   }
   if (!pd->NODWORK && rank < pd->ncpDiag)  /* If I'm a thread asked to */
      goto DODIAG;                          /* copy before computing dep */
   DEPLOOP:
      if (pd->cpydone || pd->DONE)  /* if copying or full computation done */
         return(mycnt);             /* this routine is complete */
/*
 *    We must recalculate computable cols if the number of copied blocks
 *    or the number of finished columns changes
 */
      nun = ATL_tInfoBV(gcpyDonBV, ATL_TBV_NUNSET);
      if (ndiag-nun < 2)   /* Must have 2 kpans copied to do anything! */
         goto DODIAG;
      if (nun == 0)  /* if all blocks copied, go to faster code */
      {
         pd->cpydone = 1;
         return(mycnt);   /* that skip dependency checks! */
      }
      if (nun == nunCpy)  /* number of copied blocks hasn't changed */
      {
         nun = ATL_tInfoBV(gCpanDonBV, ATL_TBV_NUNSET);
         if (!nun)          /* if no Cpans left undone */
         {
            pd->DONE = 1;
            return(mycnt);  /* all non-diag work complete, so leave */
         }
         RECALC = (nun != nunCpan);
         nunCpan = nun;
      }
      else
      {
         RECALC = 1;      /* # of copied blks changed, so recalc! */
         nunCpy = ATL_tGlb2locBV(lcpyDonBV, gcpyDonBV, 0);
         if (nunCpy == 0)
         {
            pd->cpydone = 1;
            return(mycnt);
         }
      }
      if (RECALC)
         nunCpan = compPossCols(gCpanDonBV, lcpyDonBV, lcolsBV);
      J = FindColWithCount(ndiag, UPPER, p0, ndiag, gCpanDonBV, gcblkBV0, 
                           Cstride, lcpyDonBV, lcolsBV, lrowsBV);
      if (J == -1)    /* if no cols are computable */
         goto DODIAG; /* go copy more A/A^T by computing diag blks */
/*
 *    If we've got a column, work on it until out of good row blocks
 */
@skip      if (J != -1)
      {
         void *gcblkBV;
         long I=0, DIDSOME=0;

         gcblkBV = ATL_AddBytesPtr(gcblkBV0, J*Cstride);
         while ((I = ATL_FindFirstSetBitBV(lrowsBV, I)) != -1)
         {
            const long II = UPPER ? I : I-J-1;
            if (!ATL_tSetBitBV(gcblkBV, II)) /* 0=reserved it */
            {
               TYPE *wa, *wb;
               wa = IdxAw_ip(ip, wA, I, 0) - skpA;
               wb = IdxBw_ip(ip, wB, 0, J) - skpB;
               Mjoin(PATL,iploopsK)(ip, I, J, NULL, NULL, IdxC_ip(ip, C, I, J),
                                    3, wa, wb, rC, iC, beta, blk2c);
               #if DEBUG > 1
                  fprintf(stderr, "%d: C(%ld,%ld), wA=%p, wB=%p\n", 
                          rank, I, J, wa, wb);
               #endif
               #if DEBUG
                  mycnt++;
               #endif
               DIDSOME = 1;
            }
            if (++I == ndiag)
               break;
            if (pd->cpydone)
               return(mycnt);
         }
/*
 *       If I computed some blocks, see if this column is finished
 *       If column is finished, see if whole problem is finished.
 *
 *       This is unsafe, and can result in not marking completed column.
 *       This is OK, because eventually we'll exit this function due to
 *       the copying finishing, and DoNonDiag will mark column safely.
 */
         if (DIDSOME)
         {
            if (!ATL_tInfoBV(gcblkBV, ATL_TBV_NUNSET))
            {
               ATL_tSetBitBV(gCpanDonBV, J);
               if (!ATL_tInfoBV(gCpanDonBV, ATL_TBV_NUNSET))
               {
                  pd->DONE = 1;
                  return(mycnt);
               }
            }
         }
         else             /* someone bogarted all my rows, try reducing */
            goto DODIAG;  /* contention by producing more possible cols */
      }
   goto DEPLOOP;
DODIAG:
   if (!pd->NODWORK)
   {
      J = ATL_gatmctr_dec(pd->dCtr, rank);
      if (J)
         Do1Diag(rank, pd, ndiag-J, rS, iS, rC, iC, U);
      else
         pd->NODWORK = 0;
   }
   goto DEPLOOP;
}

static long DoNonDiag
(
   const int rank,       /* worker # 0 <= rank < P */
   ATL_tsyrk_ammN_t *pd, /* problem definition */
   const unsigned int P, /* # of workers; rank < P */
   TYPE *rC,             /* real C, same as iC for real */
   TYPE *iC              /* imag C, same as rC for real */
)
{
   ipinfo_t *ip=pd->ip;
   const unsigned long ndiag=pd->ndiag;
   const int UPPER = (pd->flg & 1);
   TYPE *wA = pd->wA, *wB = pd->wAt;
   TYPE *C = pd->C;
   void *cpanDonBV = pd->cpanDonBV;
   long mycnt=0;
   #ifdef TCPLX
      const TYPE *beta=pd->beta;
   #else
      const TYPE beta=pd->beta;
   #endif
   const ablk2cmat_t blk2c=ip->blk2c;
#if 0   /* ignore this until the high-overhead cleanup debugged */
/*
 * First make one pass through cols, using only multiples of my rank
 */
   if (ndiag >= P)
   {
      unsigned int ngrab=1, ngot;
      unsigned long msk=1, old, i;
      for (j=0; j < ndiag; j += P)
      {
         const long c=j+rank;
         long r;
         void *cblkBV;
         if (c >= ndiag)
            break;
         cblkBV = ATL_AddBytesPtr(pd->cblkBV, pd->Cgap*c);
         
         if (ngrab != pd->ngrab)
         {
            ngrab = pd->ngrab;
            msk = (ngrab < bpiBV) ? ((1L<<ngrab)-1):allsetBV;
         }
/*
 *       Attempt to get ngrab rowblks of C from this c's cblkBV starting @ r
 */
         r = ATL_tFindUnsetBitBV(cblkBV, rank);
         if (r == -1)
         {
            if (!ATL_tIsBitSetBV(cpanDonBV, c))
               ATL_tSetBitBV(cpanDonBV, c);
            continue;
         }
         ngot = ngrab;
         old = ATL_tSetRangeBV(cblkBV, &ngot, r, msk);
         if (ngot)
         {
            unsigned int k;
            r = (UPPER) ? r : (1+c+r);
            for (k=0; k < ngot; k++)
            {
               if ((msk>>k)&(1L))
               {
                  Mjoin(PATL,iploopsK)(ip, k+r, c, NULL, NULL, 
                     IdxC_ip(ip, C, r+k, c), 3, IdxAw_ip(ip, wA, r+k, 0), 
                     IdxBw_ip(ip, wB, 0, c), rC, iC, beta, blk2c);
               }
            }
         }
      }
   }
#endif
/*
 * Now, all cores to into mode where they steal work from each other,
 * taking only 1 Cblk at a time.  Threads start with differing cblkBV regions 
 * to minimize contention.
 */
   pd->ngrab = 1;  /* tell other cores to stop bogarting the Cblks */
   if (!pd->DONE)
   {
      void *cblkBV;
      const long b = ndiag / P, bs=b*rank, ndi=(UPPER) ? ndiag : ndiag-1;
      long c;
      size_t skpA, skpB;

      if (UPPER)  /* Upper skips 1st colpan B */
      {
         skpA = 0;
         skpB = pd->pansz0;
      }
      else        /* Lower skips 1st rowpan A */
      {
         skpA = pd->pansz0;
         skpB = 0;
      }
      while ( (c=ATL_tFindUnsetBitBV(cpanDonBV, rank)) != -1)
      {
         long r;
         TYPE *wa, *wb;
         int DIDSOME=0;

         if (pd->DONE)
            return(mycnt);
         cblkBV = ATL_AddBytesPtr(pd->cblkBV, pd->Cgap*c);
         DOROWS:
            r = ATL_tSetUnsetBitBV(cblkBV, rank);
            if (r < 0)
            {
               if (!ATL_tIsBitSetBV(cpanDonBV, c))
                  ATL_tSetBitBV(cpanDonBV, c);
               continue;
            }
            r = (UPPER) ? r : 1+c+r;
            wa = IdxAw_ip(ip, wA, r, 0) - skpA;
            wb = IdxBw_ip(ip, wB, 0, c) - skpB;
            #if DEBUG > 1
               fprintf(stderr, "%u:Nondiag(%ld,%ld), wA=%p, wB=%p\n", 
                       rank, r, c, wa, wb);
            #endif
            Mjoin(PATL,iploopsK)(ip, r, c, NULL, NULL, IdxC_ip(ip, C, r, c), 
                                 3, wa, wb, rC, iC, beta, blk2c);
            DIDSOME = 1;
            mycnt++;
         goto DOROWS;
         @beginskip
         if (DIDSOME)
         {
            if (!ATL_tInfoBV(cblkBV, ATL_TBV_NUNSET))
            {
               if (!ATL_tIsBitSetBV(cpanDonBV, c))
               {
                  ATL_tSetBitBV(cpanDonBV, c);
                  if (!ATL_tInfoBV(cpanDonBV, ATL_TBV_NUNSET))
                  {
                     pd->DONE = 1;
                     return(mycnt);
                  }
               }
            }
         }
         @endskip
      }
   }
   return(mycnt);
}

static void DoWorkN(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tsyrk_ammN_t *pd = pp->PD;
   long ndiag=0, ndep=0, nindep=0;
   const unsigned int P = pp->nworkers, szC = Mmax(pd->ip->szC,pd->szCs);
   TYPE *iS = pd->wC + vrank*pd->wrksz, *rS, *rC, *iC, *U;

   iS = ATL_AlignPtr(iS);
   #ifdef TCPLX
      rS = iS + pd->szS;
      iC = rS + pd->szS;
      iC = ATL_AlignPtr(iC);
      rC = iC + szC;
      U  = rC + szC;
   #else
      rS = iS;
      iC = rS + pd->szS;
      rC = iC = ATL_AlignPtr(iC);
      U  = iC + szC;
   #endif
   if (pd->sya2blk && !(pd->flg & 1))
      U = NULL;
@skip   if (!(pd->NODWORK) && vrank < pd->ncpDiag)
@skip      ndiag = DoDiag(vrank, pd, rS, iS, rC, iC, U);

   if (!(pd->NOINICPY))
      DoSharDiag(P, vrank, pd, rS, iS, rC, iC, U);
   if (!pd->cpydone && !pd->DONE)
      ndep = DoDepBlks(vrank, pd, rS, iS, rC, iC, U);
   if (!pd->DONE)
      nindep = DoNonDiag(vrank, pd, P, rC, iC);
   #if DEBUG
      fprintf(stdout, "%d: Cblks=%ld+%ld, ndiag=%ld,%d Dblks=%ld(%d)\n", 
              vrank, ndep, nindep, pd->ndiag, pd->nshar, ndiag, pd->ncpDiag);
   #endif
}
/*
 * SYRK where all parallelism comes from blocks of N, built atop amm directly
 *    if (TA == AtlasNoTrans) 
 *       C = alpha * A*A' + beta*C
 *    else
 *       C = alpha * A'*A + beta*C
 *    C is an upper or lower symmetric NxN matrix, 
 *    A is a dense rectangular NxK (NoTrans) or KxN (Trans) matrix
 * RETURNS: 0 if operation performed, non-zero otherwise.
 *   This routine assumes it can copy all of A up-front to simplify parallelism.
 *   Will return non-zero if memory cannot be allocated, on the assumption
 *   it is called from recursive implementation that can recur until malloc
 *   succeeds.
 */
#ifdef Conj_
   #define tsyrk_amm_N Mjoin(PATL,therk_amm_N)
#else
   #define tsyrk_amm_N Mjoin(PATL,tsyrk_amm_N)
#endif
int tsyrk_amm_N
(
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS TA,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   void *vp=NULL, *vp0;
   size_t szA, szC, szKpan, sz, extra, szThr, szAll;
   size_t ndiag, ncblks;
   double stim;
   unsigned int i, P, idx, nb, nbS, kbS, szS, szPO, nshar=0, ncpDiag, n;
   unsigned int szBETABV=0, DISYRK=0;
   ATL_tsyrk_ammN_t pd;
   ipinfo_t ip;
   #ifdef ATL_AVX
      #define symul 1.34
   #else
      #define symul 1.2
   #endif
   #define parpen (2.0*ATL_tstart_sec)
   #ifdef Conj_
      const enum ATLAS_TRANS TB = (TA == AtlasNoTrans) ?
                                  AtlasConjTrans : AtlasNoTrans;
   #else
      const enum ATLAS_TRANS TB = (TA == AtlasNoTrans) ?
                                  AtlasTrans : AtlasNoTrans;
   #endif
/*
 * Demand at least 2 large blocks along some dimension
 */
   n = ATL_sqAMM_LASTNB<<1;
   n = Mmin(200, n);
   if (N < n && K < n)
      goto DO_SERIAL;
   pd.flg = (Uplo == AtlasUpper);
   pd.flg |= (TA == AtlasNoTrans) ? 2 : 0;
   #if 1
@skip      idx = Mjoin(PATL,tGetParTriCIndx)(ATL_NTHREADS, pd.flg, N, K, &nb);
      if (K >= (ATL_sqAMM_LASTKB<<3))
         idx = ATL_sqAMM_NCASES-1;
      else
         idx = Mjoin(PATL,GetSyrkIdx)(pd.flg, N, K, symul);
   #else
      idx = 0;
      nb = 4;
   #endif
   stim = Mjoin(PATL,sSyrkTimeEst)(idx, pd.flg, N, K, symul);
   if (stim <= parpen+ATL_tstart_sec+4.0*ATL_tstartgap_sec)
      goto DO_SERIAL;
   nb = Mjoin(PATL,sqGetAmmInfoInt)('K', idx);
   P = ATL_NTHREADS;
   if (N <= nb)  /* only possible parallelism is along K */
   {
      nshar = 1;                      /* share all K blocks */
      szKpan = (K+nb-1) / nb;
      P = Mmin(szKpan, ATL_NTHREADS); /* limit parallism to k blocks */
      goto GOT_NB;
   }
   ndiag = nb*ATL_NTHREADS;
   if (ndiag < N)            /* is there enough paralleism along N? */
      goto GOT_NB;
   if (ndiag < K)  /* is there enough parallism from K? */
   {
      nshar = 1;   /* means make all blks shared */
      goto GOT_NB;
   }
/*
 * See if we can create enough parallelism by sharing nshar initial
 * K-panels, which will unleash at least ncblks parallelism afterwords
 */
   szKpan = (K+nb-1) / nb;
   ndiag = (N+nb-1) / nb;
   if (nshar == 1)
      nshar = ndiag;
   ncblks = ((ndiag-1)*ndiag)>>1;
   if (szKpan*ndiag >= P)
   {
      nshar = P / szKpan;
      if (nshar < 2)      /* need at least 2 panels complete */
         nshar = 2;       /* before non-diag comp begins */
      goto GOT_NB;
   }
/*
 * If we reach here, need to try reducing nb to increase parallelism;
 * If this doesn't work, reduce parallelism.
 */
   nshar = 1;         /* get max parallism from K */
   nbS = nb;
   n = ATL_lcm(Mjoin(PATL,sqGetAmmInfoInt)('m', idx),
               Mjoin(PATL,sqGetAmmInfoInt)('n', idx));
   if (n <= nb)
   {
      P = szKpan;
      goto GOT_NB;
   }
   nb = (nb/n)*n;
   TRY_ANOTHER_NB:
      ndiag = N / nb;
      ncblks = ((ndiag-1)*ndiag)>>1;
      if (ndiag*szKpan+ncblks >= ATL_NTHREADS)
         goto GOT_NB;
      nbS = nb;
      nb -= n;
   if (nb > 24)
      goto TRY_ANOTHER_NB;

   nb = nbS;
   ndiag = N / nb;
   ncblks = ((ndiag-1)*ndiag)>>1;
   P = ndiag*szKpan+ncblks;
GOT_NB:
   if (P > 1)
   {
      double trem, ptim;
      trem = stim;
      ptim = parpen + ATL_tstart_sec;
      for (i=0; i < P; i++)
      {
         double t1;
         t1 = (i+1)*ATL_tstartgap_sec;
         if (trem > t1)
         {
            trem -= t1;
            ptim += ATL_tstartgap_sec;
         }
         else
         {
            ptim += trem/(i+1);
            break;
         }
      }
      ptim += trem / P;
      if (stim <= 1.2*ptim) /* don't accept less than 20% win */
         goto DO_SERIAL;    /* since parallel times hugely variable */
      else
         P = (ptim < stim) ? i : 1;
      #if DEBUG > 1
         fprintf(stdout, "   predicted speedup = %.2f (%d)\n", stim/ptim, i);
      #endif
      if (P < 1)
         goto DO_SERIAL;
      Mjoin(PATL,sqFillInIPInfo)(&ip, idx, TA, TB, N, N, K, lda, lda, ldc,
                                 alpha, beta, nb);
      ndiag = ip.nfnblks + ip.npnblks;
      ncblks = ((ndiag-1)*ndiag)>>1;
      ncpDiag = 0;
@skip   ncpDiag = ndiag - nshar;  /* max kpans that can be copied all at once */
@skip   n = P>>1;
@skip   ncpDiag = Mmin(n,ncpDiag);
/*
 * If N <= NB, always use SYRK, in this case all computation done by SYRK.
 * -> This means we can only copy A once, while doing half the computation.
 * If we do SYRK for diagonals, but GEMM for off-diagonals, we must make
 * 3 copies of of A: 2 for GEMM, one for SYRK (if SYRK's copy is the same
 * as GEMM's A or B copy, this isn't true, but this is only rarely the case,
 * since SYRK always uses K-major wt MU=NU, which is a loser on modern x86).
 * -> Cost of extra copy is N*K*(Mt) (Mt = time to copy from mem)
 * -> Diagonal computation savings is nb*N*K (4*nb*N*K for cplx)
 * ==> Use syrk when NB > Mt; we estimate Mt as 16*P
 *     -> compute speed scales perfectly with P, but Mt doesn't
 */
      #ifdef TCPLX
         DISYRK = (N <= nb || nb > (ATL_NTHREADS<<2));
      #else
         DISYRK = (N <= nb || nb > (ATL_NTHREADS<<4));
      #endif
   }
   if (P < 2)
   {
DO_SERIAL:
      #ifdef Conj_
         Mjoin(PATL,herk)(Uplo, TA, N, K, *alpha, A, lda, *beta, C, ldc);
      #else
         Mjoin(PATL,syrk)(Uplo, TA, N, K, alpha, A, lda, beta, C, ldc);
      #endif
      return(0);
   }
   #if DEBUG
   printf("%u: N=%d,%d (%d*%d+%d) F=(%d,%d;%d,%d), kb=%d SY=%d, nshar=%ld\n", 
          P, (int)N, (int)K, (int)ip.nfnblks, ip.nb, ip.pnb, ip.mF, ip.nF, 
          ip.nmuF, ip.nnuF, ip.kb, DISYRK, nshar);
   #endif
/*
 * Set up parallel data structure
 */
   szKpan = ip.nfnblks ? ip.szA : ip.pszA;
   szKpan *= ip.nfkblks+1;
   if (ndiag > 1)
   {
      szA = szKpan * (ndiag-1);
      szAll = ATL_MulBySize(szA + szA) + 2*ATL_Cachelen;
   }
   else 
      szA = szAll = 0;
   extra = (ip.mu<<1)*ip.nu;
/*
 * Each thread needs: spc C (max(SYRK,GEMM)) + spc for U (if Upper) + 1 blk syrk
 */
   nb = (ip.nfnblks) ? ip.nb : ip.pnb;
   szThr = ip.szC;
   if (DISYRK)   /* need extra workspace if doing SYRK */
   {
      extra = Mmax(extra, (ATL_SYRKK_NU+ATL_SYRKK_NU)*ATL_SYRKK_NU);
      nbS = ((nb+ATL_SYRKK_NU-1)/ATL_SYRKK_NU)*ATL_SYRKK_NU;
      kbS = ((ip.kb+ATL_SYRKK_KU-1)/ATL_SYRKK_KU)*ATL_SYRKK_KU;
      szC = ((nbS+1)*nbS)>>1; /* only need lower tri blks, not full nnu*nnu */
      szC *= ((ATL_SYRKK_NU*ATL_SYRKK_NU+ATL_SYRKK_VLEN-1)/ATL_SYRKK_VLEN)
             * ATL_SYRKK_VLEN;
      if (Uplo == AtlasUpper)
         extra = Mmax(extra, ip.szC);
      szS = nbS * kbS;
      szS = ((szS+ATL_SYRKK_VLEN-1)/ATL_SYRKK_VLEN)*ATL_SYRKK_VLEN;
      szThr = Mmax(szThr, szC);         /* spc for amm/syrk C */
      szThr = ATL_MulBySize(szS + szThr + extra) + ATL_Cachelen + ATL_sizeof-1;
      szThr = ATL_DivBySize(szThr);
   }
   else 
   {
      szS = (ip.nfnblks) ? ip.szA : ip.pszA;
      szC = (ip.nfnblks) ? ip.nb : ip.pnb;
      szC *= szC;
      szC = Mmax(szC, extra);
      szThr += szS + szC;
      szC = 0;
      szThr += ATL_DivBySize(ATL_Cachelen+ATL_Cachelen);
   }
   pd.dCgap = pd.Cgap = ATL_tSizeofBV(ndiag, P);
   pd.KBCgap = ATL_gatmctr_sizeof(P, ATL_GAC_PUB);
   pd.KDCgap = pd.KBCgap; /* ATL_gatmctr_sizeof(P, ATL_GAC_PUB); */
   pd.dCinc = sizeof(ATL_lock_t);
   pd.dCinc = (size_t) ATL_AlignSafeLS(pd.dCinc);
   pd.nkblks = ip.nfkblks + 1;
   pd.LOCgap = (ndiag+bpiBV-1) >> shBV;
   pd.LOCgap = (pd.LOCgap+1)*sizeof(ATL_BV_t);
   pd.LOCgap = (size_t) ATL_AlignSafeLS(pd.LOCgap);
   pd.diCgap = ATL_atmctr_sizeof;
   if (nshar)  /* do I need space for nshar-based work? */
   {
      szBETABV = (((nshar+bpiBV-1)>>shBV)+1)*sizeof(ATL_BV_t);
      szBETABV = (size_t) ATL_AlignSafeLS(szBETABV);
      szPO = nshar*(pd.KBCgap+pd.KDCgap+pd.dCinc+pd.diCgap) + szBETABV;
   }
   else
      szPO = 0;
   szPO += ndiag*pd.Cgap;                    /* sz of cblkBV */
   szPO += pd.KBCgap;                        /* dCtr */
   szPO += pd.dCgap;                         /* cpydonBV */
   szPO += (P+(P<<1))*pd.LOCgap;             /* each core has 3 locBVs */
   szPO += szBETABV;                         /* dbetaBV */
   sz = ATL_MulBySize(szAll + P*szThr) + szPO + ATL_SAFELS;

   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)                              /* if I'm over malloc limit */
      return(2);                            /* return and recur on K */

   pd.wrksz = szThr SHIFT;
   pd.KbegCtr  = ATL_AlignSafeLS(vp);
   pd.KdonCtr  = ATL_AddBytesPtr(pd.KbegCtr, pd.KBCgap*nshar);
   pd.cpydonBV = ATL_AddBytesPtr(pd.KdonCtr, pd.KDCgap*nshar);
   pd.locBVs   = ATL_AddBytesPtr(pd.cpydonBV, pd.dCgap);
   pd.cblkBV   = ATL_AddBytesPtr(pd.locBVs, (P+(P<<1))*pd.LOCgap);
   pd.Cdmuts   = ATL_AddBytesPtr(pd.cblkBV, pd.dCgap*ndiag);
   pd.diCtr    = ATL_AddBytesPtr(pd.Cdmuts, pd.dCinc*nshar);
   pd.dCtr     = ATL_AddBytesPtr(pd.diCtr, pd.diCgap);
   pd.dbetaBV  = ATL_AddBytesPtr(pd.dCtr, pd.KBCgap);
   if (ndiag > 1)
   {
      pd.wA    = ATL_AddBytesPtr(pd.dbetaBV, szBETABV);
      pd.wA    = ATL_AlignPtr(pd.wA);
      pd.wAt   = pd.wA + (szA SHIFT);
      pd.wAt   = ATL_AlignPtr(pd.wAt);
      pd.wC    = pd.wAt + (szA SHIFT);
   }
   else
   {
      pd.wA    = pd.wAt = NULL;
      pd.wC    = ATL_AddBytesPtr(pd.dbetaBV, szBETABV);
   }
   pd.wC       = ATL_AlignPtr(pd.wC);

   pd.ip = &ip;
   pd.ndiag = ndiag;
   pd.nshar = nshar;
   pd.ncpDiag = ncpDiag;
@skip      pd.ncpDiag = (P >= 6) ? 3 : P>>1;
   pd.ncblks = ncblks;
   pd.beta = beta;
   pd.szCs = szC;
   pd.szS = szS;
   pd.NODWORK = 0;
   pd.DONE = (ndiag > 1) ? 0 : 1;
   pd.ngrab = 4;  /* adjust this later */
      
   if (DISYRK)
   {
      pd.sya2blk=IS_COLMAJ(TA)?Mjoin(PATL,a2blk_syrkT):Mjoin(PATL,a2blk_syrkN);
      if (Uplo == AtlasLower)
      {
         pd.pansz0 = (size_t)IdxAw_ip(&ip, NULL, 1, 0);
         if (SCALAR_IS_NONE(alpha))
         {
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_an_b1);
            if (SCALAR_IS_ONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_an_b1);
            else if (SCALAR_IS_NONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_an_bn);
            else 
               pd.syblk2c = SCALAR_IS_ZERO(beta) ? 
                  Mjoin(PATL,syblk2cmat_an_b0) : Mjoin(PATL,syblk2cmat_an_bX);
         }
         else if (SCALAR_IS_ONE(alpha))
         {
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_a1_b1);
            if (SCALAR_IS_ONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_a1_b1);
            else if (SCALAR_IS_NONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_a1_bn);
            else 
               pd.syblk2c = SCALAR_IS_ZERO(beta) ? 
                  Mjoin(PATL,syblk2cmat_a1_b0) : Mjoin(PATL,syblk2cmat_a1_bX);
         }
         else
         {
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_aX_b1);
            if (SCALAR_IS_ONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_aX_b1);
            else if (SCALAR_IS_NONE(beta))
               pd.syblk2c = Mjoin(PATL,syblk2cmat_aX_bn);
            else 
               pd.syblk2c = SCALAR_IS_ZERO(beta) ? 
                  Mjoin(PATL,syblk2cmat_aX_b0) : Mjoin(PATL,syblk2cmat_aX_bX);
         }
      }
      else
      {
         pd.pansz0 = (size_t)IdxBw_ip(&ip, NULL, 0, 1);
         if (SCALAR_IS_NONE(alpha))
         {
            pd.syblk2c = Mjoin(PATL,syblk2cmat_an_b0);
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_an_b1);
         }
         else if (SCALAR_IS_ONE(alpha))
         {
            pd.syblk2c = Mjoin(PATL,syblk2cmat_a1_b0);
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_a1_b1);
         }
         else
         {
            pd.syblk2c = Mjoin(PATL,syblk2cmat_aX_b0);
            pd.syblk2c_b1 = Mjoin(PATL,syblk2cmat_aX_b1);
         }
      }
   }
   else
   {
      int ialp;

      if (Uplo == AtlasLower)
         pd.pansz0 = (size_t)IdxAw_ip(&ip, NULL, 1, 0);
      else
         pd.pansz0 = (size_t)IdxBw_ip(&ip, NULL, 0, 1);
      if (SCALAR_IS_NONE(ip.alpC))
         ialp = -1;
      else
         ialp = SCALAR_IS_ONE(ip.alpC) ? 1 : 2;
      pd.syblk2c = Mjoin(PATL,sqGetAmmInfoPtr)(idx, 1, ialp, 0);
      pd.sya2blk = NULL;
   }
   pd.pansz0 /= sizeof(TYPE);
   pd.NODWORK = (ndiag > nshar) ? 0 : 1;
   pd.NOINICPY = !nshar;
   pd.cpydone = 0;
   pd.A = A;
   pd.lda = lda;
   pd.C = C;
   pd.ldc = ldc;
/*
 * Initialize all parallel overhead structures
 */
   {
      void *vp1;

      for (vp0=pd.Cdmuts,i=0; i < nshar; i++,vp0=ATL_AddBytesPtr(vp0,pd.dCinc))
         ATL_lock_init(vp0);

      vp0 = pd.KbegCtr;
      vp1 = pd.KdonCtr;
      for (i=0; i < nshar; i++)
      {
         ATL_gatmctr_init(vp0, P, pd.nkblks, ATL_GAC_PUB);
         ATL_gatmctr_init(vp1, P, pd.nkblks, ATL_GAC_PUB);
         vp0 = ATL_AddBytesPtr(vp0, pd.KBCgap);
         vp1 = ATL_AddBytesPtr(vp1, pd.KDCgap);
      }
      if (ndiag > nshar)
         ATL_gatmctr_init(pd.dCtr, P, ndiag-nshar, ATL_GAC_PUB);
      if (nshar)
         ATL_atmctr_init(pd.diCtr, nshar);

      vp0 = pd.cblkBV;
      for (i=0; i < ndiag; i++)
      {
         int n = (Uplo == AtlasUpper) ? i : ndiag-i-1;
@skip         fprintf(stderr, "cblkBV[%u] = %p\n", i, vp0);
         ATL_tInitBV(vp0, n, P);
         vp0 = ATL_AddBytesPtr(vp0, pd.Cgap);
      }
      ATL_tInitBV(pd.cpydonBV, ndiag, P);
@skip      ATL_tInitBV(pd.dbetaBV, nshar, P);
      ATL_InitBV(nshar, pd.dbetaBV, 0);
/*
 *    cpanDonBV is set to the colpan with no non-diag work: 1st for Upper,
 *    last for Lower
 */
      if (Uplo == AtlasUpper)
      {
         pd.cpanDonBV = pd.cblkBV;
         ATL_tInitBV(pd.cpanDonBV, ndiag, P);
         ATL_tScopeBitBV(pd.cpanDonBV, 0, 514);
      }
      else  /* Uplo == AtlasLower */
      {
         pd.cpanDonBV = ATL_AddBytesPtr(pd.cblkBV, (ndiag-1)*pd.Cgap);
         ATL_tInitBV(pd.cpanDonBV, ndiag, P);
         ATL_tScopeBitBV(pd.cpanDonBV, ndiag-1, 514);
      }
   }

   ATL_goParallel(P, DoWorkN, NULL, &pd, NULL);
   #ifdef DEBUG2
      ATL_assert(ATL_FindFirstUnsetBitBV(pd.cblkBV, 0) == -1);
   #endif

/*
 * Free allocated structures and return; 
 */
   for (vp0=pd.Cdmuts,i=0; i < nshar; i++,vp0=ATL_AddBytesPtr(vp0,pd.dCinc))
      ATL_lock_destroy(vp0);
   free(vp);
   return(0);
}

#ifdef Conj_
   #define tsyrk_amm Mjoin(PATL,therk_amm)
#else
   #define tsyrk_amm Mjoin(PATL,tsyrk_amm)
#endif
void tsyrk_amm
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CSZT N,
    ATL_CSZT K, const SCALAR alpha, const TYPE *A, ATL_CSZT lda,
    const SCALAR beta, TYPE *C, ATL_CSZT ldc)

{
   #ifdef TCPLX
      const TYPE ONE[2]={ATL_rone, ATL_rzero};
      size_t kmul = (Trans==AtlasNoTrans||Trans==AtlasConj) ? lda+lda : 2;
   #else
      #define ONE ATL_rone
      size_t kmul = (Trans==AtlasNoTrans) ? lda : 1;
   #endif
@beginskip
   ATL_assert(N > ATL_sqAMM_LASTNB);
   if (N <= Mmax(ATL_sqAMM_LASTMB,ATL_sqAMM_LASTNB))
   {
      if (!Mjoin(PATL,tsyrk_amm_K)(Uplo, Trans, N, K, alpha, A, lda, 
                                   beta, C, ldc))
         return;
   }
@endskip
/*
 *  Recur on K until tsyrk_amm can allocate enough space 
 */
    if (tsyrk_amm_N(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc))
    {
       unsigned int kL = K>>1, kR=K-kL;
       const TYPE *a = A + kL*kmul;
/*
 *     This stopping criteria should never happen, but it's here in case
 *     we have a system where you can't malloc much of anything, where we'll
 *     just try serial
 */
@skip       if (kL < 32 || kR < 32)
       if (kL <= ATL_sqAMM_LASTKB)
       {
           Mjoin(PATL,syrk_IP)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
           return;
       }
      tsyrk_amm(Uplo, Trans, N, kL, alpha, A, lda, beta, C, ldc);
      tsyrk_amm(Uplo, Trans, N, kR, alpha, a, lda, ONE,  C, ldc);
    }
}
#ifndef TCPLX
   #undef ONE
#endif
@ROUT ATL_threadpool
#include "atlas_threads.h"
#include "atlas_misc.h"
#include "atlas_bitvec.h"
/*
 * These are wrapperf for original codes, which expect to get a launchstruct
 * as the problem definition, and DoWork expects to get a pointer to my entry
 * in the thread array
 */
void ATL_oldjobwrap(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp = vpp;
   ATL_LAUNCHSTRUCT_t *lp = pp->PD;
   void *vp = pp->threads[rank].vp;

   pp->threads[rank].vp = lp;
   lp->DoWork(lp, pp->threads+rank);
   pp->threads[rank].vp = vp;
}

void ATL_oldcombwrap(void *vpp, int rank, int vrank, int vhisrank)
{
   ATL_tpool_t *pp = vpp;
   ATL_LAUNCHSTRUCT_t *lp = pp->PD;

   if (lp->DoComb)
      lp->DoComb(lp->opstruct, vrank, vhisrank);
}

/*
 * Fully dynamic combine with restriction that iam=0 gets final answer
 * NOTE: assumed that during operation, have change tp->P and tp->rank
 *       to local values.  They will be restored to true rank & P here.
 */
static void ATL_dyncomb(ATL_tpool_t *pp, const int rank, const int vrank)
{
   const unsigned int P=pp->nworkers;
   ATL_thread_t *tp = pp->threads + rank;
/*
 * Let everyone know my data is ready for combining
 * Thread 0's work is not available because some combines expect
 * thread 0 to have the answer
 */
   if (vrank)
   {
      ATL_mutex_lock(pp->combmut);
      ATL_SetBitBV(pp->combReadyBV, vrank);
      ATL_mutex_unlock(pp->combmut);
   }
/*
 * Now participate in combine until everything is done, or someone
 * combines my stuff, and thus ends my participation
 */
   do
   {
      int d;
/*
 *    If my combDone bit is set, someone has combined my results: I'm finished
 */
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank))
         return;
/*
 *    Seize mutex lock and do the work if still there & I'm still in game
 */
      ATL_mutex_lock(pp->combmut);
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank)) /* I'm done */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
      d = ATL_FindFirstSetBitBV(pp->combReadyBV, 0);
      if (d == vrank)
         d = (vrank != P-1) ?  
            ATL_FindFirstSetBitBV(pp->combReadyBV, vrank+1) : -1;
      if (d != -1)  /* work still needs to be done */
      {
         ATL_SetBitBV(pp->combDoneBV, d);        /* combined thr is done */
         ATL_UnsetBitBV(pp->combReadyBV, d);     /* I took, no longer avail */
         ATL_UnsetBitBV(pp->combReadyBV, vrank); /* I'm not avail, working */
         ATL_mutex_unlock(pp->combmut);
         pp->combf(pp, rank, vrank, d);
/*
 *       After finishing my combine, my buffer ready for other's use
 */
         if (vrank)  /* thr 0 must get answer, so never available */
         {
            ATL_mutex_lock(pp->combmut);
            ATL_SetBitBV(pp->combReadyBV, vrank);   /* now avail again */
         }
/*
 *       Check if I'm the last guy left, if so, combine is complete
 *       Only vrank=0 allowed to be last guy (0's buff has final answer)
 */
         else  /* I'm 0, so see if combine is done */
         {
            ATL_mutex_lock(pp->combmut);
            if (ATL_FindFirstUnsetBitBV(pp->combDoneBV, 1) == -1)
            {
               ATL_SetBitBV(pp->combDoneBV, vrank);    /* I'm done */
               ATL_UnsetBitBV(pp->combReadyBV, vrank); /* so not ready */
               ATL_mutex_unlock(pp->combmut);
               return;
            }
         }
      }
      else if (vrank)  /* No work to do, leave to reduce lock contention */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
      ATL_mutex_unlock(pp->combmut);
   }     
   while(1);   /* end combine loop */
}

/*
 * This combine is for routines that require only leftward combination,
 * so that you can add results only from vranks greater (to the right)
 * than yourself, and then only if you have already added in all the
 * ranks in between yourself and the candidate.  This type of result
 * is naturally enforced by log2 reduction and linear sum-to-zero, which
 * some of the earlier tblas assume.
 */
static void ATL_leftcomb(ATL_tpool_t *pp, const int rank, const int vrank)
{
   const unsigned P = pp->nthr;
   const unsigned int nw = pp->nworkers;
   ATL_thread_t *tp = pp->threads + rank;
/*
 * Let everyone know my data is ready for combining
 * Thread 0's work is not available because some combines expect
 * thread 0 to have the answer
 */
   if (vrank)
   {
      ATL_mutex_lock(pp->combmut);
      ATL_SetBitBV(pp->combReadyBV, vrank);
      ATL_mutex_unlock(pp->combmut);
   }
/*
 * Now participate in combine until everything is done, or someone
 * combines my stuff, and thus ends my participation
 */
   do
   {
      int i, d;
/*
 *    If my combDone bit is set, somone has combined my results, so I'm finished
 */
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank))
         return;
/*
 *    If everyone to right of me is finished, there's nothing left for me to do
 */
      for (i=vrank+1; i < nw && ATL_IsBitSetBV(pp->combDoneBV, i); i++);
      if (i == nw)
         return;
/*
 *    Sieze mutex lock and do the work if I'm still in the game
 */
      ATL_mutex_lock(pp->combmut);
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank)) /* I'm done */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
/*
 *    Take buffers from as far right as possible, as they have least to do
 */
      for (i=nw-1; i > vrank; i--)
      {
/*
 *       If target is ready, there must be no unfinished nodes between us
 */
         if (ATL_IsBitSetBV(pp->combReadyBV, i)) /* got a candidate */
         {
            int j;
            for (j=vrank+1; j < i; j++)
               if (!ATL_IsBitSetBV(pp->combDoneBV, j)) 
                  break;
            if (j == i)  /* he's a legal partner */
               break;
         }
      }
      if (i != vrank)  /* work still needs to be done */
      {
         ATL_SetBitBV(pp->combDoneBV, i);        /* he's done, can quit */
         ATL_UnsetBitBV(pp->combReadyBV, i);     /* he's not avail, done  */
         ATL_UnsetBitBV(pp->combReadyBV, vrank); /* I'm not avail, working */
         ATL_mutex_unlock(pp->combmut);
         pp->combf(pp, rank, vrank, i);
/*
 *       After finishing my combine, my buffer ready for other's use
 */
         if (vrank)    /* rank=0 gets answer, so never available */
         {
            ATL_mutex_lock(pp->combmut);
            ATL_SetBitBV(pp->combReadyBV, vrank);   /* now avail again */
         }
/*
 *       Check if I'm the last guy left, if so, combine is complete
 *       Only vrank=0 allowed to be last guy (0's buff has final answer)
 */
         else  /* I'm 0, so see if combine is done */
         {
            ATL_mutex_lock(pp->combmut);
            if (ATL_FindFirstUnsetBitBV(pp->combDoneBV, 1) == -1)
            {
               tp->rank = rank;                       /* put my thr info back */
               tp->P = P;                             /* to global values */
               ATL_SetBitBV(pp->combDoneBV, vrank);   /* I'm done */
               ATL_UnsetBitBV(pp->combReadyBV, vrank);/* so not ready */
               ATL_mutex_unlock(pp->combmut);
               return;
            }
         }                             /* end else I'm 0 */
      }                                /* end work still to be done */
      ATL_mutex_unlock(pp->combmut);
   }     
   while(1);   /* end combine loop */
}

#ifdef ATL_OMP_THREADS
/*
 * Do a task using thread pool pp; RETURNS: virtual rank (0...nworkers-1)
 */
int ATL_tpool_dojob
(
   ATL_tpool_t *pp,    /* thread pool to use */
   const int rank,     /* actual rank, my tp found at pp->threads+rank */
   const int CFWTHR    /* unused in OpenMP version */
)
{
/*
 * Do work based on PD
 */
   pp->jobf(pp, rank, rank);     /* do the required task */
/*
 * If the calling routine has requested it, perform dynamic combine
 */
   if (pp->combf)
   {
/*
 *    Do combine using virtial ranks stored in pp->icomm
 */
      if (ATL_TPF_DYNCOMB(pp))
         ATL_dyncomb(pp, rank, rank);
      else
         ATL_leftcomb(pp, rank, rank);
   }     /* end combine if */

   return(rank);
}

void *ATL_threadpool(void *vp)
{
   ATL_assert(0);
}
#elif !defined(ATL_TP_FULLPOLL)
/*
 * Do a task using thread pool pp; RETURNS: virtual rank (0...nworkers-1)
 */
int ATL_tpool_dojob
(
   ATL_tpool_t *pp,    /* thread pool to use */
   const int rank,     /* actual rank, my tp found at pp->threads+rank */
   const int CFWTHR    /* 1: worker thr called, 0: ATL_threadpool loop called */
)
{
   const int nworkers=pp->nworkers, nthr = pp->nthr;
   int vrank, iwrk;
   void *tpmut = pp->tpmut;
   ATL_thread_t *tp = pp->threads+rank;
/*
 * Enroll thread in working pool
 */
   if (CFWTHR)
      ATL_mutex_lock(tpmut);
/*
 * On PHI, we must use our actual ranks when we are exploiting contexts
 */
   #if defined(ATL_PHI_SORTED)
   if (nworkers == nthr || nworkers == (nthr>>1) || 
       nworkers == (nthr>>2)+(nthr>>1))
      vrank = rank;
   else
/*
 * For PHI_SORTED2, we always use all contexts, and only context 3 is awoken
 * by the usual process.  Context 3 then awakens contexts 0-2 on its node.
 */
   #elif defined(ATL_PHI_SORTED2)
      if (pp->ncntxts > 1)
      {
         volatile int *chkin = (volatile int*) 
                      (((volatile char *)pd->chkin)+ATL_Cachelen*icore);
/*
 *       context 3 increments vrank count by ncontexts
 */
         if (rank & 3 == 3)
            chkin[3] = -pp->wcnt;
         else
            while (chkin[3] < ATL_NTHREADS);
         vrank = (rank&3) - chkin[3];
      }
      else
   #endif
      vrank = pp->wcnt;
   if (vrank >= nworkers || pp->wcnt >= nworkers) /* If I'm not a worker */
   {
      ATL_mutex_unlock(tpmut);                    /* release mutex and */
      return(vrank);                              /* get out of here now */
   }
   #if defined(ATL_PHI_SORTED2)
      if (pp->ncntxts > 1)
      {
         iwrk = pp->wcnt; 
         pp->wcnt += pp->ncntxts;
      }
      else
   #endif
   iwrk = pp->wcnt++;        /* inc count of participating workers */
   ATL_mutex_unlock(tpmut);  /* let other threads enroll in job */
/*
 * If this was called from threadloop, I may need to wake other workers
 * Only if I'm the first worker to get here (!iwrk) & called from tl (!CFWTHR)
 */
   if (!(CFWTHR+iwrk) && ATL_TPF_ZEROWAKES(pp))
   {
   #ifdef ATL_PHI_SORTED
      if (ATL_TPF_MICSORTED(pp))
      {
         int k, nw=nworkers;
         const int p4 = (pp->nthr >> 2);

         if (nworkers < p4)
            for (k=1; k < p4; k++)
               ATL_cond_signal(pp->wcond);
         else
         {
            ATL_cond_bcast(pp->wcond);
            nw -= p4;
            if (nw < p4)
               while (nw--)
                  ATL_cond_signal(pp->wcond2);
            else
            {
               ATL_cond_bcast(pp->wcond2);
               nw -= p4;
               if (nw < p4)
                  while (nw--)
                     ATL_cond_signal(pp->wcond3);
               else
               {
                  ATL_cond_bcast(pp->wcond3);
                  nw -= p4;
                  if (nw <  p4)
                     while (nw--)
                        ATL_cond_signal(pp->wcond4);
                  else
                     ATL_cond_bcast(pp->wcond4);
               }
            }
         }
      }
      else
   #endif
      if (nthr == nworkers)
         ATL_cond_bcast(pp->wcond);
      else
      {
         int k;
         #ifdef ATL_PHI_SORTED2
         if (pp->ncntxts > 1)
         {
            for (k=nworkers-4; k > 0; k -= 4)
               ATL_cond_signal(pp->wcond);
         }
         else
         #endif
         for (k=1; k < nworkers; k++)
            ATL_cond_signal(pp->wcond);
      }
   }
/*
 * Wake up rest of threads on my core if contexts are called for
 */
   #ifdef ATL_PHI_SORTED2
      if (rank & 3 == 3 && pp->ncntxts > 1)
         ATL_cond_bcast(pp->wconds[icore]);
   #endif
/*
 * Now, do work based on PD; change thread info to use virtual rank info
 */
   tp->P = nworkers;             /* make jobf think only nworkers thrs */
   tp->rank = vrank;             /* and use virtual rank */
   pp->jobf(pp, rank, vrank);    /* do the required task */
/*
 * If the calling routine has requested it, perform dynamic combine
 */
   if (pp->combf)
   {
/*
 *    Do combine using virtial ranks stored in pp->icomm
 */
      if (ATL_TPF_DYNCOMB(pp))
         ATL_dyncomb(pp, rank, vrank);
      else
         ATL_leftcomb(pp, rank, vrank);
   }     /* end combine if */
/*
 * Return thread values to their global values
 */
   tp->rank = rank;
   tp->P = nthr;

   return(vrank);
}
#endif

#if defined(ATL_TP_FULLPOLL) && !defined(ATL_OMP_THREADS)
/*
 * Do a task using thread pool pp; RETURNS: virtual rank (0...ncores*4-1)
 */
int ATL_tpool_dojob
(
   ATL_tpool_t *pp,    /* thread pool to use */
   const int rank,     /* actual rank, my tp found at pp->threads+rank */
   const int CFWTHR    /* 1: worker thr called, 0: ATL_threadpool loop called */
)
{
   ATL_thread_t *tp = pp->threads+rank;
   const int nworkers=pp->nworkers, nthr = pp->nthr;
   #ifdef ATL_PHI_SORTED
      const int mycont = rank / ncores;
      int vrank, iwrk;
   #else
      #define vrank rank
   #endif

/*
 * Map ranks sorted by cores (0...ncores-1, ncores...ncores*2-1,...) to one
 * where all 4 cores are sequential
 */
   
   #if ATL_PHI_SORTED
      if (pp->ncntxts > 1)
      {
         vrank = (coreid<<2) + mycont;
         if (coreid > ncores || mycont > pp->ncntxts)
            return(vrank);
         tp->P = nworkers<<2;          /* make jobf think only nworkers thrs */
         tp->rank = vrank;             /* and use virtual rank */
      }
      else
      {
         if (rank >= nworkers)
            return(rank);
         vrank = rank;
         tp->P = nworkers;
      }
   #else
      if (rank >= nworkers)
         return;
      tp->P = nworkers;
   #endif
/*
 * Now, do work based on PD; change thread info to use virtual rank info
 */
   pp->jobf(pp, rank, vrank);    /* do the required task */
/*
 * If the calling routine has requested it, perform dynamic combine
 */
   if (pp->combf)
   {
/*
 *    Do combine using virtial ranks stored in pp->icomm
 */
      if (ATL_TPF_DYNCOMB(pp))
         ATL_dyncomb(pp, rank, vrank);
      else
         ATL_leftcomb(pp, rank, vrank);
   }     /* end combine if */
/*
 * Return thread values to their global values
 */
   #ifdef ATL_PHI_SORTED
      tp->rank = rank;
   #endif
   tp->P = nthr;

   return(vrank);
}
   #ifndef ATL_PHI_SORTED
      #undef vrank
   #endif
/*
 * This version of threadpool expects master process is bound to a non-used
 * core, and all workers area always polling on volatile variables for whether
 * to work.  Since cache coherence is orders of magnitude faster than cond
 * vars or mutexes, this should always win unless it causes contention with
 * non-ATLAS threads.  For instance, on a 56-core PHI, the last thread to wake
 * up using cond vars (mutexes) is 2835 (87) times slower than waking up
 * with cache coherence!
 */
void *ATL_threadpool(void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const unsigned int rank=tp->rank;
   const unsigned int nthr=pp->nthr; 
   @beginskip
   const unsigned int p4=nthr>>2, p4_2=p4+p4, p4_3=p4_2+p4;
   const unsigned int mycontext = rank / p4;
   const unsigned int mycore = rank % pp->ncores;
   const unsigned int crank = (pp->ncntxts > 1) ? (mycore<<2) + mycontext;
   unsigned int vrank (pp->ncntxts > 1) ? crank : rank;
   void *tpmut = pp->tpmut;
   @endskip

   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER START P=%d\n", rank, pp->nthr);
   #endif
/*
 * Infinite loop over tasks.  Loop has 2 phases:
 * (1) Poll for new work
 * (2) Do work if commanded, including optional combine
 */
   while (1)
   {
/*
 *    Wait for work to be posted.
 */
      pp->chkin[rank] = 3;  /* tell master in polling loop */
      do
      {
         ATL_thread_yield();
      }
      while (pp->chkin[rank] != 1);  /* await signal to do work */
/*
 *    If no job is set, means I should quit
 */
      if (!pp->jobf)
         break;
/*
 *    If there's work and I've got work to do
 */
      if (pp->chkin[rank] == 1)
      {
         pp->chkin[rank] = 2;
         ATL_tpool_dojob(pp, rank, 0);
      }
/*
 *    If no job, scope flag to see if I should sleep, or quit
 */
      else if (ATL_TPF_DIE(pp))
         break;
   }
   pp->chkin[rank] = 4;  /* tell master I'm completely done */
   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER DONE\n", rank);
   #endif
   return(NULL);
}
#elif defined(ATL_POLLTPOOL) && !defined(ATL_OMP_THREADS)
/*
 * In this version of threadpool, worker threads restart master process once
 * job is done, but threads may still be awake and polling for work.  Since
 * the master process is not bound to a thread, this may cause contention
 * with the serial portion of the algorithm (if any), and so cause a slowdown.
 * However, it might be the right thing to do on systems where condition
 * variables are very expensive, or if the number of cores is very large,
 * or if polling does not interfere with the serial process for some reason.
 */
void *ATL_threadpool(void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const unsigned int rank=tp->rank;
   #ifdef ATL_PHI_SORTED
      const unsigned int nthr=pp->nthr, p4=nthr>>2, p4_2=p4+p4, p4_3=p4_2+p4;
   #endif
   void *tpmut = pp->tpmut;
   unsigned int jobID=0;

   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER START P=%d\n", rank, pp->nthr);
   #endif
/*
 * Infinite loop over tasks.  Loop has 3 phases:
 * (1) Do work if commanded, including optional combine
 * (2) Poll for a period of time bounded by ATL_POLLTIME.  During this
 *     period the worker actively pools for work.  If a new job is requested
 *     for time expires, worker gets started on next job w/o th everhead
 *     of going to sleep & waking up.  
 *     If ATL_POLLTIME <= 0, this phase is skipped.
 * (3) Sleep until master wakes up for new work
 * Have lock at top of loop.
 */
   ATL_mutex_lock(tpmut);  /* force wait until pp set up */
   while(1)
   {
      unsigned int vrank;
/*
 *    If I've got work to do
 */
      if (pp->jobf)
      {
         vrank = ATL_tpool_dojob(pp, rank, 0);
         ATL_mutex_lock(tpmut); 
/*
 *       If I'm the last worker, signal master process we're done
 */
         if (vrank < pp->nworkers)
         {
            if (++pp->nwdone == pp->nworkers)
            {
               #ifdef DEBUG
                  fprintf(stderr, "%d,%d: SIGNAL MASTER\n", rank, vrank);
               #endif
               pp->NOWORK = pp->WORKDONE = 1;
               ATL_cond_signal(pp->mcond);
            }
         }
      }
/*
 *    If no job, scope flag to see if I should sleep, or quit
 */
      else
      {
         if (ATL_TPF_DIE(pp))
         {
            ATL_mutex_unlock(tpmut);
            return(NULL);
         }
      }
/*
 *    If we've finished so quickly that the job is still waiting on a
 *    mandatory worker, go back to job and pretend to be a new worker,
 *    as long as we are allowed to work on problem
 */
      #ifndef ATL_PHI_SORTED2
      #ifdef ATL_PHI_SORTED
         if (pp->wcnt < pp->nworkers && pp->nworkers != nthr &&
             pp->nworkers != p4_2 && pp->nworkers != p4_3)
      #else
         if (pp->wcnt < pp->nworkers)
      #endif
            continue;
      #endif
/*
 *    Sleep for next job; nsleep only examined during startup
 */
      pp->nsleep++;  /* don't care if this rolls over */
/*
 *    Poll until time elapses or a new job is available.
 */
      if (ATL_POLLTIME > 0.0 && 
         (jobID == pp->jobID || pp->wcnt >= pp->nworkers))
      {
         double t0, t1=0.0;
         ATL_mutex_unlock(tpmut);
         t0 = ATL_walltime();
         while (t1 < ATL_POLLTIME && 
                (jobID == pp->jobID || pp->wcnt >= pp->nworkers))
         {
            ATL_thread_yield();
            t1 = ATL_walltime() - t0;
         }
         ATL_mutex_lock(tpmut);
      }
/*
 *    If no new job available, go to sleep
 */
      while (pp->jobID == jobID || pp->wcnt >= pp->nworkers)
      {
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: go to sleep\n", rank, vrank);
         #endif
         #ifdef ATL_PHI_SORTED
            ATL_cond_wait(pp->wcond, tpmut);
            if (rank < p4)
               ATL_cond_wait(pp->wcond, tpmut);
            else if (rank < p4_2)
               ATL_cond_wait(pp->wcond2, tpmut);
            else if (rank < p4_3)
               ATL_cond_wait(pp->wcond3, tpmut);
            else
               ATL_cond_wait(pp->wcond4, tpmut);
         #elif defined(ATL_PHI_SORTED2)
            if (rank >= p4_3)
               ATL_cond_wait(pp->wcond, tpmut)
            else if (rank < p4)
               ATL_cond_wait(pp->wconds[rank, tpmut]);
            else if (rank < p4_2)
               ATL_cond_wait(pp->wconds[rank-p4, tpmut]);
            else
               ATL_cond_wait(pp->wconds[rank-p4_2, tpmut]);
         #else
            ATL_cond_wait(pp->wcond, tpmut);
         #endif
      }  /* end sleep loop */
      jobID = pp->jobID;
   }     /* end loop over tasks */
/*
 * When a thread exits, decrement thread count
 */

   ATL_mutex_lock(tpmut);
   pp->nthr--;
   ATL_mutex_unlock(tpmut);
   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER DONE\n", rank);
   #endif
   return(NULL);
}
#elif !defined(ATL_OMP_THREADS)
/*
 * In this version of threadpool, all worker threads go to sleep before
 * control is returned to the master process
 */
void *ATL_threadpool(void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const unsigned int rank=tp->rank;
   #if defined(ATL_PHI_SORTED) || defined(ATL_PHI_SORTED2)
      const unsigned int nthr=pp->nthr, p4=nthr>>2, p4_2=p4+p4, p4_3=p4_2+p4;
      const int icore = rank % (ATL_NTHREADS>>2);
   #endif
   void *tpmut = pp->tpmut;

   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER START POLL=%d, P=%d\n", rank, DOPOLL, P);
   #endif
/*
 * Infinite loop over tasks.  Loop has 2 phases:
 * (1) Do work if commanded, including optional combine
 * (2) Wait for orders - can wait by polling or condition variable
 * Have lock at top of loop.
 */
   ATL_mutex_lock(tpmut);  /* force wait until pp set up */
   while(1)
   {
      unsigned int vrank;
/*
 *    If I've got work to do
 */
      if (pp->jobf)
      {
         vrank = ATL_tpool_dojob(pp, rank, 0);
         ATL_mutex_lock(tpmut);
/*
 *       Threads go to sleep or poll for work after each job
 *       If I'm the last guy, signal master process we're done
 */
         if (vrank < pp->nworkers)
         {
            if (++pp->nwdone == pp->nworkers)
            {
               pp->WORKDONE = 1;
               ATL_cond_signal(pp->mcond);
            }
         }
      }
/*
 *    If no work, scope flag to see if I should sleep, or quit
 */
      else
      {
         if (ATL_TPF_DIE(pp))
         {
            ATL_mutex_unlock(tpmut);
            return(NULL);
         }
      }
/*
 *    If the job is still waiting on workers, go back up and pretend to be
 *    a new worker as long as we're allowed to work on problem
 */
      #ifndef ATL_PHI_SORTED2
      #ifdef ATL_PHI_SORTED
         if (pp->wcnt < pp->nworkers && pp->nworkers != nthr &&
             pp->nworkers != p4_2 && pp->nworkers != p4_3)
      #else
         if (pp->wcnt < pp->nworkers)
      #endif
            continue;
      #endif
/*
 *    Sleep for next job; nsleep only examined during startup
 */
      pp->nsleep++;  /* don't care if this rolls over */
      do
      {
         #ifdef DEBUG
            fprintf(stderr, "%d: go to sleep\n", rank);
         #endif
         #ifdef ATL_PHI_SORTED
            if (rank < p4)
               ATL_cond_wait(pp->wcond, tpmut);
            else if (rank < p4+p4)
               ATL_cond_wait(pp->wcond2, tpmut);
            else if (rank < p4+p4+p4)
               ATL_cond_wait(pp->wcond3, tpmut);
            else
               ATL_cond_wait(pp->wcond4, tpmut);
         #elif defined(ATL_PHI_SORTED2)
            if (rank >= p4_3)
               ATL_cond_wait(pp->wcond, tpmut);
            else
               ATL_cond_wait(pp->wconds[icore], tpmut);
         #else
            ATL_cond_wait(pp->wcond, tpmut);
         #endif
      }
      while (pp->WORKDONE || pp->wcnt >= pp->nworkers);
   }  /* end loop over tasks */
/*
 * When a thread exits, decrement thread count
 */

   ATL_mutex_lock(tpmut);
   pp->nthr--;
   ATL_mutex_unlock(tpmut);
   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER DONE\n", rank);
   #endif
   return(NULL);
}
#endif
@ROUT ATL_threadpool_misc
#define ATL_TP_DECL 1
#include "atlas_taffinity.h"
#include "atlas_threads.h"
#include "atlas_misc.h"
#include "atlas_bitvec.h"

/*
 * Started wt pthread_create on affID==0, this guy will create the
 * rest of the work queue
 */
void *ATL_threadpool_launch(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const int P = pp->nthr;
   int i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
      {
         tp->affID = 1 - tp->affID;
         ATL_setmyaffinity(tp->affID);
      }
   #endif
   #ifndef ATL_TP_FULLPOLL
   if (!tp->rank)
      for (i=1; i < ATL_NTHREADS; i++)
         ATL_thread_start(tp+i, i, 0, ATL_threadpool_launch, tp+i);
   #endif

   ATL_threadpool(tp);
@beginskip
/*
 * When pool finished, await everyone to quit, then free the pool
 */
   while(pp->nthr)
      ATL_thread_yield();
   
   ATL_TP_PTR = NULL;
   ATL_FreeThreadPool(pp);
   if (ATL_TP1_PTR)
   {
      ATL_FreeThreadPool(ATL_TP1_PTR);
      ATL_TP1_PTR = NULL;
   }
@endskip
}
/*
 * This function is called only by one thread, and is unsafe if called
 * by more than one.  It is protected ty ATL_IsFirstThreadedCall, which
 * should return true for only one thread
 */
void ATL_InitThreadPoolStartup(int P, void *pd, void *extra)
{
   ATL_tpool_t *pp;
   int i;

   ATL_assert(!ATL_TP_PTR);


   ATL_TP_PTR = pp = ATL_NewThreadPool(ATL_NTHREADS, 1, pd);
   #if !defined(ATL_TP_FULLPOLL) && !defined(ATL_OMP_THREADS)
      ATL_mutex_lock(pp->tpmut);
   #endif
   pp->nworkers = P;
   pp->PD = pd;
   pp->extra = extra;
   #if defined(ATL_PHI_SORTED) || defined(ATL_PHI_SORTED2)
      ATL_TPF_SET_MICSORTED(pp);
   #endif

   #ifndef ATL_OMP_THREADS
      #ifdef ATL_TP_FULLPOLL
      for (i=1; i < ATL_NTHREADS; i++)
      {
         ATL_thread_start(pp->threads+i, i, 0, ATL_threadpool_launch, 
                          pp->threads+i);
      }
      #else
         ATL_thread_start(pp->threads, 0, 0, ATL_threadpool_launch,pp->threads);
      #endif
   #endif
/*
 * We leave function still holding the lock, so that master can go to sleep
 * awaiting job completion before spawned threads are able to complete
 * and thus mess up timing; ATL_TP_FULLPOLL never gets the lock, and will
 * have to call dojob() manually after allowing threads to proceed 
 */
}

#if defined(ATL_TP_FULLPOLL) || defined(ATL_OMP_THREADS)
/*
 * This version uses cache to communicate with polling cores.
 */
void ATL_goParallel
(
   const unsigned int P0,  /* # of worker threads to use on job */
   void *DoWork,           /* ptr to function doing the job */
   void *DoComb,           /* ptr to func doing combine; NULL: don't combine */
   void *PD,               /* ptr to problem definition (work queue, etc) */
   void *extra             /* extra ptr that is passed to DoWork/DoComb */
)
{
   ATL_tpool_t *pp;
   int i, n, *ip;
   int JUST_STARTED=0;
   unsigned int P;
/*
 * Don't wakeup/create threads if being called for serial execution: just
 * do work on master process to save overhead & ease debugging
 */
   if (P0 < 2)

   {
      if (!ATL_TP1_PTR)
         ATL_TP1_PTR = ATL_NewThreadPool(1, 0, NULL);
      pp = ATL_TP1_PTR;
      pp->nworkers = 1;
      pp->PD = PD;
      pp->extra = extra;
      pp->jobf = DoWork;
      pp->combf = DoComb;
      pp->jobf(pp, 0, 0);
      return;
   }
/*
 * If thread pool not currently active, must spawn it.
 */
   if (!ATL_TP_PTR)
   {
      #ifdef ATL_OMP_THREADS
      #pragma omp single
      #else
      if (ATL_IsFirstThreadedCall())
      #endif
      {
         JUST_STARTED = 1;
         ATL_setmyaffinity(0);
         ATL_InitThreadPoolStartup(Mmin(P0,ATL_NTHREADS), PD, extra);
      }
   }
   pp = ATL_TP_PTR;
   P = Mmin(P0, pp->nthr);
   for (i=0; i < P; i++)
      pp->bchkin[i<<ATL_chksh] = 0;
   pp->PD = PD;
   pp->extra = extra;
   pp->jobf = DoWork;
   pp->combf = DoComb;
   pp->nworkers = P;
/*
 * If I'm doing a combine, setup combine bitvectors based on nworkers
 */
   if (DoComb)
   {
      ATL_UnsetAllBitsBV(pp->combReadyBV);
      if (P == pp->nthr)
         ATL_UnsetAllBitsBV(pp->combDoneBV);
      else if (P >= pp->nthr - P) /* participating threads outnumber idle */
      {
         int i;
         ATL_UnsetAllBitsBV(pp->combDoneBV);
         for (i=P; i < pp->nthr; i++)
            ATL_SetBitBV(pp->combDoneBV, i);
      }
      else /* idle threads outnumber participating */
      {
         int i;
         ATL_SetAllBitsBV(pp->combDoneBV);
         for (i=0; i < P; i++)
            ATL_UnsetBitBV(pp->combDoneBV, i);
      }
   }
#ifdef ATL_OMP_THREADS
   omp_set_num_threads(P);
   #pragma omp parallel
   {
      int r;
      ATL_assert(omp_get_num_threads() == P);
      r = omp_get_thread_num();
      pp->threads[r].rank = r;
      pp->threads[r].P = P;
      #ifdef ATL_PAFF_SELF
         if (JUST_STARTED)
            ATL_setmyaffinity(r);
      #endif
      ATL_tpool_dojob(pp, r, 0);
   }
#else /* !ATL_OMP_THREADS */
/*
 * Tell workers to start working
 */
   for (i=1; i < P; i++)
   {
/*
 *    Wait for thread to get into busy-loop
 */
      while (pp->chkin[i] != 3)
         ATL_thread_yield();
/*
 *    Tell ready thread to start working
 */
      pp->chkin[i] = 1;
   }
   ATL_tpool_dojob(pp, 0, 0);
/*
 * Wait for all threads to complete
 */
   for (i=1; i < P; i++)
   {
      while (pp->chkin[i] != 3)
         ATL_thread_yield();
   }
#endif
}
#else
/*
 * Main way parallel jobs are spawned to workpool
 */
void ATL_goParallel
(
   const unsigned int P0,  /* # of worker threads to use on job */
   void *DoWork,           /* ptr to function doing the job */
   void *DoComb,           /* ptr to func doing combine; NULL: don't combine */
   void *PD,               /* ptr to problem definition (work queue, etc) */
   void *extra             /* extra ptr that is passed to DoWork/DoComb */
)
{
   ATL_tpool_t *pp;
   int n, *ip;
   int JUST_STARTED=0;
   unsigned int P;
/*
 * Don't wakeup/create threads if being called for serial execution: just
 * do work on master process to save overhead & ease debugging
 */
   if (P0 < 2)

   {
      if (!ATL_TP1_PTR)
         ATL_TP1_PTR = ATL_NewThreadPool(1, 0, NULL);
      pp = ATL_TP1_PTR;
      pp->nworkers = 1;
      pp->PD = PD;
      pp->extra = extra;
      pp->jobf = DoWork;
      pp->combf = DoComb;
      pp->jobf(pp, 0, 0);
      return;
   }
/*
 * If thread pool not currently active, must spawn it.
 */
   if (!ATL_TP_PTR)
   {
      #ifdef ATL_OMP_THREADS
      #pragma omp single
      #else
      if (ATL_IsFirstThreadedCall())
      #endif
      {
         JUST_STARTED = 1;
         ATL_InitThreadPoolStartup(Mmin(P0,ATL_NTHREADS), PD, extra);
      }
   }
   pp = ATL_TP_PTR;
   P = Mmin(P0, pp->nthr);

   if (!JUST_STARTED)
   {
      volatile char *bchk=pp->bchkin;
      ATL_mutex_lock(pp->tpmut);
      for (n=0; n < P; n++)         /* put all active thread's CBC */
         bchk[n<<ATL_chksh] = 0;    /* chkin array into coherent state */
      pp->jobID++;
      pp->PD = PD;
      pp->extra = extra;
      pp->jobf = DoWork;
      pp->combf = DoComb;
      pp->nworkers = P;
      pp->nwdone = pp->wcnt = 0;
      pp->WORKDONE = pp->NOWORK = 0;
      #ifdef ATL_TP_FORCEBCAST
         #if (ATL_TP_FORCEBCAST)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         #else
            ATL_TPF_SET_ZEROWAKES(pp);
         #endif
      #else
         if (P == pp->nthr)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         else
            ATL_TPF_SET_ZEROWAKES(pp);
         #endif
/*
 *    Wake threads up to do the new job
 */
      if (ATL_TPF_ZEROWAKES(pp))
         ATL_cond_signal(pp->wcond);
      else
      #ifdef ATL_PHI_SORTED
      {
         ATL_cond_bcast(pp->wcond);
         if (P > ATL_NTHREADS/4)
            ATL_cond_bcast(pp->wcond2);
         if (P > ATL_NTHREADS/2)
            ATL_cond_bcast(pp->wcond3);
         if (P > (3*ATL_NTHREADS)/4)
            ATL_cond_bcast(pp->wcond4);

      }
      #else
         ATL_cond_bcast(pp->wcond);
      #endif
   }
   else /* thread pool just started up, I have tpmut */
   {
      pp->jobf = DoWork;
      pp->combf = DoComb;
   }
/*
 * If I'm doing a combine, setup combine bitvectors based on nworkers
 */
   if (DoComb)
   {
      ATL_UnsetAllBitsBV(pp->combReadyBV);
      if (P == pp->nthr)
         ATL_UnsetAllBitsBV(pp->combDoneBV);
      else if (P >= pp->nthr - P) /* participating threads outnumber idle */
      {
         int i;
         ATL_UnsetAllBitsBV(pp->combDoneBV);
         for (i=P; i < pp->nthr; i++)
            ATL_SetBitBV(pp->combDoneBV, i);
      }
      else /* idle threads outnumber participating */
      {
         int i;
         ATL_SetAllBitsBV(pp->combDoneBV);
         for (i=0; i < P; i++)
            ATL_UnsetBitBV(pp->combDoneBV, i);
      }
   }
/*
 * Threads have started, so this routine awaits completion of task
 */
   do
   {
      #ifdef DEBUG
         fprintf(stderr, "master sleeps WD=%d, P=%d, comb=%p\n",
                 pp->WORKDONE, pp->nworkers, pp->combf);
      #endif
      ATL_cond_wait(pp->mcond, pp->tpmut);
      #ifdef DEBUG
         fprintf(stderr, "master awake  WD=%d\n", pp->WORKDONE);
      #endif
   }
   while (!pp->WORKDONE);
   #ifdef DEBUG
      fprintf(stderr, "MASTER AWAKE FINAL WD=%d\n", pp->WORKDONE);
   #endif
   pp->nworkers = 0;     /* make sure spurious wakeup will go back to sleep */
/*
 * If I'm using a thread pool where all threads are known to be asleep before
 * returning, then on the first launch we must make sure they have all checked
 * in before returning (in subsequent uses of pool, master will not be
 * awakened until all workers have gone to sleep)
 */
   #ifndef ATL_POLLTPOOL
      if (JUST_STARTED && pp->nsleep < pp->nthr)
      {
         do
         {
            ATL_mutex_unlock(pp->tpmut);
            while(pp->nsleep < pp->nthr)
               ATL_thread_yield();
            ATL_mutex_lock(pp->tpmut);
         }
         while(pp->nsleep < pp->nthr);
      }
   #endif
   ATL_mutex_unlock(pp->tpmut);
}
#endif

ATL_tpool_t *ATL_NewThreadPool
(
   const int P,  /* length of thread array to allocate, 0 for don't */
   int ICOM,     /* 0: don't allocate icomm array */
   void *vp      /* what to initialize thread arrays vp to */
)
{
   ATL_tpool_t *pp;
   pp = calloc(1, sizeof(ATL_tpool_t));
   ATL_assert(pp);
   if (P)
   {
      int i;
      pp->nthr = P;
      pp->threads = malloc(P*sizeof(ATL_thread_t));
      ATL_assert(pp->threads);
      if (!vp)
         vp = pp;
      for (i=0; i < P; i++)
      {
         pp->threads[i].rank = i;
         pp->threads[i].P = P;
         pp->threads[i].affID = -i-1;
         pp->threads[i].vp = vp;
      }
   }
   pp->bchkin0 = calloc((P+1)<<ATL_chksh, sizeof(char));
   ATL_assert(pp->bchkin0);
   pp->bchkin = (volatile char*) ATL_chkgap +
                (((size_t)pp->bchkin0) & ~(((size_t)ATL_chkgap)-1));
   #ifdef ATL_TP_FULLPOLL
      pp->chkin = calloc(P, sizeof(short));
      ATL_assert(pp->chkin);
   #elif !defined(ATL_OMP_THREADS) && !defined(ATL_WINTHREADS)
      pp->jobID = 0;
      pp->mcond = ATL_cond_init();
      pp->wcond = ATL_cond_init();
   #endif
   if (ICOM)
   {
      pp->icomm = calloc(P, sizeof(int));
      ATL_assert(pp->icomm);
   }
   pp->combmut = ATL_mutex_init();
   pp->tpmut = ATL_mutex_init();
   pp->cbcmut = ATL_mutex_init();
   #ifdef ATL_PHI_SORTED
      pp->wcond2 = ATL_cond_init();
      pp->wcond3 = ATL_cond_init();
      pp->wcond4 = ATL_cond_init();
   #elif defined(ATL_PHI_SORTED2)
   {
      int i, n;
      pp->ncntxts = 0;
      n = ATL_NTHREADS >> 2;
      pp->wconds = malloc(sizeof(void*)*n);
      ATL_assert(pp->wconds);
      for (i=0; i < n; i++)
         pp->wconds[i] = ATL_cond_init();
   }
   #endif
   pp->combReadyBV = ATL_NewBV(P);
   pp->combDoneBV = ATL_NewBV(P);
   return(pp);
}

void ATL_FreeThreadPool(ATL_tpool_t *pp)
{
   if (!pp)
      return;
   if (pp->bchkin0)
      free((void*)pp->bchkin0);
   if (pp->icomm)
      free((void*)pp->icomm);
   if (pp->threads)
      free(pp->threads);
   if (pp->combmut)
      ATL_mutex_free(pp->combmut);
   if (pp->tpmut)
      ATL_mutex_free(pp->tpmut);
   if (pp->cbcmut)
      ATL_mutex_free(pp->cbcmut);
   #ifdef ATL_TP_FULLPOLL
      free(pp->chkin);
   #elif !defined(ATL_OMP_THREADS) && !defined(ATL_WINTHREADS)
   if (pp->mcond)
      ATL_cond_free(pp->mcond);
   if (pp->wcond)
      ATL_cond_free(pp->wcond);
   #endif
   #if defined(ATL_PHI_SORTED)
      if (pp->mcond2)
         ATL_cond_free(pp->mcond2);
      if (pp->mcond3)
         ATL_cond_free(pp->mcond3);
   #elif defined(ATL_PHI_SORTED2)
   {
      int i, n = pp->nthr>>2;
      for (i=0; i < n; i++)
         ATL_cond_free(pp->mconds[i]);
      free(pp->mconds);
   }
   #endif
   if (pp->combReadyBV)
      ATL_FreeBV(pp->combReadyBV);
   if (pp->combDoneBV)
      ATL_FreeBV(pp->combDoneBV);
   free(pp);
}
@beginskip
/*
 * NOTE:
 *    void *ATL_threadpool_launch(void *vp)
 * is defined in ATL_goparallel.c, so it can use  ATL_setmyaffinity
 */
@beginskip
@ROUT ATL_barrier
#include "atlas_threads.h"
#include "atlas_misc.h"
/*
 * Use cache-based communication to perform a barrier for P threads.
 * This code works on any system with coherent caches (weakly-ordered OK).
 * For weakly-ordered caches, if this barrier is protecting memory, it
 * should be followed by a memory fence to avoid OOE advancing loads.
 */
void ATL_barrier
(
   ATL_CUINT P,     /* # of threads to barrier */
   ATL_CUINT iam    /* rank of calling thread in barrier */
)
{
   volatile char *bchk = ATL_TP_PTR->bchkin;
   ATL_CUINT II = iam<<ATL_chksh;
   const char newv = !bchk[II];

   if (iam)
   {
      bchk[II] = newv;
      while (*bchk != newv);
   }
   else
   {
      int i;
      for (i=1; i < P; i++)
      {
         ATL_CUINT d = i<<ATL_chksh;
         while (bchk[d] != newv);
      }
      *bchk = newv;
   }
}
@ROUT ATL_comb_iamax
#include "atlas_threads.h"
#include "atlas_misc.h"
/*
 * Use cache-based comm to perform a scalar combine for P threads.
 * This code works on any system with coherent caches (weakly-ordered OK)
 * because the data and boolean sync variables are on the same cache line.
 * We guarantee this by separating each region by ATL_chkgap, which should
 * always be >= cache line size (default value 128).  Then, as long as
 * we can fit all data being combined into the same cache line, coherence
 * will guarantee we have the sync boolean and the data regardless of
 * weakly- or strongly-ordered caches.  At least on the ARM, however,
 * we must memory barrier to prevent OOE from advancing loads above the
 * sync.
 */
int Mjoin(PATL,comb_iamax)
(
   ATL_CUINT P,     /* # of threads in combine
   ATL_CUINT iam    /* rank of calling thread in combine */
   ATL_INT idx,     /* index for iamax */
   TYPE *valp       /* input: local max, output: global max */
)
{
   volatile char *bchk = ATL_TP_PTR->bchkin;
   volatile char *mybool = bchk + (iam<<ATL_chksh);
   #if ATL_isize <= ATL_size
      volatile TYPE *myval = (volatile TYPE*)(mybool + ATL_sizeof);
      volatile ATL_INT *myidx = (volatile ATL_INT*)
         (mybool + ATL_sizeof+ATL_sizeof);
   #else /* ints longer than type */
      volatile ATL_INT *myidx = (volatile ATL_INT*)(mybool + ATL_isize);
      volatile TYPE *myval = (volatile TYPE*)(mybool+ATL_isize+ATL_isize);
   #endif
   const TYPE val = *valp;
   TYPE maxval;
   ATL_INT maxidx;
   const char newv = !(*mybool);

   if (iam)
   {
      *myidx = idx;
      *myval = val;
      *mybool = newv;
      while (*bchk != newv);
   }
   else
   {
      int i;
      TYPE aval = (val >= 0) ? val : -val;
      TYPE maxaval = aval;

      maxval = val;
      maxidx = idx;
      for (i=1; i < P; i++)
      {
         ATL_CUINT d = i<<ATL_chksh;
         TYPE hisv;
         #if ATL_isize <= ATL_size
            volatile TYPE *hisval = (volatile TYPE*)(bchk + d + ATL_sizeof);
            volatile ATL_INT *hisidx = (volatile ATL_INT*)
                                       (bchk + d + ATL_sizeof+ATL_sizeof);
         #else               /* ints longer than type */
            volatile ATL_INT *hisidx = (volatile ATL_INT*)(bchk+d+ATL_isize);
            volatile TYPE *hisval = (volatile TYPE*)
                                    (bchk+d+ATL_isize+ATL_isize);
         #endif
         while (bchk[d] != newv);
         hisv = *hival
         aval = (hisv >= 0) ? hisv : -hisv;
         if (aval > maxaval)
         {
            maxaval = aval;
            maxval = hisv;
            maxidx = *hisidx
         }
         else if (aval == maxaval)
         {
            const ATL_INT hidx = *hisidx;
            if (hidx < maxidx)
            {
               maxidx = hidx;
               maxval = hisv;
            }
         }
      }
      *bchk = newv;
   }
}
@ROUT ATL_comb1
#include "atlas_threads.h"
#include "atlas_misc.h"
/*
 * Use cache-based comm to perform a scalar combine for P threads.
 * This code works on any system with coherent caches (weakly-ordered OK)
 * because the data and boolean sync variables are on the same cache line.
 * We guarantee this by separating each region by ATL_chkgap, which should
 * always be >= cache line size (default value 128).  Then, as long as
 * we can fit all data being combined into the same cache line, coherence
 * will guarantee we have the sync boolean and the data regardless of
 * weakly- or strongly-ordered caches.  At least on the ARM, however,
 * we must memory barrier to prevent OOE from advancing loads above the
 * sync.
 */
if defined(COMBMIN)
   #define combvals(v1_, v2_) ((v1_) <= (v2_)) ? (v1_) : (v2_)
Mjoin(PATL,comb_min)
(
   ATL_CUINT P,     /* # of threads in combine
   ATL_CUINT iam    /* rank of calling thread in combine */
   TYPE val         /* local min */
)
#elif defined(COMBMAX)
   #define combvals(v1_, v2_) ((v1_) >= (v2_)) ? (v1_) : (v2_)
Mjoin(PATL,comb_max)
(
   ATL_CUINT P,     /* # of threads in combine
   ATL_CUINT iam    /* rank of calling thread in combine */
   TYPE val         /* local max */
)
#elif defined(COMBSUM)
   #define combvals(v1_, v2_) (v1_) + (v2_)
Mjoin(PATL,comb_sum)
(
   ATL_CUINT P,     /* # of threads in combine
   ATL_CUINT iam    /* rank of calling thread in combine */
   TYPE val         /* local portion of sum */
)
#else
   #error "Unknown combine!"
#endif
{
   volatile char *bchk = ATL_TP_PTR->bchkin;
   volatile char *mybool = bchk + (iam<<ATL_chksh);
   volatile TYPE *myval = (volatile TYPE*)(mybool + ATL_sizeof);
   const char newv = !(*mybool);

   if (iam)
   {
      volatile TYPE *ans = bchk + ATL_sizeof;
      *myval = val;
      *mybool = newv;
      while (*bchk != newv);
      val = *ans;
   }
   else
   {
      int i;
      for (i=1; i < P; i++)
      {
         ATL_CUINT d = i<<ATL_chksh;
         volatile TYPE *hisval = (bchk+d+ATL_sizeof);
         const TYPE hv = *hisval

         while (bchk[d] != newv);  /* wait for his answer to appear */
         val = combval(val, hv);
      }
      *myval = val;  /* provide global answer */
      *bchk = newv;  /* signal answer is ready */
   }
   return(val);
}
@ROUT ATL_tammm_gMNK
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_cbc.h"
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"

TYPE *Mjoin(PATL,ipcopyA)(ipinfo_t *ip, const TYPE *A, size_t i, size_t k,
                          TYPE *w)
{
   ATL_CUINT K = (k != ip->nfkblks) ? ip->kb : ip->kb0;
   ATL_UINT M = (i < ip->nfmblks) ? ip->mb : ip->pmb;
   if (i == ip->nfmblks + ip->npmblks - 1)
      M = ip->mF;
   #ifdef TCPLX
      TYPE *iw, *rw;
      A = IdxA_ip(ip, A, i, k);
      iw = IdxAw_ip(ip, w, i, k);
      rw = iw + ((i < ip->nfmblks) ? ip->szA : ip->pszA);
      ip->a2blk(K, M, ip->alpA, A, ip->lda, rw, iw);
      return(iw);
   #else
      w = IdxAw_ip(ip, w, i, k);
      A = IdxA_ip(ip, A, i, k);
      ip->a2blk(K, M, ip->alpA, A, ip->lda, w);
      return(w);
   #endif
}

TYPE *Mjoin(PATL,ipcopyB)(ipinfo_t *ip, const TYPE *B, size_t k, size_t j,
                          TYPE *w)
{
   ATL_CUINT K = (k != ip->nfkblks) ? ip->kb : ip->kb0;
   ATL_UINT N = (j < ip->nfnblks) ? ip->nb : ip->pnb;
   if (j == ip->nfnblks + ip->npnblks - 1)
      N = ip->nF;
   #ifdef TCPLX
      TYPE *iw, *rw;
      B = IdxB_ip(ip, B, k, j);
      iw = IdxBw_ip(ip, w, k, j);
      rw = iw + ((j < ip->nfnblks) ? ip->szB : ip->pszB);
      ip->b2blk(K, N, ip->alpB, B, ip->ldb, rw, iw);
      return(iw);
   #else
      w = IdxBw_ip(ip, w, k, j);
      B = IdxB_ip(ip, B, k, j);
      ip->b2blk(K, N, ip->alpB, B, ip->ldb, w);
      return(w);
   #endif
}

void Mjoin(PATL,DoWork_amm_gMNK)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_gMNK_t *pd = pp->PD;
   ipinfo_t *ip = pd->ip;
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *pB = pd->wB, *pA = pd->wA; 
   TYPE *pC = pd->wC + vrank*((ip->szC)SHIFT), *C=pd->C;
   ablk2cmat_t blk2c=ip->blk2c;
   #ifdef TCPLX
      TYPE *rC=pC+ip->szC;
      const TYPE *beta=pd->beta;
   #else
      const TYPE beta=pd->beta;
      #define rC pC
   #endif
   ATL_CUINT nByBlks=pd->nByBlks, nByRows=pd->nByRows;
   const size_t nmblks=ip->nfmblks+ip->npmblks, nnblks=ip->nfnblks+ip->npnblks;
   const size_t nkblks = ip->nfkblks + 1;
   const size_t nAblks = pd->nAblks, nBblks = pd->nBblks;
   size_t ctr;
/*
 * First, copy all of B & A
 */
   while ( (ctr = ATL_DecGlobalAtomicCount(pd->asgBctr, vrank)) )
   {
      size_t j, k=nBblks-ctr;
      j = k / nkblks;
      k -= j*nkblks;
      Mjoin(PATL,ipcopyB)(ip, B, k, j, pB);
      #if ATL_CBC_STRONG
         ATL_DecGlobalAtomicCount(pd->donBctr, vrank);
      #endif
   }
   while ( (ctr = ATL_DecGlobalAtomicCount(pd->asgActr, vrank)) )
   {
      size_t i, k=nAblks-ctr;
      i = k / nkblks;
      k -= i*nkblks;
      Mjoin(PATL,ipcopyA)(ip, A, i, k, pA);
      #if ATL_CBC_STRONG
         ATL_DecGlobalAtomicCount(pd->donActr, vrank);
      #endif
   }
/*
 * We now have global A&B copied for everyone's use.  For weakly-ordered caches,
 * we sync all thr to to make sure we can all see each others' copies;
 * Strongly-ordered caches need to hang-fire until A&B copy is complete.
 */
   #if ATL_CBC_STRONG
      while (ATL_GetGlobalAtomicCount(pd->donBctr, vrank))
         ATL_thread_yield();      /* await B cpy finish */
      while (ATL_GetGlobalAtomicCount(pd->donActr, vrank))
         ATL_thread_yield();      /* await A cpy finish */
   #else
      ATL_cbc_barrier(pp->nworkers, vrank, NULL);  /* barrier & memory fence */
   #endif
/*
 * Now loop over rowpans from [0,nByRows-1], wt thread doing entire col,
 * and global A & B copy known to be already in access-major in workspace
 */
   if (nByRows)
   {
      while ( (ctr = ATL_DecGlobalAtomicCount(pd->RowCtr, vrank)) )
      {
         int i = nByRows - ctr, j;
         const TYPE *a;
         TYPE *wa, *c;

         wa = IdxAw_ip(ip, pA, i, 0);
         for (j=0; j < nnblks; j++)
         {
            TYPE *wb, *c;
            wb = IdxBw_ip(ip, pB, 0, j);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, wa, wb, rC, pC,
                                 beta, blk2c);
         }
      }
   }
/*
 * Finally, load balance end of computation by using block-level scheduling
 * for last nByBlks rows
 */
   if (nByBlks)
   {
      while (1)
      {
         ATL_UINT max, i, imax=0;
         TYPE *wa;
/*
 *       Find row with maximum remaining blocks, and work on that one
 */
         max = ATL_GetGlobalAtomicCount(pd->BlkCtrs[0], vrank);
         for (i=1; i < nByBlks; i++)
         {
            int k;
            k = ATL_GetGlobalAtomicCount(pd->BlkCtrs[i], vrank);
            if (k >= max)
            {
               max = k;
               imax = i;
            }
         }
         if (!max)   /* if no blocks are left in any of the nByBlks rowpans */
            break;   /* we are done */
/*
 *       For chosen rowpan, work on individual blks of C with other threads.
 *       This is the last nByBlks rows, so add 1st and nByRows to get glob i
 *       Both the shared A and the global B have been copied at start of alg.
 */
         i = imax + nByRows;
         wa = IdxAw_ip(ip, pA, i, 0);
         while ( (ctr = ATL_DecGlobalAtomicCount(pd->BlkCtrs[imax], vrank)) )
         {
            TYPE *wb, *c;
            int j = nnblks - ctr;
            wb = IdxBw_ip(ip, pB, 0, j);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, wa, wb, rC, pC,
                                 beta, blk2c);
         }
      }
   }
}
#ifndef TCPLX
   #undef rC
#endif

int Mjoin(PATL,tammm_gMNK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   int idx;
   ipinfo_t ip;
   ATL_tamm_gMNK_t pd;
   size_t nmblks, nnblks, nkblks, szB, szA, sz;
   void *vp=NULL;
   ATL_UINT P;

   pd.beta = beta;
   idx = Mjoin(PATL,geGetAmmmIndx)(M, N, K);
   Mjoin(PATL,geComputeIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc, 
                               alpha, beta);
/*
 * Compute how many columns to handle wt block-level and colpan-lvl sheduling
 */
   nmblks = ip.nfmblks + ip.npmblks;
   nnblks = ip.nfnblks + ip.npnblks;
   nkblks = ip.nfkblks + 1;
   pd.nAblks = nmblks * nkblks;
   pd.nBblks = nkblks * nnblks;
   #if 1
      pd.nByBlks = Mmin(32,ATL_NTHREADS-1); /* huge causes too many ctrs */
      pd.nByBlks = Mmin(pd.nByBlks, nmblks);
   #elif 0
      pd.nByBlks = 0;         /* just for testing, bad load balance */
   #else
      pd.nByBlks = nmblks;    /* just for testing, bad parallel overhead */
   #endif
   pd.nByRows = nmblks - pd.nByBlks;
/*
 * Compute max parallelism, and call serial if inadequate
 */
   P = pd.nByRows + pd.nByBlks*nnblks;
   P = Mmin(P, ATL_NTHREADS);
   if (0 && P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
   #if 0
   printf("P=%d, nByRows=%d, nByBlks=%d, nfnblks=%d, nb=%d npnblks=%d, pnb=%d\n", 
          (int)P, (int)pd.nByRows, (int)pd.nByBlks, (int)ip.nfnblks, ip.nb,
          (int)ip.npnblks, ip.pnb);
   #endif
   pd.ip = &ip;
   pd.A = A;
   pd.B = B;
   pd.C = C;
/*
 * Get wrkspc
 */
   szA = (ip.szA*ip.nfmblks + ip.pszA*ip.npmblks)*nkblks;
   szB = (ip.szB*ip.nfnblks + ip.pszB*ip.npnblks)*nkblks;
   sz = (ip.szC SHIFT)*P + (ip.mu<<1)*ip.nu;
   sz = ATL_MulBySize(szA + szB + sz) + 3*ATL_Cachelen;
   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)
      return(1);
   pd.wA = ATL_AlignPtr(vp);
   pd.wB = pd.wA + (szA SHIFT);
   pd.wB = ATL_AlignPtr(pd.wB);
   pd.wC = pd.wB + (szB SHIFT);
   pd.wC = ATL_AlignPtr(pd.wC);

   if (pd.nByRows)
      pd.RowCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nByRows, P), 
                                            pd.nByRows, 0);
   else
      pd.RowCtr = NULL;
   pd.asgActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nAblks,P), pd.nAblks,0);
   pd.asgBctr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nBblks,P), pd.nBblks,0);
   #if ATL_CBC_STRONG
      pd.donActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nAblks,P), 
                                            pd.nAblks, 0);
      pd.donBctr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nBblks,P), 
                                            pd.nBblks, 0);
   #endif
   if (pd.nByBlks)
   {
      int nat, i;
      pd.BlkCtrs = malloc(pd.nByBlks * sizeof(void*));
      ATL_assert(pd.BlkCtrs);
      nat = ATL_EstNctr(nnblks,P);
      for (i=0; i < pd.nByBlks; i++)
         pd.BlkCtrs[i] = ATL_SetGlobalAtomicCount(nat, nnblks, 0);
   }
   else
      pd.BlkCtrs = NULL;
   ATL_goParallel(P, Mjoin(PATL,DoWork_amm_gMNK), NULL, &pd, NULL);

   if (pd.nByBlks)
   {
      int i;
      for (i=0; i < pd.nByBlks; i++)
         ATL_FreeGlobalAtomicCount(pd.BlkCtrs[i]);
      free(pd.BlkCtrs);
   }
   ATL_FreeGlobalAtomicCount(pd.asgActr);
   ATL_FreeGlobalAtomicCount(pd.asgBctr);
   #if ATL_CBC_STRONG
      ATL_FreeGlobalAtomicCount(pd.donActr);
      ATL_FreeGlobalAtomicCount(pd.donBctr);
   #endif
   if (pd.nByRows)
      ATL_FreeGlobalAtomicCount(pd.RowCtr);
   free(vp);
   return(0);
}
@ROUT ATL_tammm_sNK
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"
#include "atlas_cbc.h"

//#undef ATL_CBC_STRONG
//#define ATL_CBC_STRONG 0

void Mjoin(PATL,DoWork_amm_sNK)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_sNK_t *pd = pp->PD;
   ipinfo_t *ip = pd->ip;
   const TYPE *B = pd->B, *A = pd->A;
   TYPE *pB = pd->wB, *pA = pd->w + vrank*pd->wsz, *pC, *C=pd->C;
   ablk2cmat_t blk2c=ip->blk2c;
   #ifdef TCPLX
      TYPE *rC;
      const TYPE *beta=pd->beta;
   #else
      const TYPE beta=pd->beta;
      #define rC pC
   #endif
   ATL_CUINT nByBlks=pd->nByBlks, nByRows=pd->nByRows;
   const size_t nmblks=ip->nfmblks+ip->npmblks, nnblks=ip->nfnblks+ip->npnblks;
   const size_t nkblks = ip->nfkblks + 1, szAp=pd->szAp;
   int ictr;
/*
 * First, copy last nByRows A operands
 */
   pC = pA + szAp;
   pC = ATL_AlignPtr(pC);
   #ifdef TCPLX
      rC = pC + ip->szC;
   #endif
   if (nByBlks)
   {
      while ( (ictr = ATL_DecAtomicCount(pd->begABlksCtr)) )
      {
          size_t i=nmblks-ictr, k;
          ATL_UINT nb, nnu;
          TYPE *wa = pd->wAb+(nByBlks-ictr)*szAp;
          Mjoin(PATL,iploopsK)(ip, i, 0, IdxA_ip(ip, A, i, 0), NULL, NULL, 
                               3, wa, NULL, NULL, NULL, ip->alpA, NULL);
          #if ATL_CBC_STRONG
             ATL_DecAtomicCount(pd->donABlksCtr);
          #endif
      }
   }
/*
 * Now, compute 1st rowpan of C while copying global B;
 * strongly-ordered cachces uses single shared A, while weak redundant A
 */
   ictr = ATL_DecGlobalAtomicCount(pd->begBCtr, vrank);
   if (ictr)
   {
      int ACOPIED = 0;
      int j = nnblks - ictr;
      TYPE *wb, *wa;
      #if ATL_CBC_STRONG
         wa = pd->wAb + nByBlks*szAp;
         if (ATL_DecAtomicCount(pd->begACtr))
         {
             Mjoin(PATL,iploopsK)(ip, 0, 0, A, NULL, NULL, 3, wa, NULL, 
                                  NULL, NULL, ip->alpA, NULL);
            ACOPIED = ATL_DecAtomicCount(pd->donACtr);
            ATL_assert(ACOPIED);
         }
/*
 *       Copy B, then await A done ACK
 */
         wb = IdxBw_ip(ip, pB, 0, j);
         Mjoin(PATL,iploopsK)(ip, 0, j, NULL, IdxB_ip(ip, B, 0, j), NULL, 3,
                              NULL, wb, NULL, NULL, ip->alpB, NULL);
         ATL_DecGlobalAtomicCount(pd->donBCtr, vrank);
         if (!ACOPIED)
         {
            while (ATL_GetAtomicCount(pd->donACtr))  /* await A cpy finish */
               ATL_thread_yield();
         }
/*
 *       Now multiply local(weak)/glob B * local A, and write to my piece of C
 */
         Mjoin(PATL,iploopsK)(ip, 0, j, NULL, NULL, IdxC_ip(ip, C, 0, j), 
                              3, wa, wb, rC, pC, beta, blk2c);
      #else
         wa = pA;
/*
 *       Copy chosen block to correct place in global B workspace & do multiply
 */
         wb = IdxBw_ip(ip, pB, 0, j);
         Mjoin(PATL,iploopsK)(ip, 0, j, A, IdxB_ip(ip, B, 0, j), 
                              IdxC_ip(ip, C, 0, j), 3, wa, wb, rC, pC,
                              beta, blk2c);
      #endif
/*
 *    Now loop over any remaining B blks with known-good wa
 */
      while ( (ictr = ATL_DecGlobalAtomicCount(pd->begBCtr, vrank)) )
      {
         j = nnblks - ictr;
         wb = IdxBw_ip(ip, pB, 0, j);
         #if ATL_CBC_STRONG
            Mjoin(PATL,iploopsK)(ip, 0, j, NULL, IdxB_ip(ip, B, 0, j), NULL, 3, 
                                 NULL, wb, NULL, NULL, ip->alpB, NULL);
            ATL_DecGlobalAtomicCount(pd->donBCtr, vrank);
            Mjoin(PATL,iploopsK)(ip, 0, j, NULL, NULL, IdxC_ip(ip, C, 0, j), 3,
                                 wa, wb, rC, pC, beta, blk2c);
         #else
            Mjoin(PATL,iploopsK)(ip, 0, j, NULL, IdxB_ip(ip, B, 0, j),
                                 IdxC_ip(ip, C, 0, j), 3, wa, wb, rC, pC,
                                 beta, blk2c);
         #endif
      }
   }
/*
 * We now have global B copied for everyone's use.  For weakly-ordered caches,
 * we sync all thr to to make sure we can all see each others' copies;
 * Strongly-ordered caches need to hang-fire until B copy is complete.
 */
   #if ATL_CBC_STRONG
      while (ATL_GetGlobalAtomicCount(pd->donBCtr, vrank))
         ATL_thread_yield();      /* await B cpy finish */
   #else
      ATL_cbc_barrier(pp->nworkers, vrank, NULL);  /* barrier & memory fence */
   #endif
/*
 * Now loop over rowpans from [1,nByRows+1], wt thread doing entire col,
 * and global B copy known to be ready to use.
 */
   if (nByRows)
   {
      while ( (ictr = ATL_DecGlobalAtomicCount(pd->RowCtr, vrank)) )
      {
         int i = nByRows - ictr + 1, j;
         const TYPE *a; 
         TYPE *c, *wb;
/*
 *       For top block, use copied B, and copy this k-panel of A
 */
         a = IdxA_ip(ip, A, i, 0);
         c = IdxC_ip(ip, C, i, 0);
         Mjoin(PATL,iploopsK)(ip, i, 0, a, NULL, c, 3, pA, pB, rC, pC,
                              beta, blk2c);
/*
 *       Remaining blocks don't need to copy A or B
 */
         for (j=1; j < nnblks; j++)
         {
            TYPE *bw, *c;
            bw = IdxBw_ip(ip, pB, 0, j);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, pA, bw, rC, pC,
                                 beta, blk2c);
         }
      }
   }
/*
 * Finally, load balance end of computation by using block-level scheduling
 * for last nByBlks rows
 */
   if (nByBlks)
   {
      #if ATL_CBC_STRONG
         while (ATL_GetAtomicCount(pd->donABlksCtr))  /* await A cpy finish */
            ATL_thread_yield();
      #endif
      while (1)
      {
         ATL_UINT max, i, imax=0;
         TYPE *wa;
/*
 *       Find row with maximum remaining blocks, and work on that one
 */
         max = ATL_GetGlobalAtomicCount(pd->BlkCtrs[0], vrank);
         for (i=1; i < nByBlks; i++)
         {
            int k;
            k = ATL_GetGlobalAtomicCount(pd->BlkCtrs[i], vrank);
            if (k >= max)
            {
               max = k;
               imax = i;
            }
         }
         if (!max)   /* if no blocks are left in any of the nByBlks rowpans */
            break;   /* we are done */
/*
 *       For chosen rowpan, work on individual blks of C with other threads.
 *       This is the last nByBlks rows, so add 1st and nByRows to get glob i
 *       Both the shared A and the global B have been copied at start of alg.
 */
         i = imax + 1 + nByRows;
         wa = pd->wAb + imax*szAp;
         while ( (ictr = ATL_DecGlobalAtomicCount(pd->BlkCtrs[imax], vrank)) )
         {
            TYPE *bw, *c;
            int j = nnblks - ictr;
            bw = IdxBw_ip(ip, pB, 0, j);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, wa, bw, rC, pC,
                                 beta, blk2c);
         }
      }
   }
}
#ifndef TCPLX
   #undef rC
#endif
/*
 * This routine handles the case where K = i*maxKB, and M is reasonably large
 * so that the it can spend most of its time giving out entire rowpanels of C,
 * and only go to block-level syncs for a few rowpans at end.  It requires
 * enough workspace to allocate common B (whole matrix), and P*(szApan+szC).
 * The need to allocate entire B means that N can't be too large either.
 * It is particularly important for recursive panel LU/QR factorization.
 */
int Mjoin(PATL,tammm_sNK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   int idx;
   ipinfo_t ip;
   ATL_tamm_sNK_t snk;
   size_t nmblks, nnblks, nkblks, szBt, szAp, szAb, sz;
   void *vp=NULL;
   ATL_UINT P;

   snk.beta = beta;
   #if 1
   idx = Mjoin(PATL,tGetParCIndx)(&ip, ATL_NTHREADS, M, N, K);
   Mjoin(PATL,geFillInIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc,
                              alpha, beta, 
                              ip.nfmblks, ip.npmblks, ip.mb, ip.pmb, 
                              ip.nfnblks, ip.npnblks, ip.nb, ip.pnb);
   #else
   idx = Mjoin(PATL,geGetAmmmIndx)(M, N, K);
   Mjoin(PATL,geComputeIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc, 
                               alpha, beta);
   #endif
/*
 * Compute how many columns to handle wt block-level and colpan-lvl sheduling
 */
   nmblks = ip.nfmblks + ip.npmblks;
   nnblks = ip.nfnblks + ip.npnblks;
   nkblks = ip.nfkblks + 1;
   #if 1
      snk.nByBlks = ATL_NTHREADS - 1;
      snk.nByBlks = Mmin(snk.nByBlks, 32); /* huge causes too many Ctrs */
      snk.nByBlks = Mmin(snk.nByBlks, nmblks-1);
   #elif 0
      snk.nByBlks = 0;         /* just for testing, bad load balance */
   #else
      snk.nByBlks = nmblks-1;  /* just for testing, bad parallel overhead */
   #endif
   snk.nByRows = nmblks - snk.nByBlks - 1;
/*
 * Compute max parallelism, and call serial if inadequate
 */
   P = snk.nByRows + (snk.nByBlks+1)*nnblks;
   P = Mmin(P, ATL_NTHREADS);
   if (P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
   #if 0
printf("snk:P=%d, M=(%d,%d; %d,%d) N=(%d,%d; %d,%d)\n",
       P, (int)ip.nfmblks, (int)ip.npmblks, ip.mb, ip.pmb,
       (int)ip.nfnblks, (int)ip.npnblks, ip.nb, ip.pnb);
   printf("P=%d, nByRows=%d, nByBlks=%d, nfnblks=%d, nb=%d npnblks=%d, pnb=%d\n", 
          (int)P, (int)snk.nByRows, (int)snk.nByBlks, (int)ip.nfnblks, ip.nb,
          (int)ip.npnblks, ip.pnb);
   #endif
   snk.ip = &ip;
   snk.A = A;
   snk.B = B;
   snk.C = C;
/*
 * Get wrkspc
 */
   szBt = (ip.szB*ip.nfnblks + ip.pszB*ip.npnblks)*(ip.nfkblks+1);
   szAp = (ip.nfmblks) ? ip.szA:ip.pszA;
   szAp *= nkblks;
   snk.szAp = szAp SHIFT;
   #if ATL_CBC_STRONG
      szAb = szAp*(1+snk.nByBlks);
   #else
      szAb = szAp*snk.nByBlks;
   #endif
   snk.wsz = ATL_MulBySize(szAp + ip.szC) + ATL_Cachelen;
   snk.wsz = ATL_MulByCachelen(ATL_DivByCachelen(snk.wsz + ATL_Cachelen-1));
   sz = ATL_MulBySize(szBt+szAb) + P*snk.wsz + 3*ATL_Cachelen;
   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)
      return(1);
   snk.wsz = ATL_DivBySize(snk.wsz)SHIFT;
   snk.wB = ATL_AlignPtr(vp);
   snk.wAb = snk.wB + (szBt SHIFT);
   snk.wAb = ATL_AlignPtr(snk.wAb);
   snk.w = snk.wAb + (szAb SHIFT);
   snk.w = ATL_AlignPtr(snk.w);
   if (snk.nByRows)
      snk.RowCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(snk.nByRows, P), 
                                            snk.nByRows, 0);
   else
      snk.RowCtr = NULL;
   snk.begBCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nnblks,P), nnblks, 0);
   if (snk.nByBlks)
      snk.begABlksCtr = ATL_SetAtomicCount(snk.nByBlks);
   else
      snk.begABlksCtr = NULL;
   #if ATL_CBC_STRONG
      snk.donBCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nnblks,P), nnblks, 0);
      if (snk.nByBlks)
         snk.donABlksCtr = ATL_SetAtomicCount(snk.nByBlks);
      else
         snk.donABlksCtr = NULL;
      snk.begACtr = ATL_SetAtomicCount(1);
      snk.donACtr = ATL_SetAtomicCount(1);
   #endif
   if (snk.nByBlks)
   {
      int nat, i;
      snk.BlkCtrs = malloc(snk.nByBlks * sizeof(void*));
      ATL_assert(snk.BlkCtrs);
      nat = ATL_EstNctr(nnblks,P);
      for (i=0; i < snk.nByBlks; i++)
         snk.BlkCtrs[i] = ATL_SetGlobalAtomicCount(nat, nnblks, 0);
   }
   else
      snk.BlkCtrs = NULL;
   ATL_goParallel(P, Mjoin(PATL,DoWork_amm_sNK), NULL, &snk, NULL);

   if (snk.nByBlks)
   {
      int i;
      for (i=0; i < snk.nByBlks; i++)
         ATL_FreeGlobalAtomicCount(snk.BlkCtrs[i]);
      free(snk.BlkCtrs);
   }
   ATL_FreeGlobalAtomicCount(snk.begBCtr);
   if (snk.nByBlks)
      ATL_FreeAtomicCount(snk.begABlksCtr);
   #if ATL_CBC_STRONG
      ATL_FreeAtomicCount(snk.begACtr);
      ATL_FreeAtomicCount(snk.donACtr);
      ATL_FreeGlobalAtomicCount(snk.donBCtr);
      if (snk.nByBlks)
         ATL_FreeAtomicCount(snk.donABlksCtr);
   #endif
   if (snk.nByRows)
      ATL_FreeGlobalAtomicCount(snk.RowCtr);
   free(vp);
   return(0);
}
@ROUT ATL_tammm_sMK
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"
#include "atlas_cbc.h"

void Mjoin(PATL,DoWork_amm_sMK)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_sMK_t *pd = pp->PD;
   ipinfo_t *ip = pd->ip;
   const TYPE *B = pd->B, *A = pd->A;
   TYPE *pA = pd->wA, *pB = pd->w + vrank*pd->wsz, *pC, *C=pd->C;
   ablk2cmat_t blk2c=ip->blk2c;
   #ifdef TCPLX
      TYPE *rC;
      const TYPE *beta=pd->beta;
   #else
      const TYPE beta=pd->beta;
      #define rC pC
   #endif
   ATL_CUINT nByBlks=pd->nByBlks, nByCols=pd->nByCols;
   const size_t nmblks=ip->nfmblks+ip->npmblks, nnblks=ip->nfnblks+ip->npnblks;
   const size_t nkblks = ip->nfkblks + 1, szBp=pd->szBp;
   int ictr;
/*
 * First, copy last nByCols B operands
 */
   pC = pB + szBp;
   pC = ATL_AlignPtr(pC);
   #ifdef TCPLX
      rC = pC + ip->szC;
   #endif
   if (nByBlks)
   {
@skip      ATL_UINT szB=((ip->szB)SHIFT);
      while ( (ictr = ATL_DecAtomicCount(pd->begBBlksCtr)) )
      {
          size_t j=nnblks-ictr, k;
          ATL_UINT nb, nnu;
          TYPE *b = pd->wBb+(nByBlks-ictr)*szBp;
          Mjoin(PATL,iploopsK)(ip, 0, j, NULL, IdxB_ip(ip, B, 0, j), NULL, 
                               2, NULL, b, NULL, NULL, ip->alpB, NULL);
          #if ATL_CBC_STRONG
             ATL_DecAtomicCount(pd->donBBlksCtr);
          #endif
      }
   }
/*
 * Now, compute 1st colpan of C while copying global A;
 * strongly-ordered cachces uses shared B, while weak duplicate B
 */
   ictr = ATL_DecGlobalAtomicCount(pd->begACtr, vrank);
   if (ictr)
   {
      int BCOPIED = 0;
      int i = nmblks - ictr;
      TYPE *b, *a;
      #if ATL_CBC_STRONG
         b = pd->wBb + nByBlks*szBp;
         if (ATL_DecAtomicCount(pd->begBCtr))
         {
             Mjoin(PATL,iploopsK)(ip, 0, 0, NULL, B, NULL, 2, NULL, b, 
                                  NULL, NULL, ip->alpB, NULL);
            BCOPIED = ATL_DecAtomicCount(pd->donBCtr);
            ATL_assert(BCOPIED);
         }
/*
 *       Copy A, then await B done ACK
 */
         a = IdxAw_ip(ip, pA, i, 0);
         Mjoin(PATL,iploopsK)(ip, i, 0, IdxA_ip(ip, A, i, 0), NULL, NULL, 3,
                              a, NULL, NULL, NULL, ip->alpA, NULL);
         ATL_DecGlobalAtomicCount(pd->donACtr, vrank);
         if (!BCOPIED)
         {
            while (ATL_GetAtomicCount(pd->donBCtr))  /* await B cpy finish */
               ATL_thread_yield();
         }
/*
 *       Now multiply local(weak)/glob B * local A, and write to my piece of C
 */
         Mjoin(PATL,iploopsK)(ip, i, 0, NULL, NULL, IdxC_ip(ip, C, i, 0), 
                              3, a, b, rC, pC, beta, blk2c);
      #else
         b = pB;
/*
 *       Copy chosen block to correct place in global A workspace & do multiply
 */
         a = IdxAw_ip(ip, pA, i, 0);
         Mjoin(PATL,iploopsK)(ip, i, 0, IdxA_ip(ip, A, i, 0), B, 
                              IdxC_ip(ip, C, i, 0), 3, a, b, rC, pC,
                              beta, blk2c);
      #endif
/*
 *    Now loop over any remaining A blks with known-good b
 */
      while ( (ictr = ATL_DecGlobalAtomicCount(pd->begACtr, vrank)) )
      {
         i = nmblks - ictr;
         a = IdxAw_ip(ip, pA, i, 0);
         #if ATL_CBC_STRONG
            Mjoin(PATL,iploopsK)(ip, i, 0, IdxA_ip(ip, A, i, 0), NULL, NULL, 3, 
                                 a, NULL, NULL, NULL, ip->alpA, NULL);
            ATL_DecGlobalAtomicCount(pd->donACtr, vrank);
            Mjoin(PATL,iploopsK)(ip, i, 0, NULL, NULL, IdxC_ip(ip, C, i, 0), 3,
                                 a, b, rC, pC, beta, blk2c);
         #else
            Mjoin(PATL,iploopsK)(ip, i, 0, IdxA_ip(ip, A, i, 0), NULL, 
                                 IdxC_ip(ip, C, i, 0), 3, a, b, rC, pC,
                                 beta, blk2c);
         #endif
      }
   }
/*
 * We now have global A copied for everyone's use.  For weakly-ordered caches,
 * we sync all thr to to make sure we can all see each others' copies;
 * Strongly-ordered caches need to hang-fire until A copy is complete.
 */
   #if ATL_CBC_STRONG
      while (ATL_GetGlobalAtomicCount(pd->donACtr, vrank))
         ATL_thread_yield();      /* await A cpy finish */
   #else
      ATL_cbc_barrier(pp->nworkers, vrank, NULL);  /* barrier & memory fence */
   #endif
/*
 * Now loop over colpans from [1,nByCols+1], wt thread doing entire col,
 * and global A copy known to be ready to use.
 */
   if (nByCols)
   {
      ATL_CUINT nfnblks = ip->nfnblks;
      while ( (ictr = ATL_DecGlobalAtomicCount(pd->ColCtr, vrank)) )
      {
         int i, j = nByCols - ictr + 1;
         const TYPE *b; 
         TYPE *c, *wa;
/*
 *       For top block, use copied A, and copy this k-panel of B
 */
         b = IdxB_ip(ip, B, 0, j);
         c = IdxC_ip(ip, C, 0, j);
         Mjoin(PATL,iploopsK)(ip, 0, j, NULL, b, c, 3, pA, pB, rC, pC,
                              beta, blk2c);
/*
 *       Remaining blocks don't need to copy A or B
 */
         for (i=1; i < nmblks; i++)
         {
            TYPE *aw, *c;
            aw = IdxAw_ip(ip, pA, i, 0);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, aw, pB, rC, pC,
                                 beta, blk2c);
         }
      }
   }
/*
 * Finally, load balance end of computation by using block-level scheduling
 * for last nByBlks cols
 *
 * Currently, I have this designed to take any block in the last nByBlks cols.
 * However, this would result in a given thread moving pB unnecessarily.
 * A better idea is probably to nByBlks-len array of nmblks ctrs, and guys
 * can start at col+rank to spread around usage.  This will allow us to reuse
 * a given pB maximally.
 */
   if (nByBlks)
   {
      #if ATL_CBC_STRONG
         while (ATL_GetAtomicCount(pd->donBBlksCtr))  /* await B cpy finish */
            ATL_thread_yield();
      #endif
      while (1)
      {
         ATL_UINT max, j, jmax=0;
         TYPE *wb;
/*
 *       Find column with maximum remaining blocks, and work on that one
 */
         max = ATL_GetGlobalAtomicCount(pd->BlkCtrs[0], vrank);
         for (j=1; j < nByBlks; j++)
         {
            int k;
            k = ATL_GetGlobalAtomicCount(pd->BlkCtrs[j], vrank);
            if (k >= max)
            {
               max = k;
               jmax = j;
            }
         }
         if (!max)   /* if no blocks are left in any of the nByBlks colpans */
            break;   /* we are done */
/*
 *       For chosen colpan, work on individual blks of C with other threads.
 *       This is the last nByBlks columns, so add 1st and nByCols to get glob j
 *       Both the shared B and the global A have been copied at start of alg.
 */
         j = jmax + 1 + nByCols;
         wb = pd->wBb + jmax*szBp;
         while ( (ictr = ATL_DecGlobalAtomicCount(pd->BlkCtrs[jmax], vrank)) )
         {
            TYPE *aw, *c;
            int i = nmblks - ictr;
            aw = IdxAw_ip(ip, pA, i, 0);
            c = IdxC_ip(ip, C, i, j);
            Mjoin(PATL,iploopsK)(ip, i, j, NULL, NULL, c, 3, aw, wb, rC, pC,
                                 beta, blk2c);
         }
      }
   }
}
#ifndef TCPLX
   #undef rC
#endif
/*
 * This routine handles the case where K = i*maxKB, and N is reasonably large
 * so that the it can spend most of its time giving out entire colpans of C,
 * and only go to block-level syncs for a few colpans at end.  It requires
 * enough workspace to allocate common A (whole matrix), and P*(szB+szC).
 * The need to allocate entire A means that M can't be too large either.
 * It is particularly important for the statically blocked right-looking LU
 * or QR factorizations.
 */
int Mjoin(PATL,tammm_sMK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{
   int idx;
   ipinfo_t ip;
   ATL_tamm_sMK_t smk;
   size_t nmblks, nnblks, nkblks, szAt, szBp, szBb, sz;
   void *vp=NULL;
   ATL_UINT P;

   smk.beta = beta;
#if 1
   idx = Mjoin(PATL,tGetParCIndx)(&ip, ATL_NTHREADS, M, N, K);
   Mjoin(PATL,geFillInIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc,
                              alpha, beta, 
                              ip.nfmblks, ip.npmblks, ip.mb, ip.pmb, 
                              ip.nfnblks, ip.npnblks, ip.nb, ip.pnb);
#else
   idx = Mjoin(PATL,geGetAmmmIndx)(M, N, K);
   Mjoin(PATL,geComputeIPInfo)(&ip, idx, TA, TB, M, N, K, lda, ldb, ldc, 
                               alpha, beta);
#endif
/*
 * Compute how many columns to handle wt block-level and colpan-lvl sheduling
 */
   nmblks = ip.nfmblks + ip.npmblks;
   nnblks = ip.nfnblks + ip.npnblks;
   nkblks = ip.nfkblks + 1;
   #if 1
      smk.nByBlks = ATL_NTHREADS - 1;
      smk.nByBlks = Mmin(smk.nByBlks, 32); /* huge causes too many Ctrs */
      smk.nByBlks = Mmin(smk.nByBlks, nnblks-1);
   #elif 1
      smk.nByBlks = 0;         /* just for testing, bad load balance */
   #else
      smk.nByBlks = nnblks-1;  /* just for testing, bad parallel overhead */
   #endif
   smk.nByCols = nnblks - smk.nByBlks - 1;
/*
 * Compute max parallelism, and call serial if inadequate
 */
   P = smk.nByCols + (smk.nByBlks+1)*nmblks;
   P = Mmin(P, ATL_NTHREADS);
   if (P < 2)
   {
      Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
      return(0);
   }
#if 0
printf("smk:P=%d, M=(%d,%d; %d,%d) N=(%d,%d; %d,%d)\n",
       P, (int)ip.nfmblks, (int)ip.npmblks, ip.mb, ip.pmb,
       (int)ip.nfnblks, (int)ip.npnblks, ip.nb, ip.pnb);
#endif
   smk.ip = &ip;
   smk.A = A;
   smk.B = B;
   smk.C = C;
/*
 * Get wrkspc
 */
   szAt = (ip.szA*ip.nfmblks + ip.pszA*ip.npmblks)*(ip.nfkblks+1);
   szBp = (ip.nfnblks) ? ip.szB:ip.pszB;
   szBp *= nkblks;
   smk.szBp = szBp SHIFT;
   #if ATL_CBC_STRONG
      szBb = szBp*(1+smk.nByBlks);
   #else
      szBb = szBp*smk.nByBlks;
   #endif
   smk.wsz = ATL_MulBySize(szBp + ip.szC) + ATL_Cachelen;
   smk.wsz = ATL_MulByCachelen(ATL_DivByCachelen(smk.wsz + ATL_Cachelen-1));
   sz = ATL_MulBySize(szAt+szBb) + P*smk.wsz + 3*ATL_Cachelen;
   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)
      return(1);
   smk.wsz = ATL_DivBySize(smk.wsz)SHIFT;
   smk.wA = ATL_AlignPtr(vp);
   smk.wBb = smk.wA + (szAt SHIFT);
   smk.wBb = ATL_AlignPtr(smk.wBb);
   smk.w = smk.wBb + (szBb SHIFT);
   smk.w = ATL_AlignPtr(smk.w);
   if (smk.nByCols)
      smk.ColCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(smk.nByCols, P), 
                                            smk.nByCols, 0);
   else
      smk.ColCtr = NULL;
   smk.begACtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P), nmblks, 0);
   if (smk.nByBlks)
      smk.begBBlksCtr = ATL_SetAtomicCount(smk.nByBlks);
   else
      smk.begBBlksCtr = NULL;
   #if ATL_CBC_STRONG
      smk.donACtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P), nmblks, 0);
      if (smk.nByBlks)
         smk.donBBlksCtr = ATL_SetAtomicCount(smk.nByBlks);
      else
         smk.donBBlksCtr = NULL;
      smk.begBCtr = ATL_SetAtomicCount(1);
      smk.donBCtr = ATL_SetAtomicCount(1);
   #endif
   if (smk.nByBlks)
   {
      int nat, i;
      smk.BlkCtrs = malloc(smk.nByBlks * sizeof(void*));
      ATL_assert(smk.BlkCtrs);
      nat = ATL_EstNctr(nmblks,P);
      for (i=0; i < smk.nByBlks; i++)
         smk.BlkCtrs[i] = ATL_SetGlobalAtomicCount(nat, nmblks, 0);
   }
   else
      smk.BlkCtrs = NULL;
   ATL_goParallel(P, Mjoin(PATL,DoWork_amm_sMK), NULL, &smk, NULL);

   if (smk.nByBlks)
   {
      int i;
      for (i=0; i < smk.nByBlks; i++)
         ATL_FreeGlobalAtomicCount(smk.BlkCtrs[i]);
      free(smk.BlkCtrs);
   }
   ATL_FreeGlobalAtomicCount(smk.begACtr);
   if (smk.nByBlks)
      ATL_FreeAtomicCount(smk.begBBlksCtr);
   #if ATL_CBC_STRONG
      ATL_FreeAtomicCount(smk.begBCtr);
      ATL_FreeAtomicCount(smk.donBCtr);
      ATL_FreeGlobalAtomicCount(smk.donACtr);
      if (smk.nByBlks)
         ATL_FreeAtomicCount(smk.donBBlksCtr);
   #endif
   if (smk.nByCols)
      ATL_FreeGlobalAtomicCount(smk.ColCtr);
   free(vp);
   return(0);
}
@ROUT _ATL_tammm_gK
static int ATL_loopK
(
   ipinfo_t *ip,
   size_t K,
   size_t Kp,
   const TYPE *A,
   const TYPE *B,
   TYPE *C,
   TYPE *w,
   size_t szAw,
   size_t wsz
)
{
   ATL_tamm_gK_t s0, s1, *sp;
   size_t nmblks, nnblks, nCblks, KK;
   ipinfo last;
   TYPE *wA0, *wA1;

   wA0 = ATL_AlignPtr(w);
   wA1 = wA0 +

   s1.ip = s0.ip = ip;
   s1.A = s0.A = A;
   s1.B = s0.B = B;
   s1.C = s0.C = C;
   nmblks = ip->nfmblks + ip->npmblks;
   nnblks = ip->nfnblks + ip->npnblks;
   nCblks = nmblks * nnblks;
   s1.betaBV = s0.betaBV = ATL_NewBV(nCblks);
   s1.Cmut = s0.Cmut = ATL_mutex_init();
   s1.wsz = s0.wsz = wsz;
   s1.RDY = s0.RDY = 0;

   s0.wA = ATL_AlignPtr(w);
   s1.wA = s0.wA + szAw;
   s1.wA = ATL_AlignPtr(s1.wA);
   s0.w = s1.wA + szAw;
   s1.w = s0.w = ATL_AlignPtr(s0.w);

   s0.done = ATL_SetAtomicCount(P);
   s0.ColCtr = ATL_SetGlobalAtomicCount(Mmin((P>>1),16), nnblks, 0);
   s0.begActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P), nmblks, 0);
   #if ATL_CBC_STRONG
      s0.endActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P), nmblks, 0);
   #endif
   if (K > Kp)
   {
      s1.done = ATL_SetAtomicCount(P);
      s1.ColCtr = ATL_SetGlobalAtomicCount(Mmin((P>>1),16), nnblks, 0);
      s1.begActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P), nmblks, 0);
      #if ATL_CBC_STRONG
         s1.endActr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks,P),nmblks,0);
      #endif
      KK = (K/Kp)*Kp;
   }
   else
      KK = K;
   sp = &s0;
   for (k=0; k < KK; k += Kp)
   {
      
      sp = (sp == &s0) ? &s1 : s0;
/*
 *    Now, must wait new sp being done, then somebody must claim it (need new
 *    Ctr or mutex), then get it ready.  I think we can remove the RDY var.
 */
   }
}
static int ATL_recur_gK
(
   ipinfo_t *ip,
   ATL_UINT nKPblks,
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   size_t int Kp = Mmin(K, nKPblks*ATL_geAMM_LASTKB);
   size_t sz, szAw, wsz;
   void *vp=NULL;
   ATL_UINT P;

   idx = Mjoin(PATL,geGetAmmmIndx)(M, N, Kp);
   Mjoin(PATL,geComputeIPInfo)(ip, idx, TA, TB, M, N, Kp, lda, ldb, ldc,
                               alpha, beta);

   if (Kp <= K)                   /* later on, specialize above calls */
      assert(ip->kb0 == ip->kb);  /* to make this true, for now assert */

   P = ip->nfnblks + ip->npnblks;
   P = Mmin(ATL_NTHREADS, P);
   wsz = P*(ATL_MulBySize(szB+szC)+ATL_Cachelen);
   wsz = ATL_MulByCachelen(ATL_DivByCachelen(wsz + ATL_Cachelen-1));
   wsz = ATL_DivBySize(wsz);
   szAw = (ip->nkblks+1) * (ip->nfnblks*ip->szA+ip->npnblks*ip->pszA);
   sz = ATL_MulBySize(szAw + szAw + wsz + (ip->mu<<1)*ip->nu);
   if (sz <= ATL_PTMAXMALLOC)
      vp = malloc(sz);
   if (!vp)
      return(1);
   #ifdef TCPLX
      szAw += szAw;
      wsz  += wsz;
   #endif
   ATL_loopK(ip, K, Kp, A, B, C, vp, szAw, wsz);
}
/*
 * This routine used for full GEMM built from a series of rank-Kp updates,
 * where Kp is some multiple of MAXKB.  It deals out work by C column panel,
 * and so it expects nnblks >= P.  It copies all of A up front so that all
 * working threads can use the shared A wrkspc, and therefore requires:
 *   wrkspc = 2*nkblks*nnblks*szA + P(szB+szC)
 * We'll reduce K by Kp, and N by recursion to meet this wrkspc limit.
 */
int Mjoin(PATL,tammm_gK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   size_t M,
   size_t N,
   size_t K,
   const SCALAR alpha,
   const TYPE *A,
   size_t lda,
   const TYPE *B,
   size_t ldb,
   const SCALAR beta,
   TYPE *C,
   size_t ldc
)
{

   return(ATL_recur_gK(ip, A, B, C));
}
@ROUT ATL_tsyrk_tN
#define ATL_GLOBIDX 1
#include "atlas_misc.h"
#include Mstr(Mjoin(ATLAS_PRE,sysinfo.h))
#include Mstr(Mjoin(ATLAS_PRE,amm_sum.h))
#include Mstr(Mjoin(ATLAS_PRE,sqamm_perf.h))
#include Mstr(Mjoin(ATLAS_PRE,amm_syrk.h))
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"


static void DoWork(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tsyrk_tN_t *pd = pp->PD;
   const TYPE *A = pd->A;
   #ifdef TCPLX
      TYPE *iA = pd->w + vrank*pd->szW, *rA=iA+pd->szA, *iC=rA+pd->szA, *rC, *a;
      #ifdef Conj_
         TYPE *crA, *ciA;
      #endif
      ammkern_t syrkK_bn = Mjoin(PATL,amsyrkK_b0); 
      ammkern_t syrkK_b1 = Mjoin(PATL,amsyrkK_b0);
      #define one pd->ONE
   #else
      TYPE *wA = pd->w + vrank*pd->szW, *wC = wA + pd->szA;
      ammkern_t syrkK = Mjoin(PATL,amsyrkK_b0);
      #define one ATL_rone
   #endif
   cm2am_t a2blk = pd->a2blk;
   const size_t njobs=pd->njobs, lda=pd->lda, nK1=pd->nK1, incAk=pd->incAk;
   ATL_CUINT kb=pd->kb, N=pd->N, nnu=pd->nnu;

   #ifdef TCPLX
      iC = ATL_AlignPtr(iC);
      rC = iC + pd->szC;
      #ifdef Conj_
         crA = (pd->NoTrans) ? rA : iA;
         ciA = (pd->NoTrans) ? iA : rA;
      #endif
   #else
      wC = ATL_AlignPtr(wC);
   #endif
/*
 * First do the big chunky jobs
 */
   if (njobs)
   {
      void *jobCtr = pd->jobCtr;
      size_t Jctr, njobs=pd->njobs;
      ATL_CUINT jobshift=pd->jobshift;
      while ( (Jctr = ATL_DecGlobalAtomicCount(jobCtr, vrank)) )
      {
         size_t k = (njobs-Jctr)<<jobshift;
         const size_t kend = k + (1<<jobshift);
         do
         {
            #ifdef TCPLX
               a2blk(kb, N, one, A+incAk*k, lda, rA, iA);
               #ifdef Conj_
                  syrkK_b1(nnu, nnu, kb, iA, iA, rC, crA, ciA, iC);
                  syrkK_bn(nnu, nnu, kb, crA, ciA, iC, rA, rA, rC);
                  Mjoin(PATL,amsyrkK_b1)(nnu, nnu, kb, rA, rA, rC, ciA,crA,iC);
                  Mjoin(PATL,amsyrkK_bn)(nnu, nnu, kb, ciA, crA, iC, iA,iA,rC);
               #else
                  syrkK_bn(nnu, nnu, kb, iA, iA, rC, rA, iA, iC);
                  syrkK_b1(nnu, nnu, kb, rA, iA, iC, rA, rA, rC);
                  Mjoin(PATL,amsyrkK_bn)(nnu, nnu, kb, rA, rA, rC, iA, rA, iC);
                  Mjoin(PATL,amsyrkK_b1)(nnu, nnu, kb, iA, rA, iC, iA, iA, rC);
               #endif
               syrkK_bn = Mjoin(PATL,amsyrkK_bn);
               syrkK_b1 = Mjoin(PATL,amsyrkK_b1);
            #else
               a2blk(kb, N, ATL_rone, A+incAk*k, lda, wA);
               syrkK(nnu, nnu, kb, wA, wA, wC, wA, wA, wC);
               syrkK = Mjoin(PATL,amsyrkK_b1);
            #endif
         }
         while (++k != kend);
      }
   }
/*
 * Now loop over individual full-kb computations
 */
   if (nK1)
   {
      const size_t k0 = (njobs << pd->jobshift);  /* # of k blks already done */
      size_t k, Kctr;
      void *K1ctr=pd->K1ctr;
      while ( (Kctr = ATL_DecGlobalAtomicCount(K1ctr, vrank)) )
      {
         k = k0 + nK1 - Kctr;
         #ifdef TCPLX
            a2blk(kb, N, one, A+incAk*k, lda, rA, iA);
            #ifdef Conj_
               syrkK_b1(nnu, nnu, kb, iA, iA, rC, crA, ciA, iC);
               syrkK_bn(nnu, nnu, kb, crA, ciA, iC, rA, rA, rC);
               Mjoin(PATL,amsyrkK_b1)(nnu, nnu, kb, rA, rA, rC, ciA, crA, iC);
               Mjoin(PATL,amsyrkK_bn)(nnu, nnu, kb, ciA, crA, iC, iA, iA, rC);
            #else
               syrkK_bn(nnu, nnu, kb, iA, iA, rC, rA, iA, iC);
               syrkK_b1(nnu, nnu, kb, rA, iA, iC, rA, rA, rC);
               Mjoin(PATL,amsyrkK_bn)(nnu, nnu, kb, rA, rA, rC, iA, rA, iC);
               Mjoin(PATL,amsyrkK_b1)(nnu, nnu, kb, iA, rA, iC, iA, iA, rC);
            #endif
            syrkK_bn = Mjoin(PATL,amsyrkK_bn);
            syrkK_b1 = Mjoin(PATL,amsyrkK_b1);
         #else
            a2blk(kb, N, ATL_rone, A+incAk*k, lda, wA);
            syrkK(nnu, nnu, kb, wA, wA, wC, wA, wA, wC);
            syrkK = Mjoin(PATL,amsyrkK_b1);
         #endif
      }
   }
/*
 * If last kb is partial, only one guy does that
 */
   if (pd->kb0)
   {
      if (ATL_DecAtomicCount(pd->KB0ctr))
      {
         size_t k = (njobs<<pd->jobshift) + nK1;
         #ifdef TCPLX
            ATL_CUINT KB=pd->KB0;
            a2blk(pd->kb0, N, one, A+incAk*k, lda, rA, iA);
            #ifdef Conj_
               syrkK_b1(nnu, nnu, KB, iA, iA, rC, crA, ciA, iC);
               syrkK_bn(nnu, nnu, KB, crA, ciA, iC, rA, rA, rC);
               Mjoin(PATL,amsyrkK_b1)(nnu, nnu, KB, rA, rA, rC, ciA, crA, iC);
               Mjoin(PATL,amsyrkK_bn)(nnu, nnu, KB, ciA, crA, iC, iA, iA, rC);
            #else
               syrkK_bn(nnu, nnu, KB, iA, iA, rC, rA, iA, iC);
               syrkK_b1(nnu, nnu, KB, rA, iA, iC, rA, rA, rC);
               Mjoin(PATL,amsyrkK_bn)(nnu, nnu, KB, rA, rA, rC, iA, rA, iC);
               Mjoin(PATL,amsyrkK_b1)(nnu, nnu, KB, iA, rA, iC, iA, iA, rC);
               syrkK_b1 = NULL;
            #endif
         #else
            a2blk(pd->kb0, N, ATL_rone, A+incAk*k, lda, wA);
            syrkK(nnu, nnu, pd->KB0, wA, wA, wC, wA, wA, wC);
            syrkK = NULL;
         #endif
      }
   }
/*
 * Write out if we did any work
 */
#ifdef TCPLX
   if (syrkK_b1 != Mjoin(PATL,amsyrkK_b0)) /* we did at least 1 blk */
#else
   if (syrkK != Mjoin(PATL,amsyrkK_b0))
#endif
   {
      if (pd->wT)  /* Upper triangle must transpose in workspace */
      {
         ablk2cmat_t blk2c=pd->blk2c_b1;
         TYPE *wc = pd->wT + (vrank SHIFT) * ((size_t)N)*N, *C=pd->C;
         ATL_CSZT ldc=pd->ldc;
         ATL_UINT k, APPBETA=0;
   
         if (blk2c == Mjoin(PATL,syblk2cmat_a1_b1))
         blk2c = Mjoin(PATL,syblk2cmat_a1_b0);
         else 
            blk2c = (blk2c == Mjoin(PATL,syblk2cmat_an_b1)) ? 
               Mjoin(PATL,syblk2cmat_an_b0) : Mjoin(PATL,syblk2cmat_aX_b0);
         #ifdef TCPLX
         #else
            blk2c(N, N, pd->alpha, wC, ATL_rzero, wc, N);
         #endif
         ATL_mutex_lock(pd->Cmut);
         if (pd->appBeta)  /* only set if beta != 1 & we are first here */
         {
            const SCALAR beta = pd->beta;
            if (SCALAR_IS_ZERO(pd->beta))
               for (k=0; k < N; k++, wc += (1 SHIFT), C += (ldc SHIFT))
                   Mjoin(PATL,copy)(k+1, wc, N, C, 1);
            else
               for (k=0; k < N; k++, wc += (1 SHIFT), C += (ldc SHIFT))
                  Mjoin(PATL,axpby)(k+1, one, wc, N, beta, C, 1);
            pd->appBeta = 0;
         }
         else
            for (k=0; k < N; k++, wc += (1 SHIFT), C += (ldc SHIFT))
               Mjoin(PATL,axpby)(k+1, one, wc, N, one, C, 1);
         ATL_mutex_unlock(pd->Cmut);
      }
      else   /* Lower triangle can just write to C */
      {
         #ifdef Conj_
            ATL_UINT ZeroDiag=0;
         #endif
         ATL_mutex_lock(pd->Cmut);
         if (pd->appBeta)
         {
            pd->appBeta = 0;
            #ifdef TCPLX
               pd->blk2c(N, N, pd->alpha, rC, iC, pd->beta, pd->C, pd->ldc);
            #else
               pd->blk2c(N, N, pd->alpha, wC, pd->beta, pd->C, pd->ldc);
            #endif
         }
         else
         {
            #ifdef TCPLX
               pd->blk2c_b1(N, N, pd->alpha, rC, iC, pd->beta, pd->C, pd->ldc);
            #else
               pd->blk2c_b1(N, N, pd->alpha, wC, pd->beta, pd->C, pd->ldc);
            #endif
         }
         #ifdef Conj_
            if (!(--(pd->Pcnt)))
            #ifdef Conj_  /* must zero complex part of diagonal! */
               Mjoin(PATLU,zero)(N, pd->C+1, (pd->ldc+1)SHIFT);
            #endif
         #endif
         ATL_mutex_unlock(pd->Cmut);
      }
   }  /* end of if we did work */
}
#undef one

#ifdef Conj_
void Mjoin(PATL,therk_tN)
#else
void Mjoin(PATL,tsyrk_tN)
#endif
(
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS TA,
   ATL_CSZT N,
   ATL_CSZT K,
#ifdef Conj_
   const TYPE ralpha,
#else
   const SCALAR alpha,
#endif
   const TYPE *A,
   ATL_CSZT lda,
#ifdef Conj_
   const TYPE rbeta,
#else
   const SCALAR beta,
#endif
   TYPE *C,
   ATL_CSZT ldc
)
/*
 * C NxN, A NxK
 * SYRK:
 *    C = alpha * A * A^T + beta*C, if TA == AtlasNoTrans
 *    C = alpha * A^T * A + beta*C, if TA == AtlasTrans
 * HERK:
 *    C = alpha * A * A^H + beta*C, if TA == AtlasNoTrans
 *    C = alpha * A^H * A + beta*C, if TA == AtlasTrans
 */
{
   ATL_tsyrk_tN_t pd;
   void *vp;
   #ifdef TCPLX
      const TYPE ONE[2] = {ATL_rone, ATL_rzero};
   #endif
   #ifdef Conj_
      const TYPE alpha[2] = {ralpha, ATL_rzero};
      const TYPE beta[2] = {rbeta, ATL_rzero};
   #endif
   size_t szA, szC, szT, szE, szP, incAk, nkb, k;
   ATL_CUINT nnu = (N+ATL_SYRKK_NU-1)/ATL_SYRKK_NU, NN=nnu*ATL_SYRKK_NU;
   ATL_UINT kb, P;
/*
 * Handle degenerate cases
 */
   if (!N)                            /* no output! */
      return;
   if (SCALAR_IS_ZERO(alpha) || !K)  /* really scale of C */
   {
      if (SCALAR_IS_ONE(beta))       /* no-op */
         return;
      if (SCALAR_IS_ZERO(beta))      /* explicit zero */
      {
         if (Uplo == AtlasLower)
            Mjoin(PATL,trsetL)(N, N, beta, beta, C, ldc);
         else
            Mjoin(PATL,trsetU)(N, N, beta, beta, C, ldc);
         return;
      }
      Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
      #ifdef Conj_  /* must zero complex part of diagonal for HERK! */
         Mjoin(PATLU,zero)(N, C+1, (ldc+1)SHIFT);
      #endif
      return;
   }
/*
 * Quick return if problem isn't worth parallelizing
 */
   #if 0
   if (K < (Mmax(ATL_sqAMM_66KB,ATL_SYRKK_KU)*Mmin(ATL_NTHREADS,4)) || 
       N == 1)
   {
      #ifdef Conj_
         Mjoin(PATL,herk_IP)(Uplo, TA, N, K, ralpha, A, lda, rbeta, C, ldc);
      #else
         Mjoin(PATL,syrk_IP)(Uplo, TA, N, K, alpha, A, lda, beta, C, ldc);
      #endif
       return;
   }
   #endif
/*
 * Choose a KB that is a multiple of KU.  Usually, we like near-square sizes,
 * but for small problems, you want to expand KB to avoid having vector
 * reduction (KVEC) and C store dominate cost.  There's some advantage to
 * containing one or more operands in the L1: on some systems, this results
 * in better max perf, but on all systems it helps to reduce bandwidth demands
 * for parallel operations.  So, don't expand kb beyond L1 if a square problem
 * could fit (since longer than NB KB has only marginal benefit, that might
 * prevent a more substantial in-L1 benefit).  If problem is really tiny,
 * may make sense to contain both C & A in the cache.  We ignore the case
 * where, C/A and the col-major A all fit, as that is super-tiny
 */
   szC = ((ATL_SYRKK_NU*ATL_SYRKK_NU+ATL_SYRKK_VLEN-1)/ATL_SYRKK_VLEN)
         * ATL_SYRKK_VLEN;
   szC *= ((nnu+1)*nnu)>>1;  /* only need lower tri blks, not full nnu*nnu */
   if (szC < ATL_L1elts)  /* If we can't fit C in L1, no square block can */
   {
      szE = ATL_L1elts - szC;  /* szE = elts in L1 after C stored there */
      kb = (szE>>1) / NN;      /* fit A, Ac & C all in L1 */
      if (kb < N)
         kb = ATL_L1elts / N;  /* fill L1 with just A */
      kb = (kb/ATL_SYRKK_KU)*ATL_SYRKK_KU;
      if (kb < N)              /* if we can't make at least square */
         kb = (ATL_sqAMM_LASTKB/ATL_SYRKK_KU)*ATL_SYRKK_KU;
   }
   else  /* just use kb that worked for out-of-L1 best GEMM case */
      kb = (ATL_sqAMM_LASTKB/ATL_SYRKK_KU)*ATL_SYRKK_KU;
   ATL_assert(kb);
/*
 * Now, see if we need to reduce kb to enhance parallelism, or if we have full
 */
   nkb = K / kb;
   if (nkb < ATL_NTHREADS && kb > ATL_sqAMM_66KB)  /* need to reduce */
   {
      do
      {
         kb -= ATL_SYRKK_KU;
         nkb = K / kb;
      }
      while (nkb < ATL_NTHREADS && kb > ATL_sqAMM_66KB);
   }
   pd.kb = kb;
   pd.nnu = nnu;
   pd.szC = szC;
   pd.KB0 = pd.kb0 = K - nkb*kb;
   if (pd.kb0)
   {
      #if ATL_SYRKK_KVEC > 1
         pd.KB0 = ((pd.kb0+ATL_SYRKK_KVEC-1)/ATL_SYRKK_KVEC)*ATL_SYRKK_KVEC;
      #endif
   }
/*
 * Compute the number of jobs (consisting of multiple kblks) and number of
 * k blocks given out individually (for load balancing)
 */
   pd.N = N;
   #if 1
      if (nkb > ATL_NTHREADS*3)
      {
         size_t nj, nk1;
         for (k=1; k < 8; k++)
         {
            nk1 = (ATL_NTHREADS-1)<<k;
            if (nk1+nk1 > nkb)
               break;
            nj = (nkb - nk1) >> k;
            if (nj < 2*ATL_NTHREADS)
               break;
         }
         pd.jobshift = --k;
         pd.njobs = (nkb - ((ATL_NTHREADS-1)<<k)) >> k;
         pd.njobs = (pd.njobs/ATL_NTHREADS)*ATL_NTHREADS;
         pd.nK1 = nkb - (pd.njobs<<k);
      }
      else
      {
         pd.nK1 = nkb;
         pd.jobshift = pd.njobs = 0;
      }
      #if 0
      printf("nkb=%d, njob=%d, jobCnt=%d, nK1=%d\n", 
             (int)nkb, (int)pd.njobs, 1<<pd.jobshift, (int)pd.nK1);
      #endif
   #elif 1
      pd.nK1 = nkb;
      pd.jobshift = pd.njobs = 0;
   #elif 0
      pd.njobs = nkb;
      pd.jobshift = 0;
      pd.nK1 = 0;
   #endif
   k = (pd.kb0) ? 1 : 0;
   k += pd.nK1 + pd.njobs << pd.jobshift;
   P = Mmin(ATL_NTHREADS, k);
//P = 1;
   #if Conj_
      pd.Pcnt = P;
      pd.NoTrans = (TA == AtlasNoTrans);
   #endif
/*
 * Our SYRK C copy is always to Lower, so if output is Upper, will need
 * workspace to put the Lower part, before reflecting it to Upper.
 * Need max of this extra space, or preload distance.
 */
   if (Uplo == AtlasUpper)
   {
      szT = P * N * N;
      szE = (szT >= ATL_SYRKK_NU*ATL_SYRKK_NU*2)?ATL_SYRKK_NU*ATL_SYRKK_NU*2:0;
   }
   else
   {
      szT = 0;
      szE = ATL_SYRKK_NU*ATL_SYRKK_NU*2;
   }
   szA = nnu*ATL_SYRKK_NU * kb;
   szP = ATL_MulBySize(szA+szC) + ATL_Cachelen;
   szP = ATL_MulByCachelen(ATL_DivByCachelen(szP + ATL_Cachelen-1));
   vp = malloc(ATL_MulBySize(P*(szP+szT)+szE)+2*ATL_Cachelen);
   ATL_assert(vp);

   pd.szA = szA;
   pd.szW = szP = ATL_DivBySize(szP)SHIFT;
   pd.w = ATL_AlignPtr(vp);
   if (Uplo == AtlasUpper)
   {
      pd.wT = pd.w + szP*P;
      pd.wT = ATL_AlignPtr(pd.wT);
   }
   else
      pd.wT = NULL;
   pd.A = A;
   pd.C = C;
   pd.lda = lda;
   pd.ldc = ldc;
   pd.alpha = alpha;
   pd.beta = beta;
   #ifdef TCPLX
      pd.ONE = ONE;
   #endif
   pd.appBeta = !SCALAR_IS_ONE(beta);
   pd.Cmut = ATL_mutex_init();
   if (pd.kb0)
      pd.KB0ctr = ATL_SetAtomicCount(1);
   else
      pd.KB0ctr = NULL;
   if (pd.njobs)
      pd.jobCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.njobs,P),pd.njobs,0);
   else
      pd.jobCtr = NULL;
   if (pd.nK1)
      pd.K1ctr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nK1,P), pd.nK1, 0);
   else
      pd.K1ctr = NULL;


   if (IS_COLMAJ(TA))
   {
      pd.incAk = lda*(kb SHIFT);
      pd.a2blk = Mjoin(PATL,a2blk_syrkT);
   }
   else
   {
      pd.incAk = kb SHIFT;
      pd.a2blk = Mjoin(PATL,a2blk_syrkN);
   }
   if (Uplo == AtlasLower)
   {
      if (SCALAR_IS_NONE(alpha))
      {
         pd.blk2c_b1 = Mjoin(PATL,syblk2cmat_an_b1);
         if (SCALAR_IS_ONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_an_b1);
         else if (SCALAR_IS_NONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_an_bn);
         else
            pd.blk2c = SCALAR_IS_ZERO(beta) ?  Mjoin(PATL,syblk2cmat_an_b0) :
                       Mjoin(PATL,syblk2cmat_an_bX);
      }
      else if (SCALAR_IS_ONE(alpha))
      {
         pd.blk2c_b1 = Mjoin(PATL,syblk2cmat_a1_b1);
         if (SCALAR_IS_ONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_a1_b1);
         else if (SCALAR_IS_NONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_a1_bn);
         else
            pd.blk2c = SCALAR_IS_ZERO(beta) ?  Mjoin(PATL,syblk2cmat_a1_b0) :
                       Mjoin(PATL,syblk2cmat_a1_bX);
      }
      else /* alpha = X */
      {
         pd.blk2c_b1 = Mjoin(PATL,syblk2cmat_aX_b1);
         if (SCALAR_IS_ONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_aX_b1);
         else if (SCALAR_IS_NONE(beta))
            pd.blk2c = Mjoin(PATL,syblk2cmat_aX_bn);
         else
            pd.blk2c = SCALAR_IS_ZERO(beta) ?  Mjoin(PATL,syblk2cmat_aX_b0) :
                       Mjoin(PATL,syblk2cmat_aX_bX);
      }
   }
   else 
   {
      if (SCALAR_IS_NONE(alpha))
         pd.blk2c = Mjoin(PATL,syblk2cmat_an_b0);
      else
         pd.blk2c = SCALAR_IS_ONE(alpha) ?
                 Mjoin(PATL,syblk2cmat_a1_b0):Mjoin(PATL,syblk2cmat_aX_b0);
   }

   ATL_goParallel(P, DoWork, NULL, &pd, NULL);

   ATL_mutex_free(pd.Cmut);
   if (pd.KB0ctr)
      ATL_FreeAtomicCount(pd.KB0ctr);
   if (pd.K1ctr)
      ATL_FreeGlobalAtomicCount(pd.K1ctr);
   if (pd.jobCtr)
      ATL_FreeGlobalAtomicCount(pd.jobCtr);
   free(vp);
}
@ROUT 
/*
 * This routine allocates the data structure for dealing out parallel jobs
 * with minimal overhead.  The idea each thread has 3 areas, each of which
 * is rounded up to ATL_SAFELS in size to avoid false sharing.  The three
 * areas are: 
 * 1. lock storage area
 * 2. locBV area (unsigned long): <nleft> <nbits> <bitvec>
 * 3. local buff: <N> <ngrab> <nused> <idx> [loc1] ... [locN]
 *
 * All work initially resides in the locBV area.  When the thread calls
 * getTask, up to <ngrab> jobs will be moved from locBV to local buff
 * (the global bit positon is stored as a long task number).  If <nleft>
 * is less than P, getTask may instead move some tasks back from local buff
 * to locBV, so it can be stolen by other threads.
 * We also have a header block (aligned to SAFELS) that contains:
 * <P> <lckSz> <bvSz> <buffSz> <nbigblks> <B> nsmallblks> <b>
 * Totsz = SAFELS*(1+3*P), assuming sizeof(BV) < 128, else rounded up.
 */
void *ATL_newTasklist(unsigned long ntasks, unsigned int P)
{
   unsigned long i, lckSz, bvSz, buffSz;

   i = ntasks >> 1;
   P = Mmin(P, i);
   lckSz = sizeof(ATL_lock_t);
   lckSz = ATL_AlignSafeLS(lckSz);
}
@ROUT atlas_gatmctr.h
#ifndef ATLAS_GATMCTR_H
   #define ATLAS_GATMCTR_H

#include "atlas_bitvec.h"
#include "atlas_threads.h"
#include "atlas_type.h"
/*
 * Define bit positions in the flag bitvec of boolean flag
 * NOTE: POLL implies LAST1
 */
#define ATL_GAC_MOV   1  /* set: move numbers priv->pub */
#define ATL_GAC_NOPUB 2  /* set: make all data private (static scheduling) */
#define ATL_GAC_NOPRV 4  /* set: make all data public */
#ifndef  GAC_FLAG_SET
   #define GAC_FLAG_SET(field_, bit_) ( ((field_) & (bit_)) != 0 )
#endif
#define ATL_GAC_PUB ATL_GAC_NOPRV
#define ATL_GAC_PRV ATL_GAC_NOPUB
#define ATL_GAC_MIX ATL_GAC_MOV
/*
 * Define offsets for initial SAFELS of global part of data structure
 */
#define ATL_GAC_P   0     /* these defines used to index */
#define ATL_GAC_FLG 1     /* data describing the gatmctr */
@skip #define ATL_GAC_GSZ 2     /* the first cacheline of global */
#define ATL_GAC_INC 2     /* only used in mixed case */
#define ATL_GAC_N   3     /* only used in mixed case */

/*
 * Global atomic counters return number between [1,N], with 0 meaning no
 * more count.  Count range is split amongst rank-linked counters, and so
 * is affectively random.  However, NOPRV always returns 1 as last job #.
 * Below, off is offset (negative) to get to the unaligned ptr to free.
 * (1) NOPRV: Perfect work stealing, but high overhead.  Last call returns 1.
 *            Use for large jobs that dominate mutex cost, and perfect stealing
 *            is very helpful.
 * -> [<P><flg><incLow><incCnt><off>][<low1>...<lowP>][<act1>]...[<actP>][actL]
 * (2) NOPUB: Pure static scheduling, very low overhead.  Use for very
 *            small jobs, where limited dynamic recovery enabled by having
 *            threads finish their portion, and re-enroll as a new rank.
 * -> data: [<P><flg><off>][<cnt1><low1>]....[<cntP><lowP>]
 * (3) else : mixed case with private and public jobs.  Low average overhead,
 *            but some threads may return 0 before the job is finished.
 *            Useful only when you've got another batch of jobs to do after
 *            this one.  See below for how work stealing works, and struct.
 * NOTE: only (1) above guarantees that the last non-zero return is 1.  
 *       (2) & (3) must assume random count return.
 *
 * MIXED PUBLIC AND PRIVATE COUNTERS:
 * The total range is [low,N], low >= 1, N > low, so totSpan = N-low+1.
 * Public jobs are in range [lowPub,N] and count down (PubSpan=cntPub-lowPub+1,
 * where lowPub initialized to N).  Private jobs are in range [low,lowPub-1] 
 * and count up (PrvSpan=lowPub-cntPrv, where cntPrv initialized to low).
 * This look like:
 *     ..Private count..|..Public Count..
 *      ---------------> <---------------
 *     [cntPrv,LowPub-1]|[lowPub, cntPub]
 *
 * -> private jobs consumed by cntPrv++, public jobs by cntPub--,
 *    with cntPrv initialized to low, and cntPub to N.
 * -> Work is moved from private to public by subtracting from LowPub, which
 *    increases the public span while decreasing the private span.
 * -> If public work taken since last access (lastPubcnt != PubCnt), move 
 *    private range to public
 * -> Start wt P jobs in public, rest private, except for small problems.
 * -> Once cntPub starts to change, private count will be moved to public
 *    to keep number of public jobs constant.
 *
 * SMALL PROBLEMS:
 * ===============
 * For very small problems, dynamic scheculing has too much overhead, so we
 * set the public count to 0 (cntPub=0).  This gives us counters that
 * implement pure static scheduling for P threads.  If the last core wakes
 * up terribly late, this can cause horrible delays, but this can be 
 * ameliorated by having finished threads rejoin the work with a new virtual
 * rank.  Thus, in the best case, small problems have no scheduling overhead,
 * and in worst (where scheduling would die), we have only the cost of
 * re-enrolling, so something like: (job cost) + (mutex cost).
 *
 * DATA STRUCTURE EXPLANATION
 * ==========================
 * Keeping independent items on separate cache lines prevents false sharing,
 * which is critical for parallel performance (false sharing on locks or
 * data can cause safe access times to increase by orders of magnitude).
 * ATLAS therefore defines a macro ATL_SAFELS which is assumed >= actual
 * line size.  Presently it is set at 128 bytes. The data structure is then
 * manually laid out to minimize false sharing, where each [] is SAFELS region. 
 * The global description is:
 *   [<P><flag><inc><N><off>]
 * The per-core information is:
 *    P*([<cntPrv><lastPubCnt>][lowPub][<cntPub>][<lock>]) :
 *  totSz = SAFELS + P*inc, inc=3*SAFELS+CEIL(sizeof(lock)/SAFELS),
 */

@skip void *ATL_gatmctr_allocArray(unsigned int P, long N, long flg);
size_t ATL_gatmctr_sizeof(unsigned int P, long flg);
void *ATL_gatmctr_init(void *ac, unsigned int P, long cnt, long flg);
void *ATL_gatmctr_alloc(unsigned int nctr, unsigned int P, long cnt, long flg,
                        long *inc);
void ATL_gatmctr_initByRank(void *ac, unsigned int rank, long lowPrv, 
                            long npriv, long npub);
void *ATL_gatmctr_new(unsigned int P, long N, long flg);
void ATL_gatmctr_free(void *ac);
void ATL_gatmctr_print(FILE *fpout, void *ac);
long ATL_gatmctr_dec(void *ac, unsigned int rank);

#endif  /* end ifndef ATLAS_GATMCTR_H */
@ROUT ATL_gatmctr_new
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_gatmctr.h" /* scope to understand data struct */
void *ATL_gatmctr_init(void *ac, unsigned int P, long cnt, long flg)
{
   long *gp = ATL_AlignSafeLS(ac);
   long cntPrv, lowPub, cntPub;
   long lcntPub, lcntPrv, remPub, remPrv;
   unsigned int i;

   ATL_assert(cnt > 0);
   P = Mmax(P,1);
   P = Mmin(P,cnt);

   gp[ATL_GAC_P] = P;
   gp[ATL_GAC_FLG] = flg;

   if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))   /* private count only */
   {
      cntPub = 0;
      cntPrv = cnt;
      ATL_assert(!(flg&ATL_GAC_NOPRV));
      gp[2] = ((size_t)gp) - ((size_t)ac);
   }
   else if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))  /* public count only */
   {
      long inc, incLow;

      cntPrv = 0;
      cntPub = cnt;
      ATL_assert(!(flg&ATL_GAC_NOPUB));

      #if ATL_ATM_ASM
         inc = ATL_SAFELS;
      #else
         inc = ATL_SAFELS + (long) ATL_AlignSafeLS(sizeof(ATL_lock_t));
      #endif
      incLow = sizeof(long)*P;
      incLow = (long) ATL_AlignSafeLS(incLow);
      gp[2] = incLow;   /* inc to skip over low array to atmctrs */
      gp[3] = inc;      /* inc between atmctrs */
      gp[4] = ((size_t)gp) - ((size_t)ac);
   }
   else /* work stealing divides count between private/public */
   {
      long inc;
      if (cnt < (P<<2))         /* if we can't have both pub & prv */
         cntPub = cnt;         /* just make all jobs public */
      else
      {
         cntPub = P*(P-1);       /* ideal scheduling case */
         cntPrv = cnt>>1;
         cntPub = Mmin(cntPrv, cntPub);
      }
      cntPrv = cnt - cntPub;

      inc = sizeof(ATL_lock_t);           /* sz of native lock struct */
      inc = (long) ATL_AlignSafeLS(inc);  /* make mul of SAFELS */
      inc += ATL_SAFELS + (ATL_SAFELS<<1);
      gp[ATL_GAC_P] = P;
      gp[ATL_GAC_INC] = inc;
      gp[ATL_GAC_N] = cnt;
      gp[ATL_GAC_FLG] = flg;
      gp[4] = ((size_t)gp) - ((size_t)ac);
   }

   lcntPub = cntPub / P;
   remPub = cntPub - lcntPub*P;
   lcntPrv = cntPrv / P;
   remPrv = cntPrv - lcntPrv*P;

   cnt=1;
   for (i=0; i < P; i++)
   {
      long mypub, myprv;

      myprv = (i < remPrv) ? lcntPrv+1 : lcntPrv;
      mypub = (i < remPub) ? lcntPub+1 : lcntPub;
      ATL_gatmctr_initByRank(gp, i, cnt, myprv, mypub);
      cnt += myprv + mypub;
   }
   return(gp);
}

size_t ATL_gatmctr_sizeof(unsigned int P, long flg)
/*
 * RETURNS: size of specified global atomic counter, not including any
 *          alignment guard.
 */
{
   size_t sz;
   if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))   /* all-public */
   {
      long incCnt=ATL_SAFELS, incLow;
      #if !ATL_ATM_ASM
         incCnt += (long) ATL_AlignSafeLS(sizeof(ATL_lock_t));
      #endif
      incLow = sizeof(long)*P;
      incLow = (long) ATL_AlignSafeLS(incLow);
      sz = ATL_SAFELS + incLow + (P+1)*incCnt;
   }
   else if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))  /* all private */
      sz = (P+1)*ATL_SAFELS;
   else
   {
      sz = sizeof(ATL_lock_t);             /* sz of native lock struct */
      sz = (size_t) ATL_AlignSafeLS(sz);   /* make mul of SAFELS */
      sz += (ATL_SAFELS<<1)+ATL_SAFELS;
      sz = sz*P + ATL_SAFELS;
   }
   return(sz);
}

void *ATL_gatmctr_alloc(unsigned int nctr, unsigned int P, long cnt, long flg,
                        long *inc)
{
   if (cnt && nctr)
   {
      size_t sz;
      void *vp;
      void *gp;
      unsigned int i;

      sz = ATL_gatmctr_sizeof(P, flg);
      if (inc)
         *inc = sz;
      vp = malloc(ATL_SAFELS+nctr*sz);
      ATL_assert(vp);
      return(vp);
   }
   else
      *inc = 0;
   return(NULL);
}

void ATL_gatmctr_initByRank(void *ac, unsigned int rank, long lowPrv, 
                            long npriv, long npub)
{
   if (ac)
   {
      long *ap = ac;
      const long P=ap[ATL_GAC_P];

      if (rank < P)
      {
         const long flg=ap[ATL_GAC_FLG];

         if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))
         {
            const long incLow=ap[2], incCnt=ap[3];
            long *lp=ATL_IncBySafeLS(ac), *atc;
            npub += npriv;
            if (rank)
            {
               atc = ATL_AddBytesPtr(lp, incLow+incCnt*rank);
               lp[rank] = lowPrv-1;
               ATL_atmctr_init(atc, npub);
            }
            else /* rank == 0, reserve one count for last */
            {
               atc = ATL_AddBytesPtr(lp, incLow);
               *lp = lowPrv;
               ATL_assert(npub && lowPrv == 1);
               ATL_atmctr_init(atc, --npub);
               atc = ATL_AddBytesPtr(lp, incLow+incCnt*P);
               ATL_atmctr_init(atc, 1);
            }
         }
         else if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))
         {
            long *lp = ATL_AddBytesPtr(ac, (rank+1)*ATL_SAFELS);
            npriv += npub;
            *lp   = npriv;
            lp[1] = lowPrv-1;
         }
         else
         {
            const long inc=ap[ATL_GAC_INC];
            const long lowPub = lowPrv+npriv;
            ap = ATL_AddBytesPtr(ap, ATL_SAFELS+rank*inc);
            *ap = lowPrv;
            ap[1] = (lowPub + npub) - 1;
            ap = ATL_IncBySafeLS(ap);
            *ap = lowPub;
            ap = ATL_IncBySafeLS(ap);
            *ap = (lowPub + npub) - 1;
            ap = ATL_IncBySafeLS(ap);
            ATL_lock_init(ap);
         }
      }
   }
}

void *ATL_gatmctr_new
(
   unsigned int P,    /* # of worker threads  */
   long N,    /* Count from [N,1] (0: already done) */
   long flg
)
{
   long *lp=NULL;
@skip   long cntPrv, lowPub, cntPub, cnt;
@skip   long lcntPub, lcntPrv, remPub, remPrv;
@skip   unsigned int i;

   if (!N)
      return(NULL);
   P = Mmax(P,1);
   P = Mmin(P,N);
   if (N)
   {
      lp = ATL_gatmctr_alloc(1, P, N, flg, NULL);
      lp = ATL_gatmctr_init(lp, P, N, flg);
   }
@beginskip
   if (N <= (P<<2) || GAC_FLAG_SET(flg, ATL_GAC_NOPUB))
   {
      cntPub = 0;
      flg |= ATL_GAC_NOPUB;
      flg &= ~ATL_GAC_MOV;
   }
   else if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))
      cntPub = N;
   else
   {
      cntPub = P-1;    /* assume we've got P*(P-1) jobs */
      cntPub *= P;     /* to do via public tasks */
      if (cntPub > (N>>1) ) /* not best case, refine cntPub */
      {
         cntPub = P+P;
         cntPub = (N >= cntPub) ? cntPub : N;
      }
   }
   cntPrv = N - cntPub;
   if (!cntPrv)
   {
      ATL_assert(cntPub);
      flg |= ATL_GAC_NOPRV;
      flg &= ~(ATL_GAC_NOPUB | ATL_GAC_MOV);
   }
   else if (!cntPub)
   {
      flg |= ATL_GAC_NOPUB;
      flg &= ~(ATL_GAC_NOPRV | ATL_GAC_MOV);
   }
   else
      ATL_assert(!GAC_FLAG_SET(flg, ATL_GAC_NOPUB) &&
                 !GAC_FLAG_SET(flg, ATL_GAC_NOPRV));
   if (GAC_FLAG_SET(flg, ATL_GAC_MOV)) /* if we move jobs to global */
      flg &= ~ATL_GAC_NOPUB;           /* we must scope global queue */

   lcntPub = cntPub / P;
   remPub = cntPub - lcntPub*P;
   lcntPrv = cntPrv / P;
   remPrv = cntPrv - lcntPrv*P;

   cnt=1;
   for (i=0; i < P; i++)
   {
      long mypub, myprv;

      myprv = (i < remPrv) ? lcntPrv+1 : lcntPrv;
      mypub = (i < remPub) ? lcntPub+1 : lcntPub;
      ATL_gatmctr_initByRank(lp, i, cnt, myprv, mypub);
      cnt += myprv + mypub;
   }
@endskip
   return(lp);
}
@ROUT ATL_gatmctr_free
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_gatmctr.h"
void ATL_gatmctr_free(void *ac)
{
   if (ac)
   {
      long *lp = ac;
      const long flg = lp[ATL_GAC_FLG];

      if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))
      {
         const long inc=lp[3];
         const unsigned int P=lp[ATL_GAC_P];
         unsigned int i;
         ac = ATL_AddBytesPtr(ac, ATL_SAFELS+lp[2]);
         for (i=0; i < P+1; i++, ac = ATL_AddBytesPtr(ac, inc))
            ATL_atmctr_destroy(ac);
         ac = ATL_SubBytesPtr(lp, lp[4]);
      }
      else if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))
         ac = ATL_SubBytesPtr(ac, lp[2]);
      else
      {
         const unsigned int P=lp[ATL_GAC_P];
         unsigned int i;
         const long inc = lp[ATL_GAC_INC];
         void *lck;

         lck = ATL_AddBytesPtr(lp, (ATL_SAFELS<<2));
         for (i=0; i < P; i++, lck = ATL_AddBytesPtr(lck, inc))
            ATL_lock_destroy(lck);
         ac = ATL_SubBytesPtr(ac, lp[4]);
      }
      free(ac);
   }
}

@ROUT ATL_gatmctr_print
#include "atlas_gatmctr.h"  /* see to understand usage & data struct */
void ATL_gatmctr_print(FILE *fpout, void *ac)
{
   long *gp=ac, *tp;
   const long P=gp[ATL_GAC_P], flg=gp[ATL_GAC_FLG];
   long i;

   if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))
   {
      long *lp = ATL_IncBySafeLS(gp), *lac;
      const long incLow = gp[2], incCnt=gp[3];
      fprintf(fpout, 
              "gatmctr-PUB, P=%ld, iLow=%ld, iCnt=%ld, off=%ld, flg=%lx\n",
              P, incLow, incCnt, gp[4], flg);
      lac = ATL_AddBytesPtr(lp, incLow);
      for (i=0; i < P; i++, lac=ATL_AddBytesPtr(lac, incCnt))
         fprintf(fpout, "   %ld: low=%ld, cnt=%ld\n", i, lp[i], *lac);
      fprintf(fpout, "   L1: low=0, cnt=%ld\n", *lac);
   }
   else if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))
   {
      long *lp = ATL_IncBySafeLS(gp);
      fprintf(fpout, "gatmctr-PRV, P=%ld, off=%ld, flg=%lx\n", P, gp[2], flg);
      for (i=0; i < P; i++, lp=ATL_IncBySafeLS(lp))
         fprintf(fpout, "   %ld: low=%ld, cnt=%ld\n", i, lp[1], *lp);
   }
   else
   {
      const long inc=gp[ATL_GAC_INC];
      fprintf(fpout, "gAtmCtr, P=%ld, inc=%ld, N=%ld, flg=%lx:\n", 
              P, inc, gp[ATL_GAC_N], flg);
      tp = ATL_AddBytesPtr(gp, ATL_SAFELS);
      for (i=0; i < P; i++)
      {
         long *prv=tp, *lowPubP=ATL_IncBySafeLS(tp), 
                      *cntPubP=ATL_IncBySafeLS(lowPubP);
         fprintf(fpout, 
            "   %ld: prv=[%ld, %ld], pub=[%ld, %ld], lastPubCnt=%ld, lck=%p\n",
                 i, *prv, *lowPubP-1, *lowPubP, *cntPubP, prv[1], 
                 ATL_IncBySafeLS(cntPubP));
         tp = ATL_AddBytesPtr(tp, inc);
      }
   }         
}
@ROUT ATL_gatmctr_dec
@extract -b @(topd)/cw.inc lang=c -define cwdate 2016
#include "atlas_gatmctr.h"  /* see to understand usage & data struct */
@skip #define DEBUG 1

static INLINE long allPub(unsigned int rank, long *lp)
{
   const unsigned long P=lp[ATL_GAC_P];
   unsigned long i, PP;
   const long incCnt = lp[3];
   long *lowp = ATL_IncBySafeLS(lp), *acp = ATL_AddBytesPtr(lowp, lp[2]), *ac;
   unsigned long vrk;

   ac = ATL_AddBytesPtr(acp, incCnt*P);
   if (!(*ac))
      return(0);
/*
 * Use owned counter until exhausted
 */
   if (rank < P)
   {
      ac = ATL_AddBytesPtr(acp, incCnt*rank);
      if (*ac)
      {
         long lret;
         lret = ATL_atmctr_dec(ac);
         if (lret)
            return(lret+lowp[rank]);
      }
      vrk = (rank == P-1) ? 0 : rank+1;
      PP = P-1;
   }
   else
   {
      vrk = rank - (rank/P)*P;
      PP = P;
   }
/*
 * See if anybody else has count I can steal
 */
   ac = ATL_AddBytesPtr(acp, incCnt*vrk);
   for (i=0; i < PP; i++)
   {
      if (*ac)
      {
         long lret;
         lret = ATL_atmctr_dec(ac);
         if (lret)
            return(lret+lowp[vrk]);
      }
      if (++vrk != P)
         ac = ATL_AddBytesPtr(ac, incCnt);
      else
      {
         vrk = 0;
         ac = acp;
      }
   }
/*
 * Finally, check 1/0 counter
 */
   ac = ATL_AddBytesPtr(acp, incCnt*P);
   if (*ac)
      return(ATL_atmctr_dec(ac));
   return(0);
}
static INLINE long allPrv(unsigned int rank, long *lp)
{
   const unsigned long P=lp[ATL_GAC_P];
/*
 * Private counters are all that are available
 */
   if (rank < P)  /* so only P owners can hope for jobs */
   {
      lp = ATL_AddBytesPtr(lp, (rank+1)*ATL_SAFELS);
      if (*lp)
      {
         long lret;
         lret = *lp;
         if (lret)
         {
            *lp = lret - 1;
            return(lret + lp[1]);
         }
      }
   }
   return(0);
}

/*
 * Helper function that steals work from another thread.  Will make only 
 * one pass through public queues in search of work, and return 0 if none
 * available at this time.
 */
static INLINE long tryPub
   (unsigned int rank, volatile long *gp, long *lp)
{
   const long P=gp[ATL_GAC_P], inc=gp[ATL_GAC_INC];
   unsigned int i;
/*
 * If I have my own public counter, prefentially use that up first
 */
   if (rank < P)
   {              
      volatile long *lowPubP=ATL_AddBytesPtr(lp, inc*rank+ATL_SAFELS),
                            *cntPubP= ATL_IncBySafeLS(lowPubP);
      if (*cntPubP >= *lowPubP)  /* any public count left? */
      {
         void *lck = ATL_IncBySafeLS(cntPubP);
         long cnt;

         ATL_lock(lck);
         cnt = *cntPubP;
         if (cnt >= *lowPubP)
         {
            *cntPubP = cnt - 1;
            ATL_unlock(lck);
            return(cnt);
         }
         else 
            ATL_unlock(lck);
      }
   }
/*
 * Make one pass through all P public counts, then give up
 */
   {
      unsigned int vrk = (rank >= P) ?  rank-P : rank;
      volatile long *cntPubP;
      cntPubP = ATL_AddBytesPtr(lp, (ATL_SAFELS<<1)+inc*vrk);
      for (i=0; i < P; i++)
      {
         volatile long *lowPubP = ATL_DecBySafeLS(cntPubP);
         
         if (rank >= P || i != rank)
         {
            if (*cntPubP >= *lowPubP)  /* any public count left? */
            {
               char *lck=ATL_IncBySafeLS(cntPubP);
               long lowPub, cnt;

               ATL_lock(lck);
               lowPub = *lowPubP;
               cnt = *cntPubP;
               if (cnt >= lowPub)
               {
                  *cntPubP = cnt - 1;
                  ATL_unlock(lck);
                  return(cnt);
               }
               else 
                  ATL_unlock(lck);
            }
         }
         if (++vrk < P)
            cntPubP = ATL_AddBytesPtr(cntPubP, inc);
         else
         {
            vrk = 0;
            cntPubP = ATL_AddBytesPtr(lp, (ATL_SAFELS<<1));
         }
      }
/* 
 *    If I'm quitting set it so we stop moving data from private to public.
 *    This avoids moving private to public when we have no workers left.
 *    QUIT should be set when the quitters can do another task, so ending
 *    load balance shouldn't matter much
 */
      gp[ATL_GAC_FLG] &= ~ATL_GAC_MOV;
   }
   return(0);
}

/*
 * Gets a single count from private range, returns 0 if none left
 */
static INLINE long 
   prvCnt(unsigned int rank, volatile long *gp, long *tp)
{
   long *cntPrvP, *lowPubP;
   const long inc=gp[ATL_GAC_INC];
   long cnt, lowPub;

   if (rank > *gp)
      return(0);
   cntPrvP = ATL_AddBytesPtr(tp, inc*rank);
   lowPubP = ATL_IncBySafeLS(cntPrvP);

   cnt = *cntPrvP;
   if (cnt >= *lowPubP)
      return(0);
   *cntPrvP = cnt + 1;
   return(cnt);
}

static INLINE long prvPubCnt
   (unsigned int rank, volatile long *gp, long *lp)
/*
 * Get count from either private or public count.  Always prefers private
 * to public, and will move private work to public if the number of my
 * public jobs has changed (indicating work stealing has begun).
 * ASSUMES: rank < P.
 */
{
   long *prvP, *lowPubP;
   const long P=gp[ATL_GAC_P], inc=gp[ATL_GAC_INC], flg=gp[ATL_GAC_FLG];
   long lowPub, npriv;

   prvP = ATL_AddBytesPtr(lp, rank*inc);
   lowPubP = ATL_IncBySafeLS(prvP);
   lowPub = *lowPubP;
   npriv = *prvP;
   npriv = (npriv < lowPub) ? lowPub-npriv : 0;
   if (npriv)  /* can get job from private */
   {
      long *cntPubP = ATL_IncBySafeLS(lowPubP);
      const long lastPub=prvP[1];
/*
 *    Should we move some work from private to public queue? Only do so if
 *    this ctr allows moving work, we have extra private work,, and the number
 *    of public jobs has changes since the last time we checked (ctrs starting
 *    with 0 public work result in static scheduling on virtual rank).
 */
      if (GAC_FLAG_SET(flg, ATL_GAC_MOV) && npriv > 1 && (*cntPubP != lastPub))
      {
         long nmov, cntPub;
         void *lck = ATL_IncBySafeLS(cntPubP);
/*
 *       Lock my public ctr, and move some private jobs to it
 */
         ATL_lock(lck);
         lowPub = *lowPubP;
         npriv = *prvP;
         npriv = (npriv < lowPub) ? lowPub-npriv : 0;
         cntPub = *cntPubP;
         ATL_assert(lastPub > cntPub);
         nmov = lastPub - cntPub;  /* default: keep #of public jobs constant */
         nmov = (nmov < npriv) ? nmov : npriv-1;
         *lowPubP = lowPub - nmov;
         ATL_unlock(lck);
         prvP[1] = cntPub + nmov;  /* set lastPubCnt to new value */
         return((*prvP)++);
      }
      else
         return(prvCnt(rank, gp, lp));
   }
   return(tryPub(rank, gp, lp));
}

long ATL_gatmctr_dec(void *ac, unsigned int rank)
{
   if (ac)
   {
      long *gp=ac;
      const long P=gp[ATL_GAC_P], flg=gp[ATL_GAC_FLG];
      long ret;

      if (GAC_FLAG_SET(flg, ATL_GAC_NOPRV))
         ret = allPub(rank, ac);
      else if (GAC_FLAG_SET(flg, ATL_GAC_NOPUB))
         ret = allPrv(rank, ac);
      else
      {
         const long inc=gp[ATL_GAC_INC];
         long *lp=ATL_IncBySafeLS(gp);

         if (rank < P)
            ret = prvPubCnt(rank, gp, lp);
         else
         {
            rank = P + rank%P;
            ret = tryPub(rank, gp, lp);
         }
      }
      #ifdef DEBUG
         fprintf(stderr, "gDec=%ld\n", ret);
      #endif
      return(ret);
   }
   return(0);
}
@ROUT ATL_GetSchedDelayIdx
#include "atlas_tsched_dly.h"
#ifdef ATL_AMM_NSCHED
unsigned int ATL_GetSchedDelayIdx(float dly)
/*
 * Searches ATL_AMM_NSCHED-entry array ATL_AMM_SCHEDdly for last entry <= dly.
 * Uses binary search with assumption time is strictly increasing, which may
 * not always be true due to timing variance & kernel choice, but it should
 * be good enough for estimation.  Don't want the overhead of linear search
 * everytime we want to estimate parallel schedule overhead!
 */
{
   unsigned int i0=0, dist=ATL_AMM_NSCHED>>1, iret;
   float mid;
/*
 * Early exit for out-of-range delay 
 */
   if (ATL_AMM_NSCHED<2 || dly <= *ATL_AMM_SCHEDdly)
      return(0);
   else if (dly >= ATL_AMM_SCHEDdly[ATL_AMM_NSCHED-1])
      return(ATL_AMM_NSCHED-1);
/*
 * DELAY[0] < dly < DELAY[N-1], do binary search
 */
   do
   {
      iret = i0 + dist;
      mid=ATL_AMM_SCHEDdly[iret];
      if (mid < dly) /* change i0 */
         i0 = iret;
      else if (mid == dly)
         return(iret);
      dist >>= 1;     /* eliminated half of entries, endpt dist halved too */
   }
   while (dist > 1);
   iret = (mid > dly) ? iret-1 : iret;
   return(iret);
}
#endif
@ROUT ATL_GetSchedTime
#ifdef ATL_MUT
   #define ATL_GetSchedTime ATL_GetSchedTime_mut
   #define ATL_TIMEs ATL_AMM_SCHEDmut
   #define NOSCHEDlac 1
   #define NOSCHEDpub 1
   #define NOSCHEDprv 1
   #define NOSCHEDmix 1
#elif defined(ATL_LAC)
   #define ATL_GetSchedTime ATL_GetSchedTime_lac
   #define ATL_TIMEs ATL_AMM_SCHEDlac
   #define NOSCHEDmut 1
   #define NOSCHEDpub 1
   #define NOSCHEDprv 1
   #define NOSCHEDmix 1
#elif defined(ATL_PUB)
   #define ATL_GetSchedTime ATL_GetSchedTime_pub
   #define ATL_TIMEs ATL_AMM_SCHEDpub
   #define NOSCHEDmut 1
   #define NOSCHEDlac 1
   #define NOSCHEDprv 1
   #define NOSCHEDmix 1
#elif defined(ATL_MIX)
   #define ATL_GetSchedTime ATL_GetSchedTime_mix
   #define ATL_TIMEs ATL_AMM_SCHEDmix
   #define NOSCHEDmut 1
   #define NOSCHEDpub 1
   #define NOSCHEDprv 1
   #define NOSCHEDlac 1
#elif defined(ATL_PRV)
   #define ATL_GetSchedTime ATL_GetSchedTime_prv
   #define ATL_TIMEs ATL_AMM_SCHEDprv
   #define NOSCHEDmut 1
   #define NOSCHEDpub 1
   #define NOSCHEDlac 1
   #define NOSCHEDmix 1
#endif
#include "atlas_tsched_time.h"
#ifdef ATL_AMM_NSCHED
float ATL_GetSchedTime(unsigned int idx)
{
   return(ATL_TIMEs[idx]);
}
#endif
