@ROUT atlas_trsmK.h
/*
 * Prototypes trsm kernel routines used in gemm or amm-based BLAS routines
 */
#ifndef ATLAS_TRSMK_H
   #define ATLAS_TRSMK_H 1
#include "atlas_misc.h"
#include "atlas_amm.h"

@multidef typ double float
@whiledef pre d s

int ATL_@(pre)trsmKL_rk4
   (enum ATLAS_SIDE Side, enum ATLAS_UPLO Uplo, enum ATLAS_TRANS TA, 
    enum ATLAS_DIAG Diag, ATL_CINT M, ATL_CINT N, const @(typ) alpha, 
    const @(typ) *A, ATL_CINT lda, @(typ) *B, ATL_CINT ldb);
void ATL_@(pre)ktrsmLLN_rk4  /* rank-4 based trsm kernel */
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const @(typ) alpha,  /* scale factor for B */
   const @(typ) *A, /* MxM lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb, /* leading dim of B */
   @(typ) *W        /* Mx4 workspace with good alignment */
);

int ATL_@(pre)trsmKL_amm  /* amm-based trsm kernel primitive */
(
   amminfo_t *mmp,      /* chosen amm kernel info */
   enum ATLAS_DIAG Diag,
   ATL_CUINT mb0,       /* sizeof first triangular block */
   ATL_CUINT nmblks,    /* CEIL(M/mb); M = (nmblks-1)*b+mb0 */
   ATL_CUINT nnblks,    /* FLOOR(N/nb) */
   ATL_CUINT nr,        /* mod(N/nb) */
   const SCALAR alpha,  /* scale factor for rhs */
   const @(typ) *L,       /* L, already copied to row-panel storage as above */
   size_t incL,         /* distance between blocks in L */
   @(typ) *R,             /* ptr to col-major rhs (B on input, X on output */
   size_t ldr,          /* leading dim of RHS matrix R */
   ATL_CUINT nmu,       /* nmu = mb/mu, assume mod(mb/mu) == 0 */
   ATL_CUINT nnu,       /* nnu = NB/nu, assume mod(NB/nu) == 0 */
   TYPE *c,             /* bxNB with trailing readable space for amm */
   TYPE *W,             /* aligned workspace for amm's B blocks */
   size_t incW          /* stride between b*NB blocks */
);

int ATL_@(pre)trsmL_amm      /* general Left amm-based trsm */
(                       /* RETURNS: 0 on success, non-0 if op not done */
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,         /* size of triangular matrix A */
   ATL_CINT  N,         /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const @(typ) *T,     /* MxM triangular matrix A */
   ATL_CINT  ldt,       /* leading dim of T */
   @(typ) *R,           /* on input, B, on output X, of A x = b */
   ATL_CINT  ldr        /* leading dim of R */
);

void Mjoin(PATL,trsmK_L2blk)   /* takes data in col-maj Lower format */
(                              /* puts in format required by trsmL_amm */
   enum ATLAS_DIAG Diag,
   const int mb0,   /* size of first triangular block */
   const int b,     /* size of all other blocks */
   int nmblks,      /* number of blocks of size b (not incl mb0) */
   const TYPE *L,   /* lower triang mat of x = inv(L) b */
   size_t ldl,      /* leading dim of L */
   TYPE *W,         /* wrkspc to copy blocks to */
   size_t incW,     /* stride between blocks (incW >= MAX(b,mb0)^2) */
   cm2am_t a2blk    /* copy rect blks to amm storage */
);
   @undef typ
@endwhile
#endif
@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4 ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
#include "atlas_misc.h"
#include "atlas_prefetch.h"
#define RTYPE register TYPE

#if defined(__GNUC__) || \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__/100 >= 1999))
   #define ATL_SINLINE static inline
#else
   #define ATL_SINLINE static
#endif
@ROUT ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
#if defined(ATL_AVX) && defined(DCPLX)
   #define NRHS 3
   #define TRU  2     /* this kernel requires A padded to vector length */
   #include <immintrin.h>
/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=3, ku=2.  This version is for 16 AVX regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                           TYPE *pB0, ATL_CINT ldb0, TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   const TYPE *pA1 = pA0+lda; 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   TYPE *pB2 = pB0 + (ldb<<1);
   ATL_CINT MM =  (M & 2) ? M-2 : M-4; 
   int i; 
   register __m256d rB00, iB00, rB01, iB01, rB02, iB02;
   register __m256d C00, C01, C02; 
   register __m256d C20, C21, C22; 
   register __m256d A, a;

   a = _mm256_set1_pd(ATL_rnone);
   rB00 = _mm256_set_pd(pB0[2], *pB0, pB0[2], *pB0);  
                                                /* rB10 rB00 rB10 rB00 */
   rB00 = _mm256_mul_pd(a, rB00);                 
                                                /* negate B for alpha=-1 */
   iB00 = _mm256_set_pd(pB0[3], pB0[1], pB0[3], pB0[1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB00 = _mm256_mul_pd(a, iB00);                   
                                                /* negate B for alpha=-1 */

   rB01 = _mm256_set_pd(pB0[ldb+2], pB0[ldb], pB0[ldb+2], pB0[ldb]);  
                                                /* rB10 rB00 rB10 rB00 */
   rB01 = _mm256_mul_pd(a, rB01);
                                                /* negate B for alpha=-1 */
   iB01 = _mm256_set_pd(pB0[ldb+3], pB0[ldb+1], pB0[ldb+3], pB0[ldb+1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB01 = _mm256_mul_pd(a, iB01);               /* negate B for alpha=-1 */
                                                
   rB02 = _mm256_set_pd(pB2[2], *pB2, pB2[2], *pB2);  
                                                /* rB12 rB02 rB12 rB02 */
   rB02 = _mm256_mul_pd(a, rB02);                 
                                                /* negate B for alpha=-1 */
   iB02 = _mm256_set_pd(pB2[3], pB2[1], pB2[3], pB2[1]);
                                                /* iB10 iB00 iB10 iB00 */
   iB02 = _mm256_mul_pd(a, iB02);                   
                                                /* negate B for alpha=-1 */

   C00  = _mm256_load_pd(pC0);                  /* iC10 rC10 iC00 rC00 */
   C01  = _mm256_load_pd(pC1);                  /* iC11 rC11 iC01 rC01 */
   C02  = _mm256_load_pd(pC2);                  /* iC12 rC12 iC02 rC02 */

   A    = _mm256_load_pd(pA0);                  /* iA10 rA10 iA00 rA00 */
   a    = _mm256_shuffle_pd(A, A, 0x5);         /* rA10 iA10 rA00 iA00 */

   for (i=0; i < MM; i += 4, pA0 += 8, pA1 += 8, pC0 += 8, pC1 += 8, pC2 += 8)
   {                                     /* rB00 = rB10 rB00 rB10 rB00 */
                                         /* iB00 = iB10 iB00 iB10 iB00 */
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      #ifdef ATL_AVXMAC
         C00 = _mm256_fmadd_pd(A, b, C00);
      #else
         m    = _mm256_mul_pd(A, b);
         C00 = _mm256_add_pd(m, C00);
      #endif  
      C20 = _mm256_load_pd(pC0+4);
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      #ifdef ATL_AVXMAC
         C01 = _mm256_fmadd_pd(A, b, C01);
      #else
         m    = _mm256_mul_pd(A, b);
         C01 = _mm256_add_pd(m, C01);
      #endif
      C21 = _mm256_load_pd(pC1+4);
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      #ifdef ATL_AVXMAC
         C02 = _mm256_fmadd_pd(A, b, C02);
      #else
         m    = _mm256_mul_pd(A, b);
         C02 = _mm256_add_pd(m, C02);
      #endif 
      A = _mm256_load_pd(pA1);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      #ifdef ATL_AVXMAC00
         C00 = _mm_fmaddsub_pd(a, b, C00);
      #else
         m    = _mm256_mul_pd(a, b);
         C00 = _mm256_addsub_pd(C00, m);
      #endif  
      C22 = _mm256_load_pd(pC2+4);
      b = _mm256_unpacklo_pd(iB01, iB01);
      #ifdef ATL_AVXMAC00
         C01 = _mm_fmaddsub_pd(a, b, C01);
      #else
         m    = _mm256_mul_pd(a, b);
         C01 = _mm256_addsub_pd(C01, m);
      #endif
      b = _mm256_unpacklo_pd(iB02, iB02);
      #ifdef ATL_AVXMAC00
         C02 = _mm_fmaddsub_pd(a, b, C02);
      #else
         m    = _mm256_mul_pd(a, b);
         C02 = _mm256_addsub_pd(C02, m);
      #endif 
      a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      #ifdef ATL_AVXMAC
         C00 = _mm256_fmadd_pd(A, b, C00);
      #else
         m    = _mm256_mul_pd(A, b);
         C00 = _mm256_add_pd(m, C00);
      #endif
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      #ifdef ATL_AVXMAC
         C01 = _mm256_fmadd_pd(A, b, C01);
      #else
         m    = _mm256_mul_pd(A, b);
         C01 = _mm256_add_pd(m, C01);
      #endif
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      #ifdef ATL_AVXMAC
         C02 = _mm256_fmadd_pd(A, b, C02);
      #else
         m    = _mm256_mul_pd(A, b);
         C02 = _mm256_add_pd(m, C02);
      #endif  
      A = _mm256_load_pd(pA0+4);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      #ifdef ATL_AVXMAC00
         C00 = _mm_fmaddsub_pd(a, b, C00);
      #else
         m    = _mm256_mul_pd(a, b);
         C00 = _mm256_addsub_pd(C00, m); 
      #endif
      _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      #ifdef ATL_AVXMAC00
         C01 = _mm_fmaddsub_pd(a, b, C01);
      #else
         m    = _mm256_mul_pd(a, b);
         C01 = _mm256_addsub_pd(C01, m); 
      #endif
      _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      #ifdef ATL_AVXMAC00
         C02 = _mm_fmaddsub_pd(a, b, C02);
      #else
         m    = _mm256_mul_pd(a, b);
         C02 = _mm256_addsub_pd(C02, m);
      #endif
      _mm256_store_pd(pC2, C02);
/*
 *    Do M=2, K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      #ifdef ATL_AVXMAC
         C20 = _mm256_fmadd_pd(A, b, C20);
      #else
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);
      #endif  
      a = _mm256_shuffle_pd(A, A, 0x5);
      b = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         C21 = _mm256_fmadd_pd(A, b, C21);
      #else
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
      #endif  
      C00 = _mm256_load_pd(pC0+8);
      b = _mm256_unpacklo_pd(rB02, rB02); 
      #ifdef ATL_AVXMAC
         C22 = _mm256_fmadd_pd(A, b, C22);
      #else
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);
      #endif
      A = _mm256_load_pd(pA1+4);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      #ifdef ATL_AVXMAC00
         C20 = _mm_fmaddsub_pd(a, b, C20);
      #else
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m);
      #endif
      C01 = _mm256_load_pd(pC1+8);
      b = _mm256_unpacklo_pd(iB01, iB01);
      #ifdef ATL_AVXMAC00
         C21 = _mm_fmaddsub_pd(a, b, C21);
      #else
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m);
      #endif  
      C02 = _mm256_load_pd(pC2+8);
      b = _mm256_unpacklo_pd(iB02, iB02);
      #ifdef ATL_AVXMAC00
         C22 = _mm_fmaddsub_pd(a, b, C22);
      #else
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m);   
      #endif
      a = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    M=2, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      #ifdef ATL_AVXMAC
         C20 = _mm256_fmadd_pd(A, b, C20);
      #else
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);
      #endif
      b = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         C21 = _mm256_fmadd_pd(A, b, C21);
      #else
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
      #endif
      b = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         C22 = _mm256_fmadd_pd(A, b, C22);
      #else
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);   
      #endif
      A = _mm256_load_pd(pA0+8);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      #ifdef ATL_AVXMAC00
         C20 = _mm_fmaddsub_pd(a, b, C20);
      #else
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m);
      #endif
      _mm256_store_pd(pC0+4, C20);
      b = _mm256_unpackhi_pd(iB01, iB01);
      #ifdef ATL_AVXMAC00
         C21 = _mm_fmaddsub_pd(a, b, C21);
      #else
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m);
      #endif
      _mm256_store_pd(pC1+4, C21);
      b = _mm256_unpackhi_pd(iB02, iB02);
      #ifdef ATL_AVXMAC00
         C21 = _mm_fmaddsub_pd(a, b, C22);
      #else
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m);
      #endif 
      _mm256_store_pd(pC2+4, C22);   
      a    = _mm256_shuffle_pd(A, A, 0x5);
   }
/*
 * Drain pipes
 */
   {
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      #ifdef ATL_AVXMAC
         C00 = _mm256_fmadd_pd(A, b, C00);
      #else
         m    = _mm256_mul_pd(A, b);
         C00 = _mm256_add_pd(m, C00);  
      #endif
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      #ifdef ATL_AVXMAC
         C01 = _mm256_fmadd_pd(A, b, C01);
      #else
         m    = _mm256_mul_pd(A, b);
         C01 = _mm256_add_pd(m, C01);  
      #endif
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      #ifdef ATL_AVXMAC
         C02 = _mm256_fmadd_pd(A, b, C02);
      #else
         m    = _mm256_mul_pd(A, b);
         C02 = _mm256_add_pd(m, C02);  
      #endif
      A = _mm256_load_pd(pA1);
      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      #ifdef ATL_AVXMAC
         C00 = _mm256_fmadd_pd(A, b, C00);
      #else
         m    = _mm256_mul_pd(A, b);
         C00 = _mm256_add_pd(m, C00);
      #endif
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      #ifdef ATL_AVXMAC
         C01 = _mm256_fmadd_pd(A, b, C01);
      #else
         m    = _mm256_mul_pd(A, b);
         C01 = _mm256_add_pd(m, C01);
      #endif
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      #ifdef ATL_AVXMAC
         C02 = _mm256_fmadd_pd(A, b, C02);
      #else
         m    = _mm256_mul_pd(A, b);
         C02 = _mm256_add_pd(m, C02);
      #endif

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m); _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m); _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); _mm256_store_pd(pC2, C02);
      if (!(M&2))
      {  
         A = _mm256_load_pd(pA0+4);
         C20 = _mm256_load_pd(pC0+4);
         C21 = _mm256_load_pd(pC1+4);
         C22 = _mm256_load_pd(pC2+4);
         a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       Do M=2, K=0 calcs
 */
         b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);  a    = _mm256_shuffle_pd(A, A, 0x5);
         b = _mm256_unpacklo_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpacklo_pd(rB02, rB02); 
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  A = _mm256_load_pd(pA1+4);

         b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m);
         b = _mm256_unpacklo_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m);
         b = _mm256_unpacklo_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m);   a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       M=2, K=1 calcs
 */
         b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);
         b = _mm256_unpackhi_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpackhi_pd(rB02, rB02);
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  
   
         b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m); _mm256_store_pd(pC0+4, C20);
         b = _mm256_unpackhi_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m); _mm256_store_pd(pC1+4, C21);
         b = _mm256_unpackhi_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m); _mm256_store_pd(pC2+4, C22);   
      }
   }
}
#elif defined(ATL_SSE3) && defined(DCPLX)
   #define NRHS 3
   #define TRU 2
   #include <xmmintrin.h>
   #include <pmmintrin.h>
/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=4, ku=2.  This version is for 16 SSE regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                           TYPE *pB0, ATL_CINT ldb0, TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   const TYPE *pA1 = pA0+lda;
   const TYPE none = ATL_rnone;
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   ATL_INT i;
   ATL_CINT MM = (M&1) ? M-1 : M-2;
   register __m128d B00, B10, B01, B11, B02, B12;
   register __m128d C00, C01, C02, C10, C11, C12;
   register __m128d A, a;

   A = _mm_loaddup_pd(&none);
   B00 = _mm_load_pd(pB0);
   B00 = _mm_mul_pd(B00, A);         
   B10 = _mm_load_pd(pB0+2);
   B10 = _mm_mul_pd(B10, A);         
   B01 = _mm_load_pd(pB0+ldb);
   B01 = _mm_mul_pd(B01, A);         
   B11 = _mm_load_pd(pB0+ldb+2);
   B11 = _mm_mul_pd(B11, A);         
   B02 = _mm_load_pd(pB0+ldb+ldb);
   B02 = _mm_mul_pd(B02, A);
   B12 = _mm_load_pd(pB0+ldb+ldb+2);    	/* iB12, rB12 */
   B12 = _mm_mul_pd(B12, A);


   C00 = _mm_load_pd(pC0);
   C01 = _mm_load_pd(pC1);
   C02 = _mm_load_pd(pC2);
   A = _mm_load_pd(pA0);                		/* iA00, rA00 */
   for (i=0; i < MM; i += 2, pA0 += 4, pA1 += 4, pC0 += 4, pC1 += 4, pC2 += 4)
   {
      register __m128d b;
/*
 *    K=0, M=[0,1], apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA00*rB00, rA00*rB00 */
      C00 = _mm_add_pd(C00, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA00, iA00 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
         C10 = _mm_load_pd(pC0+2);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA1);                		/* iA01, rA01 */
/*
 *    K=0, M=0, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA00*iB00, iA00*iB00 */
      C00 = _mm_addsub_pd(C00, b);
         C11 = _mm_load_pd(pC1+2);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         C12 = _mm_load_pd(pC2+2);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
/*
 *    K=1, M=0, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA01*rB10, rA01*rB10 */
      C00 = _mm_add_pd(C00, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA01, iA01 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA0+2);                /* iA10, rA10 */
/*
 *    K=1, M=0, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA01*iB10, iA01*iB10 */
      C00 = _mm_addsub_pd(C00, b);
         _mm_store_pd(pC0, C00);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         _mm_store_pd(pC1, C01);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
         _mm_store_pd(pC2, C02);   
/*
 *    K=0, M=1, apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA10*rB00, rA10*rB00 */
      C10 = _mm_add_pd(C10, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA10, iA10 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C11 = _mm_add_pd(C11, b);
         C00 = _mm_load_pd(pC0+4);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C12 = _mm_add_pd(C12, b);
         A = _mm_load_pd(pA1+2);               		/* iA11, rA11 */
/*
 *    K=0, M=1, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA10*iB00, iA10*iB00 */
      C10 = _mm_addsub_pd(C10, b);
         C01 = _mm_load_pd(pC1+4);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C11 = _mm_addsub_pd(C11, b);
         C02 = _mm_load_pd(pC2+4);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C12 = _mm_addsub_pd(C12, b);
/*
 *    K=1, M=1, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA11*rB10, rA11*rB10 */
      C10 = _mm_add_pd(C10, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA11, iA11 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C11 = _mm_add_pd(C11, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C12 = _mm_add_pd(C12, b);
         A = _mm_load_pd(pA0+4);               		/* iA20, rA20 */
/*
 *    K=1, M=1, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA11*iB10, iA11*iB10 */
      C10 = _mm_addsub_pd(C10, b);
         _mm_store_pd(pC0+2, C10);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C11 = _mm_addsub_pd(C11, b);
         _mm_store_pd(pC1+2, C11);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C12 = _mm_addsub_pd(C12, b);
         _mm_store_pd(pC2+2, C12);   
   }
/*
 * Drain pipes
 */
   {
      register __m128d b;
/*
 *    K=0, M=[0,1], apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA00*rB00, rA00*rB00 */
      C00 = _mm_add_pd(C00, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA00, iA00 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA1);                		/* iA01, rA01 */
/*
 *    K=0, M=0, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA00*iB00, iA00*iB00 */
      C00 = _mm_addsub_pd(C00, b);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
/*
 *    K=1, M=0, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA01*rB10, rA01*rB10 */
      C00 = _mm_add_pd(C00, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA01, iA01 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
/*
 *    K=1, M=0, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA01*iB10, iA01*iB10 */
      C00 = _mm_addsub_pd(C00, b);
         _mm_store_pd(pC0, C00);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         _mm_store_pd(pC1, C01);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
         _mm_store_pd(pC2, C02);   
      if (!(M&1))
      {
         C10 = _mm_load_pd(pC0+2);
         C11 = _mm_load_pd(pC1+2);
         C12 = _mm_load_pd(pC2+2);
         A = _mm_load_pd(pA0+2);                /* iA10, rA10 */
/*
 *       K=0, M=1, apply real components of B0x
 */
         b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
         b = _mm_mul_pd(b, A);                     /* iA10*rB00, rA10*rB00 */
         C10 = _mm_add_pd(C10, b);
            a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA10, iA10 */
         b = _mm_movedup_pd(B01);	
         b = _mm_mul_pd(b, A);    
         C11 = _mm_add_pd(C11, b);
         b = _mm_movedup_pd(B02);	
         b = _mm_mul_pd(b, A);    
         C12 = _mm_add_pd(C12, b);
            A = _mm_load_pd(pA1+2);               		/* iA11, rA11 */
/*
 *       K=0, M=1, apply imaginary components of B0x
 */
         b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
         b = _mm_mul_pd(b, a);                     /* rA10*iB00, iA10*iB00 */
         C10 = _mm_addsub_pd(C10, b);
         b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C11 = _mm_addsub_pd(C11, b);
         b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C12 = _mm_addsub_pd(C12, b);
/*
 *       K=1, M=1, apply real components of B1x
 */
         b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
         b = _mm_mul_pd(b, A);                     /* iA11*rB10, rA11*rB10 */
         C10 = _mm_add_pd(C10, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA11, iA11 */
         b = _mm_movedup_pd(B11);	
         b = _mm_mul_pd(b, A);    
         C11 = _mm_add_pd(C11, b);
         b = _mm_movedup_pd(B12);	
         b = _mm_mul_pd(b, A);    
         C12 = _mm_add_pd(C12, b);
/*
 *       K=1, M=1, apply imaginary components of B1x
 */
         b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
         b = _mm_mul_pd(b, a);                     /* rA11*iB10, iA11*iB10 */
         C10 = _mm_addsub_pd(C10, b);
            _mm_store_pd(pC0+2, C10);   
         b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C11 = _mm_addsub_pd(C11, b);
            _mm_store_pd(pC1+2, C11);   
         b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C12 = _mm_addsub_pd(C12, b);
            _mm_store_pd(pC2+2, C12);   
      }
   }
}
#elif defined(ATL_SSE3) && defined(SCPLX)
   #define NRHS 4
   #define TRU  4
   #include <xmmintrin.h>
   #include <pmmintrin.h>
/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=4, ku=2.  This version is for 16 SSE regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                           TYPE *pB0, ATL_CINT ldb0, TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   const TYPE *pA1 = pA0+lda;
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1), *pC3 = pC2+ldc;
   ATL_INT i;
   ATL_CINT MM = (M&2) ? M-2 : M-4;
   register __m128 B00, B01, B02, B03;
   register __m128 C00, C01, C02, C03;
   register __m128 C20, C21, C22, C23;
   register __m128 A, a;

   A = _mm_set1_ps(ATL_rnone);
   B00 = _mm_load_ps(pB0);              /* iB10 rB10 iB00 rB00 */
   B00 = _mm_mul_ps(B00, A);            /* negate */
   B01 = _mm_load_ps(pB0+ldb);
   B01 = _mm_mul_ps(B01, A);           
   B02 = _mm_load_ps(pB0+ldb+ldb);
   B02 = _mm_mul_ps(B02, A);          
   B03 = _mm_load_ps(pB0+ldb+ldb+ldb);
   B03 = _mm_mul_ps(B03, A);         

   C00 = _mm_load_ps(pC0);
   C01 = _mm_load_ps(pC1);
   C02 = _mm_load_ps(pC2);
   C03 = _mm_load_ps(pC3);
   A = _mm_load_ps(pA0);                /* iA10 rA10 iA00 rA00 */
   a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
       /* rA10 iA10 rA00 iA00 */
   for (i=0; i < MM; i += 4, pA0 += 8, pA1 += 8, 
        pC0 += 8, pC1 += 8, pC2 += 8, pC3 += 8)
   {
      register __m128 b;
/*
 *    M=K=0 block, multiply by real B vals
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x00);
                                        /* rB00 rB00 rB00 rB00 */
      b = _mm_mul_ps(b, A);             
      C00 = _mm_add_ps(C00, b);  C20 = _mm_load_ps(pC0+4);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x00); 
      b = _mm_mul_ps(b, A);             
      C01 = _mm_add_ps(C01, b);  C21 = _mm_load_ps(pC1+4);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x00);
      b = _mm_mul_ps(b, A);             
      C02 = _mm_add_ps(C02, b);  C22 = _mm_load_ps(pC2+4);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x00);
      b = _mm_mul_ps(b, A);             
      C03 = _mm_add_ps(C03, b); A = _mm_load_ps(pA1);
/*
 *    M=K=0 block, multiply by imaginary B vals
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x55); 
                                        /* iB00 iB00, iB00, iB00 */
      b = _mm_mul_ps(b, a);
      C00 = _mm_addsub_ps(C00, b);  C23 = _mm_load_ps(pC3+4);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x55);
      b = _mm_mul_ps(b, a);             
      C01 = _mm_addsub_ps(C01, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x55);
      b = _mm_mul_ps(b, a);             
      C02 = _mm_addsub_ps(C02, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x55);
      b = _mm_mul_ps(b, a);             
      C03 = _mm_addsub_ps(C03, b); 
      a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
/*
 *    M=0,K=1, multiply by real B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xAA); 
                                                /* rB10 rB10, rB10, rBi0 */
      b = _mm_mul_ps(b, A);             
      C00 = _mm_add_ps(C00, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xAA);
      b = _mm_mul_ps(b, A);             
      C01 = _mm_add_ps(C01, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xAA);
      b = _mm_mul_ps(b, A);             
      C02 = _mm_add_ps(C02, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xAA);
      b = _mm_mul_ps(b, A);             
      C03 = _mm_add_ps(C03, b); A = _mm_load_ps(pA0+4);
/*
 *    M=0,K=1, multiply by imaginary B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xFF); 
                                                /* iB10 iB10, iB10, iB10 */
      b = _mm_mul_ps(b, a);
      C00 = _mm_addsub_ps(C00, b); _mm_store_ps(pC0, C00);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xFF);
      b = _mm_mul_ps(b, a);             
      C01 = _mm_addsub_ps(C01, b); _mm_store_ps(pC1, C01);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xFF);
      b = _mm_mul_ps(b, a);             
      C02 = _mm_addsub_ps(C02, b); _mm_store_ps(pC2, C02);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xFF);
      b = _mm_mul_ps(b, a);             
      C03 = _mm_addsub_ps(C03, b); 
      a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
/*
 *    M=2,K=0 block, multiply by real B vals
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x00);
                                                /* rB00 rB00 rB00 rB00 */
      b = _mm_mul_ps(b, A);             
      C20 = _mm_add_ps(C20, b); _mm_store_ps(pC3, C03);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x00);
      b = _mm_mul_ps(b, A);             
      C21 = _mm_add_ps(C21, b);  C00 = _mm_load_ps(pC0+8);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x00);
      b = _mm_mul_ps(b, A);             
      C22 = _mm_add_ps(C22, b);  C01 = _mm_load_ps(pC1+8);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x00);
      b = _mm_mul_ps(b, A);             
      C23 = _mm_add_ps(C23, b); A = _mm_load_ps(pA1+4);
/*
 *    M=2,K=0 block, multiply by imaginary B vals
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x55);
                                                /* iB00 iB00, iB00, iB00 */
      b = _mm_mul_ps(b, a);
      C20 = _mm_addsub_ps(C20, b);  C02 = _mm_load_ps(pC2+8);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x55);
      b = _mm_mul_ps(b, a);             
      C21 = _mm_addsub_ps(C21, b);  C03 = _mm_load_ps(pC3+8);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x55);
      b = _mm_mul_ps(b, a);             
      C22 = _mm_addsub_ps(C22, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x55);
      b = _mm_mul_ps(b, a);             
      C23 = _mm_addsub_ps(C23, b); 
      a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
/*
 *    M=2, K=1, multiply by real B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xAA);     
                                                /* rB10 rB10, rB10, rBi0 */
      b = _mm_mul_ps(b, A);             
      C20 = _mm_add_ps(C20, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xAA);
      b = _mm_mul_ps(b, A);             
      C21 = _mm_add_ps(C21, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xAA);
      b = _mm_mul_ps(b, A);             
      C22 = _mm_add_ps(C22, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xAA);
      b = _mm_mul_ps(b, A);             
      C23 = _mm_add_ps(C23, b); A = _mm_load_ps(pA0+8);
/*
 *    M=2, K=1, multiply by imaginary B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xFF);
                                                /* iB10 iB10, iB10, iB10 */
      b = _mm_mul_ps(b, a);
      C20 = _mm_addsub_ps(C20, b);  _mm_store_ps(pC0+4, C20);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xFF);
      b = _mm_mul_ps(b, a);             
      C21 = _mm_addsub_ps(C21, b);  _mm_store_ps(pC1+4, C21);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xFF);
      b = _mm_mul_ps(b, a);             
      C22 = _mm_addsub_ps(C22, b);  _mm_store_ps(pC2+4, C22);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xFF);
      b = _mm_mul_ps(b, a);             
      C23 = _mm_addsub_ps(C23, b);  _mm_store_ps(pC3+4, C23);
      a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
   }
/*
 * Drain pipe
 */
   {
      register __m128 b;
/*
 *    M=K=0 block, multiply by real B vals
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x00);
          /* rB00 rB00 rB00 rB00 */
      b = _mm_mul_ps(b, A); 
          /* iA10*rB00 rA10*rB00 iA00*rB00 rA00*rB00 */
      C00 = _mm_add_ps(C00, b);  
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x00); 
      b = _mm_mul_ps(b, A);             
      C01 = _mm_add_ps(C01, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x00);
      b = _mm_mul_ps(b, A);             
      C02 = _mm_add_ps(C02, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x00);
      b = _mm_mul_ps(b, A);             
      C03 = _mm_add_ps(C03, b); A = _mm_load_ps(pA1);
/*
 *    M=K=0 block, multiply by imaginary B vals
 */
      /* a = rA10 iA10 rA00 iA00 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x55); 
                                        /* iB00 iB00, iB00, iB00 */
      b = _mm_mul_ps(b, a); /* rA10*iB00 iA10*iB00 rA00*iB00 iA00*iB00 */
      C00 = _mm_addsub_ps(C00, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x55);
      b = _mm_mul_ps(b, a);             
      C01 = _mm_addsub_ps(C01, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x55);
      b = _mm_mul_ps(b, a);             
      C02 = _mm_addsub_ps(C02, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x55);
      b = _mm_mul_ps(b, a);             
      C03 = _mm_addsub_ps(C03, b); 
      a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
/*
 *    M=0,K=1, multiply by real B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xAA); 
                                                /* rB10 rB10, rB10, rB10 */
      b = _mm_mul_ps(b, A);             
      C00 = _mm_add_ps(C00, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xAA);
      b = _mm_mul_ps(b, A);             
      C01 = _mm_add_ps(C01, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xAA);
      b = _mm_mul_ps(b, A);             
      C02 = _mm_add_ps(C02, b);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xAA);
      b = _mm_mul_ps(b, A);             
      C03 = _mm_add_ps(C03, b);
/*
 *    M=0,K=1, multiply by imaginary B values
 */
      b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xFF); 
                                                /* iB10 iB10, iB10, iB10 */
      b = _mm_mul_ps(b, a);
      C00 = _mm_addsub_ps(C00, b); _mm_store_ps(pC0, C00);
      b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xFF);
      b = _mm_mul_ps(b, a);             
      C01 = _mm_addsub_ps(C01, b); _mm_store_ps(pC1, C01);
      b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xFF);
      b = _mm_mul_ps(b, a);             
      C02 = _mm_addsub_ps(C02, b); _mm_store_ps(pC2, C02);
      b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xFF);
      b = _mm_mul_ps(b, a);             
      C03 = _mm_addsub_ps(C03, b);  _mm_store_ps(pC3, C03);
/*
 *    M=2,K=0 block, multiply by real B vals
 */
      if (!(M&2))
      {
         A = _mm_load_ps(pA0+4);
         a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
         C20 = _mm_load_ps(pC0+4);
         C21 = _mm_load_ps(pC1+4);
         b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x00);
         C22 = _mm_load_ps(pC2+4);
         C23 = _mm_load_ps(pC3+4); 
                                                   /* rB00 rB00 rB00 rB00 */
         b = _mm_mul_ps(b, A);             
         C20 = _mm_add_ps(C20, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x00);
         b = _mm_mul_ps(b, A);             
         C21 = _mm_add_ps(C21, b); 
         b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x00);
         b = _mm_mul_ps(b, A);             
         C22 = _mm_add_ps(C22, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x00);
         b = _mm_mul_ps(b, A);             
         C23 = _mm_add_ps(C23, b); A = _mm_load_ps(pA1+4);
/*
 *       M=2,K=0 block, multiply by imaginary B vals
 */
         b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0x55);
                                                   /* iB00 iB00, iB00, iB00 */
         b = _mm_mul_ps(b, a);
         C20 = _mm_addsub_ps(C20, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0x55);
         b = _mm_mul_ps(b, a);             
         C21 = _mm_addsub_ps(C21, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0x55);
         b = _mm_mul_ps(b, a);             
         C22 = _mm_addsub_ps(C22, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0x55);
         b = _mm_mul_ps(b, a);             
         C23 = _mm_addsub_ps(C23, b); 
         a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
/*
 *       M=2, K=1, multiply by real B values
 */
         b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xAA);     
                                                   /* rB10 rB10, rB10, rBi0 */
         b = _mm_mul_ps(b, A);             
         C20 = _mm_add_ps(C20, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xAA);
         b = _mm_mul_ps(b, A);             
         C21 = _mm_add_ps(C21, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xAA);
         b = _mm_mul_ps(b, A);             
         C22 = _mm_add_ps(C22, b);
         b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xAA);
         b = _mm_mul_ps(b, A);             
         C23 = _mm_add_ps(C23, b);
/*
 *       M=2, K=1, multiply by imaginary B values
 */
         b = (__m128)_mm_shuffle_epi32((__m128i)B00, 0xFF);
                                                   /* iB10 iB10, iB10, iB10 */
         b = _mm_mul_ps(b, a);
         C20 = _mm_addsub_ps(C20, b);  _mm_store_ps(pC0+4, C20);
         b = (__m128)_mm_shuffle_epi32((__m128i)B01, 0xFF);
         b = _mm_mul_ps(b, a);             
         C21 = _mm_addsub_ps(C21, b);  _mm_store_ps(pC1+4, C21);
         b = (__m128)_mm_shuffle_epi32((__m128i)B02, 0xFF);
         b = _mm_mul_ps(b, a);             
         C22 = _mm_addsub_ps(C22, b);  _mm_store_ps(pC2+4, C22);
         b = (__m128)_mm_shuffle_epi32((__m128i)B03, 0xFF);
         b = _mm_mul_ps(b, a);             
         C23 = _mm_addsub_ps(C23, b);  _mm_store_ps(pC3+4, C23);
         a = (__m128)_mm_shuffle_epi32((__m128i)A, 0xB1);
      }
   }
}
#else   /* 32 register version goes here */
   #define NRHS 3
   #define TRU  2   /* this kernel requires no padding, but solve needs 2 */
/*
 * This routine optimized for OOE machines; for statically scheduled
 * machines, the asg of rCxx followed immediately by the dependent store will
 * be bad news.  Would need to pipeline another loop iteration to avoid.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, ATL_CINT lda0, 
                         TYPE *B, ATL_CINT ldb0, TYPE *pC0, ATL_CINT ldc0) 
{
   ATL_CINT lda=lda0+lda0, ldb=ldb0+ldb0, ldc=ldc0+ldc0;
   ATL_CINT MM =  (M & 1) ? M-1 : M-2;
   ATL_INT i;
   TYPE *pC1=pC0+ldc, *pC2=pC0+(ldc<<1), *pC3=pC1+(ldc<<1);
   const TYPE *pA1=pA0+lda;
   const RTYPE rB00=(*B), iB00=B[1], rB01=B[ldb], iB01=B[ldb+1],
               rB02=B[ldb+ldb], iB02=B[ldb+ldb+1];
   const RTYPE rB10=B[2], iB10=B[3], rB11=B[ldb+2], iB11=B[ldb+3],
               rB12=B[ldb+ldb+2], iB12=B[ldb+ldb+3];
   RTYPE rC00, rC01, rC02, rC10, rC11, rC12;
   RTYPE iC00, iC01, iC02, iC10, iC11, iC12;
   RTYPE rA00, rA10, rA01, rA11, iA00, iA10, iA01, iA11;

/*
 * Fetch C and A for M=0
 */
   rC00 = *pC0; iC00 = pC0[1];
   rC01 = *pC1; iC01 = pC1[1];
   rC02 = *pC2; iC02 = pC2[1];
   rA00 = *pA0; iA00 = pA0[1];
   rA01 = *pA1; iA01 = pA1[1];
   for (i=0; i < MM; i += 2, pA0 += 4, pA1 += 4, pC0 += 4, pC1 += 4, pC2 += 4)
   {
/*
 *    M=0, K=0, fetch M=1 C's and then K=1's A's
 */
      rC00 -= rA00 * rB00; rC10 = pC0[2];
      iC00 -= rA00 * iB00; iC10 = pC0[3];
      rC01 -= rA00 * rB01; rC11 = pC1[2];
      iC01 -= rA00 * iB01; iC11 = pC1[3];
      rC02 -= rA00 * rB02; rC12 = pC2[2];
      iC02 -= rA00 * iB02; iC12 = pC2[3];

      rC00 += iA00 * iB00; rA10 = pA0[2];
      iC00 -= iA00 * rB00; iA10 = pA0[3];
      rC01 += iA00 * iB01; rA11 = pA1[2];
      iC01 -= iA00 * rB01; iA11 = pA1[3];
      rC02 += iA00 * iB02;
      iC02 -= iA00 * rB02;
/*
 *    K == 1, and then finished, so store C out
 */
      rC00 -= rA01 * rB10;
      iC00 -= rA01 * iB10;
      rC01 -= rA01 * rB11;
      iC01 -= rA01 * iB11;
      rC02 -= rA01 * rB12;
      iC02 -= rA01 * iB12;

      rC00 += iA01 * iB10; *pC0 = rC00;
      iC00 -= iA01 * rB10; pC0[1] = iC00;
      rC01 += iA01 * iB11; *pC1 = rC01;
      iC01 -= iA01 * rB11; pC1[1] = iC01;
      rC02 += iA01 * iB12; *pC2 = rC02;
      iC02 -= iA01 * rB12; pC2[1] = iC02;
/*
 *    M=1, K=0, fetch M=2's C's and A's
 */
      rC10 -= rA10 * rB00; rC00 = pC0[4];
      iC10 -= rA10 * iB00; iC00 = pC0[5];
      rC11 -= rA10 * rB01; rC01 = pC1[4];
      iC11 -= rA10 * iB01; iC01 = pC1[5];
      rC12 -= rA10 * rB02; rC02 = pC2[4];
      iC12 -= rA10 * iB02; iC02 = pC2[5];

      rC10 += iA10 * iB00; rA00 = pA0[4];
      iC10 -= iA10 * rB00; iA00 = pA0[5];
      rC11 += iA10 * iB01; rA01 = pA1[4];
      iC11 -= iA10 * rB01; iA01 = pA1[5];
      rC12 += iA10 * iB02;
      iC12 -= iA10 * rB02;
/*
 *    M=1, K=1
 */
      rC10 -= rA11 * rB10;
      iC10 -= rA11 * iB10;
      rC11 -= rA11 * rB11;
      iC11 -= rA11 * iB11;
      rC12 -= rA11 * rB12;
      iC12 -= rA11 * iB12;

      rC10 += iA11 * iB10; pC0[2] = rC10;
      iC10 -= iA11 * rB10; pC0[3] = iC10;
      rC11 += iA11 * iB11; pC1[2] = rC11;
      iC11 -= iA11 * rB11; pC1[3] = iC11;
      rC12 += iA11 * iB12; pC2[2] = rC12;
      iC12 -= iA11 * rB12; pC2[3] = iC12;
   }
/*
 * Drain pipeline
 */
/*
 *    M=0, K=0
 */
   rC00 -= rA00 * rB00; 
   iC00 -= rA00 * iB00; 
   rC01 -= rA00 * rB01; 
   iC01 -= rA00 * iB01; 
   rC02 -= rA00 * rB02; 
   iC02 -= rA00 * iB02; 

   rC00 += iA00 * iB00; 
   iC00 -= iA00 * rB00; 
   rC01 += iA00 * iB01; 
   iC01 -= iA00 * rB01; 
   rC02 += iA00 * iB02;
   iC02 -= iA00 * rB02;
/*
 * K == 1, and then finished, so store C out
 */
   rC00 -= rA01 * rB10;
   iC00 -= rA01 * iB10;
   rC01 -= rA01 * rB11;
   iC01 -= rA01 * iB11;
   rC02 -= rA01 * rB12;
   iC02 -= rA01 * iB12;

   rC00 += iA01 * iB10; *pC0 = rC00;
   iC00 -= iA01 * rB10; pC0[1] = iC00;
   rC01 += iA01 * iB11; *pC1 = rC01;
   iC01 -= iA01 * rB11; pC1[1] = iC01;
   rC02 += iA01 * iB12; *pC2 = rC02;
   iC02 -= iA01 * rB12; pC2[1] = iC02;
/*
 * M=1, K=0, fetch M=2's C's and A's
 */
   if (!(M&1))
   {
      rC10 = pC0[2];
      iC10 = pC0[3];
      rC11 = pC1[2];
      iC11 = pC1[3];
      rC12 = pC2[2];
      iC12 = pC2[3];
      rA10 = pA0[2];
      iA10 = pA0[3];
      rA11 = pA1[2];
      iA11 = pA1[3];
      rC10 -= rA10 * rB00;
      iC10 -= rA10 * iB00;
      rC11 -= rA10 * rB01;
      iC11 -= rA10 * iB01;
      rC12 -= rA10 * rB02;
      iC12 -= rA10 * iB02;

      rC10 += iA10 * iB00;
      iC10 -= iA10 * rB00;
      rC11 += iA10 * iB01;
      iC11 -= iA10 * rB01;
      rC12 += iA10 * iB02;
      iC12 -= iA10 * rB02;
/*
 *    M=1, K=1
 */
      rC10 -= rA11 * rB10;
      iC10 -= rA11 * iB10;
      rC11 -= rA11 * rB11;
      iC11 -= rA11 * iB11;
      rC12 -= rA11 * rB12;
      iC12 -= rA11 * iB12;

      rC10 += iA11 * iB10; pC0[2] = rC10;
      iC10 -= iA11 * rB10; pC0[3] = iC10;
      rC11 += iA11 * iB11; pC1[2] = rC11;
      iC11 -= iA11 * rB11; pC1[3] = iC11;
      rC12 += iA11 * iB12; pC2[2] = rC12;
      iC12 -= iA11 * rB12; pC2[3] = iC12;
   }
}
#endif

@ROUT ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
#if NRHS == 3
/* 
 * Solve 2x2 L with 3 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmU2(const TYPE *U, ATL_CINT ldu, TYPE *B, ATL_CINT ldb)
{
   ATL_CINT ldU=ldu+ldu, ldB = ldb+ldb, ldB2=(ldb<<2);
   const RTYPE rU00=(*U), iU00=U[1], rU01=U[ldU], iU01=U[ldU+1];
   const RTYPE rU11=U[ldU+2], iU11 = U[ldU+3];
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldB], iB01=B[ldB+1], rB11=B[ldB+2], iB11=B[ldB+3];
   RTYPE rB02=B[ldB2], iB02=B[ldB2+1], rB12=B[ldB2+2], iB12=B[ldB2+3];
   RTYPE rX;
/*
 * x1 = b1 / U11;  U11 is recipricol, so solve x1 = b1 * U11;
 */
   B[2] = rX = rB10 * rU11 - iB10 * iU11;
   B[3] = iB10 = rB10 * iU11 + iB10 * rU11; rB10 = rX;
   B[ldB+2] = rX = rB11 * rU11 - iB11 * iU11;
   B[ldB+3] = iB11 = rB11 * iU11 + iB11 * rU11; rB11 = rX;
   B[ldB2+2] = rX = rB12 * rU11 - iB12 * iU11;
   B[ldB2+3] = iB12 = rB12 * iU11 + iB12 * rU11; rB12 = rX;
/*
 * x0 = (b0 - U01*x1) / U00, do x0 = (b0 - U01*x1) first
 */
   rB00 = rB00 - rU01*rB10 + iU01*iB10;
   iB00 = iB00 - rU01*iB10 - iU01*rB10;
   rB01 = rB01 - rU01*rB11 + iU01*iB11;
   iB01 = iB01 - rU01*iB11 - iU01*rB11;
   rB02 = rB02 - rU01*rB12 + iU01*iB12;
   iB02 = iB02 - rU01*iB12 - iU01*rB12;
/*
 * Finish x0 = (b0 - U01*x1) / U00 by multiplying by U00 (1/U00)
 */
   *B = rB00 * rU00 - iB00 * iU00;
   B[1] = rB00 * iU00 + iB00 * rU00;
   B[ldB]   = rB01 * rU00 - iB01 * iU00;
   B[ldB+1] = rB01 * iU00 + iB01 * rU00;
   B[ldB2]   = rB02 * rU00 - iB02 * iU00;
   B[ldB2+1] = rB02 * iU00 + iB02 * rU00;
}

/* 
 * Solve 2x2 L with 3 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmL2(const TYPE *L, ATL_CINT ldl, TYPE *B, ATL_CINT ldb0)
{
   const RTYPE rL00=(*L), iL00=L[1], rL10=L[2], iL10=L[3], 
               rL11=L[ldl+ldl+2], iL11=L[ldl+ldl+3];
   ATL_CINT ldb=ldb0+ldb0, ldb2=(ldb0<<2);
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldb], iB01=B[ldb+1], rB11=B[ldb+2], iB11=B[ldb+3];
   RTYPE rB02=B[ldb2], iB02=B[ldb2+1], rB12=B[ldb2+2], iB12=B[ldb2+3];
   RTYPE rX;
/*
 * x0 = b0 / L00 
 */
   *B        = rX = rB00*rL00 - iB00*iL00;
   B[1]      = iB00 = rB00*iL00 + iB00*rL00; rB00 = rX;
   B[ldb]    = rX = rB01*rL00 - iB01*iL00;
   B[ldb+1]  = iB01 = rB01*iL00 + iB01*rL00; rB01 = rX;
   B[ldb2]   = rX = rB02*rL00 - iB02*iL00;
   B[ldb2+1] = iB02 = rB02*iL00 + iB02*rL00; rB02 = rX;
/*
 * x1 = (b1 - L10 * x0)  [divide by diagonal in next step]
 */
   rB10 = (rB10 - rL10*rB00 + iL10*iB00);
   iB10 = (iB10 - rL10*iB00 - iL10*rB00);
   rB11 = (rB11 - rL10*rB01 + iL10*iB01);
   iB11 = (iB11 - rL10*iB01 - iL10*rB01);
   rB12 = (rB12 - rL10*rB02 + iL10*iB02);
   iB12 = (iB12 - rL10*iB02 - iL10*rB02);
/*
 * Mult by recipricol of L11 to finish x1 = (b1 - L10 * x0)/L11.
 */
   B[2]      = rB10*rL11 - iB10*iL11;
   B[3]      = rB10*iL11 + iB10*rL11;
   B[ldb+2]  = rB11*rL11 - iB11*iL11;
   B[ldb+3]  = rB11*iL11 + iB11*rL11;
   B[ldb2+2] = rB12*rL11 - iB12*iL11;
   B[ldb2+3] = iB12 = rB12*iL11 + iB12*rL11;
}
#elif NRHS == 4
/* 
 * Solve 2x2 L with 4 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmU2(const TYPE *U, ATL_CINT ldu, TYPE *B, ATL_CINT ldb)
{
   ATL_CINT ldU=ldu+ldu, ldB = ldb+ldb, ldB2=(ldb<<2), ldB3=ldB+ldB2;
   const RTYPE rU00=(*U), iU00=U[1], rU01=U[ldU], iU01=U[ldU+1];
   const RTYPE rU11=U[ldU+2], iU11 = U[ldU+3];
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldB], iB01=B[ldB+1], rB11=B[ldB+2], iB11=B[ldB+3];
   RTYPE rB02=B[ldB2], iB02=B[ldB2+1], rB12=B[ldB2+2], iB12=B[ldB2+3];
   RTYPE rB03=B[ldB3], iB03=B[ldB3+1], rB13=B[ldB3+2], iB13=B[ldB3+3];
   RTYPE rX;
/*
 * x1 = b1 / U11;  U11 is recipricol, so solve x1 = b1 * U11;
 */
   B[2] = rX = rB10 * rU11 - iB10 * iU11;
   B[3] = iB10 = rB10 * iU11 + iB10 * rU11; rB10 = rX;
   B[ldB+2] = rX = rB11 * rU11 - iB11 * iU11;
   B[ldB+3] = iB11 = rB11 * iU11 + iB11 * rU11; rB11 = rX;
   B[ldB2+2] = rX = rB12 * rU11 - iB12 * iU11;
   B[ldB2+3] = iB12 = rB12 * iU11 + iB12 * rU11; rB12 = rX;
   B[ldB3+2] = rX = rB13 * rU11 - iB13 * iU11;
   B[ldB3+3] = iB13 = rB13 * iU11 + iB13 * rU11; rB13 = rX;
/*
 * x0 = (b0 - U01*x1) / U00, do x0 = (b0 - U01*x1) first
 */
   rB00 = rB00 - rU01*rB10 + iU01*iB10;
   iB00 = iB00 - rU01*iB10 - iU01*rB10;
   rB01 = rB01 - rU01*rB11 + iU01*iB11;
   iB01 = iB01 - rU01*iB11 - iU01*rB11;
   rB02 = rB02 - rU01*rB12 + iU01*iB12;
   iB02 = iB02 - rU01*iB12 - iU01*rB12;
   rB03 = rB03 - rU01*rB13 + iU01*iB13;
   iB03 = iB03 - rU01*iB13 - iU01*rB13;
/*
 * Finish x0 = (b0 - U01*x1) / U00 by multiplying by U00 (1/U00)
 */
   *B = rB00 * rU00 - iB00 * iU00;
   B[1] = rB00 * iU00 + iB00 * rU00;
   B[ldB]   = rB01 * rU00 - iB01 * iU00;
   B[ldB+1] = rB01 * iU00 + iB01 * rU00;
   B[ldB2]   = rB02 * rU00 - iB02 * iU00;
   B[ldB2+1] = rB02 * iU00 + iB02 * rU00;
   B[ldB3]   = rB03 * rU00 - iB03 * iU00;
   B[ldB3+1] = rB03 * iU00 + iB03 * rU00;
}

/* 
 * Solve 2x2 L with 4 RHS symbolically for complex arithmetic; 
 * Diagonal elements have already been inverted
 */
ATL_SINLINE void ATL_trsmL2(const TYPE *L, ATL_CINT ldl, TYPE *B, ATL_CINT ldb0)
{
   const RTYPE rL00=(*L), iL00=L[1], rL10=L[2], iL10=L[3], 
               rL11=L[ldl+ldl+2], iL11=L[ldl+ldl+3];
   ATL_CINT ldb=ldb0+ldb0, ldb2=(ldb0<<2), ldb3=ldb+ldb2;
   RTYPE rB00=(*B), iB00=B[1], rB10=B[2], iB10=B[3];
   RTYPE rB01=B[ldb], iB01=B[ldb+1], rB11=B[ldb+2], iB11=B[ldb+3];
   RTYPE rB02=B[ldb2], iB02=B[ldb2+1], rB12=B[ldb2+2], iB12=B[ldb2+3];
   RTYPE rB03=B[ldb3], iB03=B[ldb3+1], rB13=B[ldb3+2], iB13=B[ldb3+3];
   RTYPE rX;
/*
 * x0 = b0 / L00 
 */
   *B        = rX = rB00*rL00 - iB00*iL00;
   B[1]      = iB00 = rB00*iL00 + iB00*rL00; rB00 = rX;
   B[ldb]    = rX = rB01*rL00 - iB01*iL00;
   B[ldb+1]  = iB01 = rB01*iL00 + iB01*rL00; rB01 = rX;
   B[ldb2]   = rX = rB02*rL00 - iB02*iL00;
   B[ldb2+1] = iB02 = rB02*iL00 + iB02*rL00; rB02 = rX;
   B[ldb3]   = rX = rB03*rL00 - iB03*iL00;
   B[ldb3+1] = iB03 = rB03*iL00 + iB03*rL00; rB03 = rX;
/*
 * x1 = (b1 - L10 * x0)  [divide by diagonal in next step]
 */
   rB10 = (rB10 - rL10*rB00 + iL10*iB00);
   iB10 = (iB10 - rL10*iB00 - iL10*rB00);
   rB11 = (rB11 - rL10*rB01 + iL10*iB01);
   iB11 = (iB11 - rL10*iB01 - iL10*rB01);
   rB12 = (rB12 - rL10*rB02 + iL10*iB02);
   iB12 = (iB12 - rL10*iB02 - iL10*rB02);
   rB13 = (rB13 - rL10*rB03 + iL10*iB03);
   iB13 = (iB13 - rL10*iB03 - iL10*rB03);
/*
 * Mult by recipricol of L11 to finish x1 = (b1 - L10 * x0)/L11.
 */
   B[2]      = rB10*rL11 - iB10*iL11;
   B[3]      = rB10*iL11 + iB10*rL11;
   B[ldb+2]  = rB11*rL11 - iB11*iL11;
   B[ldb+3]  = rB11*iL11 + iB11*rL11;
   B[ldb2+2] = rB12*rL11 - iB12*iL11;
   B[ldb2+3] = iB12 = rB12*iL11 + iB12*rL11;
   B[ldb3+2] = rB13*rL11 - iB13*iL11;
   B[ldb3+3] = iB13 = rB13*iL11 + iB13*rL11;
}
#endif

@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4
#if defined(ATL_AVX) && defined(DREAL)
   #define NRHS 3
   #define ATL_BINWRK 1
   #include <immintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=4, nu=3, ku=4.  This version is for 16 AVX regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   ATL_CINT MM =  (M & 4) ? M-4 : M-8; 
   int i; 
   register __m256d rB00, rB01, rB02; 
   register __m256d rB20, rB21, rB22;
   register __m256d rC00, rC01, rC02;
   register __m256d rC40, rC41, rC42;
   register __m256d rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm256_broadcast_pd((void*)pB0);              /* B10 B00 B10 B00 */
   rB20 = _mm256_broadcast_pd((void*)(pB0+2));
   rB01 = _mm256_broadcast_pd((void*)(pB0+ldb));
   rB21 = _mm256_broadcast_pd((void*)(pB0+ldb+2));
   rB02 = _mm256_broadcast_pd((void*)(pB0+ldb+ldb));
   rB22 = _mm256_broadcast_pd((void*)(pB0+ldb+ldb+2));

   rC00 = _mm256_load_pd(pC0);                          /* C30 C20 C10 C00 */
   rC01 = _mm256_load_pd(pC1);
   rC02 = _mm256_load_pd(pC2);
   rA0  = _mm256_load_pd(pA0);                          /* A30 A20 A10, A00 */
   for (i=0; i < MM; i += 8, pA0 += 8, pA1 += 8, pA2 += 8, pA3 += 8,
        pC0 += 8, pC1 += 8, pC2 += 8)
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      rC40 =_mm256_load_pd(pC0+4);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA0, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB);
      #endif 
      rC41 =_mm256_load_pd(pC1+4);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      rC42 =_mm256_load_pd(pC2+4);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB22, rB22);
      rB = _mm256_mul_pd(rB, rA0);
      rC02 = _mm256_sub_pd(rC02, rB); rA0 = _mm256_load_pd(pA0+4);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA1 = _mm256_load_pd(pA1+4);
/*
 *    2nd row of C regs
 */
      rB = _mm256_unpacklo_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA0, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC40 = _mm256_sub_pd(rC40, rB); 
      #endif
      _mm256_store_pd(pC2, rC02);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA0, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC41 = _mm256_sub_pd(rC41, rB); 
      #endif
      rC00 = _mm256_load_pd(pC0+8);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA0, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC42 = _mm256_sub_pd(rC42, rB); 
      #endif
      rA0 = _mm256_load_pd(pA2+4);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA1, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC40 = _mm256_sub_pd(rC40, rB); 
      #endif
      rC01 = _mm256_load_pd(pC1+8);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA1, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC41 = _mm256_sub_pd(rC41, rB); 
      #endif
      rC02 = _mm256_load_pd(pC2+8);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA1, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC42 = _mm256_sub_pd(rC42, rB);
      #endif 
      rA1 = _mm256_load_pd(pA3+4);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA0, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC40 = _mm256_sub_pd(rC40, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA0, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC41 = _mm256_sub_pd(rC41, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA0, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC42 = _mm256_sub_pd(rC42, rB);
      #endif
      rA0 = _mm256_load_pd(pA0+8);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA1, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC40 = _mm256_sub_pd(rC40, rB);
      #endif
      _mm256_store_pd(pC0+4, rC40);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA1, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC41 = _mm256_sub_pd(rC41, rB); 
      #endif
      _mm256_store_pd(pC1+4, rC41);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA1, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC42 = _mm256_sub_pd(rC42, rB); 
      #endif
      _mm256_store_pd(pC2+4, rC42);
   }
/*
 * Drain C load/use pipeline
 */
   if (M-MM == 4)   /* drain pipe over 1 iteration */
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA0, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB);
      #endif
      rB = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB);
      #endif
      rB = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA0, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC02 = _mm256_sub_pd(rC02, rB);
      #endif

      rB = _mm256_unpackhi_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      _mm256_store_pd(pC2, rC02);
   }
   else /* M-MM = 8, drain pipe over 2 iterations */
   {
      register __m256d rB;

      rB = _mm256_unpacklo_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      rA1 = _mm256_load_pd(pA1);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
         rC40 =_mm256_load_pd(pC0+4);
      rB = _mm256_unpacklo_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA0, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA0 = _mm256_load_pd(pA2);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      rC41 =_mm256_load_pd(pC1+4);
      rB = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      rC42 =_mm256_load_pd(pC2+4);
      rB = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA1 = _mm256_load_pd(pA3);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA0, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC00 = _mm256_sub_pd(rC00, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA0, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC01 = _mm256_sub_pd(rC01, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA0, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA0 = _mm256_load_pd(pA0+4);

      rB = _mm256_unpackhi_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC00 = _mm256_fnmadd_pd(rB, rA1, rC00);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC00 = _mm256_sub_pd(rC00, rB); 
      #endif
      _mm256_store_pd(pC0, rC00);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC01 = _mm256_fnmadd_pd(rB, rA1, rC01);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC01 = _mm256_sub_pd(rC01, rB); 
      #endif
      _mm256_store_pd(pC1, rC01);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC02 = _mm256_fnmadd_pd(rB, rA1, rC02);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC02 = _mm256_sub_pd(rC02, rB); 
      #endif
      rA1 = _mm256_load_pd(pA1+4);
/*
 *    2nd row of C regs
 */
      rB = _mm256_unpacklo_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA0, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC40 = _mm256_sub_pd(rC40, rB); 
      #endif
      _mm256_store_pd(pC2, rC02);
      rB = _mm256_unpacklo_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA0, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC41 = _mm256_sub_pd(rC41, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA0, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC42 = _mm256_sub_pd(rC42, rB); 
      #endif
      rA0 = _mm256_load_pd(pA2+4);

      rB = _mm256_unpackhi_pd(rB00, rB00);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA1, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC40 = _mm256_sub_pd(rC40, rB);
      #endif
      rB = _mm256_unpackhi_pd(rB01, rB01);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA1, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC41 = _mm256_sub_pd(rC41, rB);
      #endif
      rB = _mm256_unpackhi_pd(rB02, rB02);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA1, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC42 = _mm256_sub_pd(rC42, rB); 
      #endif
      rA1 = _mm256_load_pd(pA3+4);

      rB = _mm256_unpacklo_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA0, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC40 = _mm256_sub_pd(rC40, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA0, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC41 = _mm256_sub_pd(rC41, rB);
      #endif
      rB = _mm256_unpacklo_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA0, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA0);
         rC42 = _mm256_sub_pd(rC42, rB);
      #endif

      rB = _mm256_unpackhi_pd(rB20, rB20);
      #ifdef ATL_AVXMAC
         rC40 = _mm256_fnmadd_pd(rB, rA1, rC40);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC40 = _mm256_sub_pd(rC40, rB); 
      #endif
      _mm256_store_pd(pC0+4, rC40);
      rB = _mm256_unpackhi_pd(rB21, rB21);
      #ifdef ATL_AVXMAC
         rC41 = _mm256_fnmadd_pd(rB, rA1, rC41);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC41 = _mm256_sub_pd(rC41, rB); 
      #endif
      _mm256_store_pd(pC1+4, rC41);
      rB = _mm256_unpackhi_pd(rB22, rB22);
      #ifdef ATL_AVXMAC
         rC42 = _mm256_fnmadd_pd(rB, rA1, rC42);
      #else
         rB = _mm256_mul_pd(rB, rA1);
         rC42 = _mm256_sub_pd(rC42, rB); 
      #endif
      _mm256_store_pd(pC2+4, rC42);
   }
}
#elif defined(ATL_SSE2) && defined(DREAL)
   #define NRHS 3
   #define ATL_BINWRK 1
   #include <xmmintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=4, nu=3, ku=4.  This version is for 16 SSE2 regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   const int MM = M-4; 
   int i; 
   register __m128d rB00, rB01, rB02; 
   register __m128d rB20, rB21, rB22;
   register __m128d rC00, rC01, rC02;
   register __m128d rC20, rC21, rC22;
   register __m128d rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm_load_pd(pB0);
   rB20 = _mm_load_pd(pB0+2);
   rB01 = _mm_load_pd(pB0+ldb);
   rB21 = _mm_load_pd(pB0+ldb+2);
   rB02 = _mm_load_pd(pB0+2*ldb);
   rB22 = _mm_load_pd(pB0+2*ldb+2);

   rC00 = _mm_load_pd(pC0);
   rC01 = _mm_load_pd(pC1);
   rC02 = _mm_load_pd(pC2);
   rA0  = _mm_load_pd(pA0);  /* A1, A0 */
   for (i=0; i < MM; i += 4, pA0 += 4, pA1 += 4, pA2 += 4, pA3 += 4,
        pC0 += 4, pC1 += 4, pC2 += 4)
   {
      register __m128d rB;

      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB); rA1 = _mm_load_pd(pA1);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB); rC20 =_mm_load_pd(pC0+2);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); rC21 =_mm_load_pd(pC1+2);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); rC22 =_mm_load_pd(pC2+2);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA3);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA0+2);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); _mm_store_pd(pC0, rC00);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); _mm_store_pd(pC1, rC01);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA1+2);
/*
 *    2nd row of C regs
 */
      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC2, rC02);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB); rC00 = _mm_load_pd(pC0+4);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA2+2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); rC01 = _mm_load_pd(pC1+4);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); rC02 = _mm_load_pd(pC2+4);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); rA1 = _mm_load_pd(pA3+2);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA0+4);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC0+2, rC20);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); _mm_store_pd(pC1+2, rC21);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); _mm_store_pd(pC2+2, rC22);
   }
/*
 * Drain C load/use pipeline
 */
   {
      register __m128d rB;

      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB); rA1 = _mm_load_pd(pA1);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB); rC20 =_mm_load_pd(pC0+2);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); rC21 =_mm_load_pd(pC1+2);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); rC22 =_mm_load_pd(pC2+2);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA3);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC00 = _mm_sub_pd(rC00, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC01 = _mm_sub_pd(rC01, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC02 = _mm_sub_pd(rC02, rB); rA0 = _mm_load_pd(pA0+2);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC00 = _mm_sub_pd(rC00, rB); _mm_store_pd(pC0, rC00);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC01 = _mm_sub_pd(rC01, rB); _mm_store_pd(pC1, rC01);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC02 = _mm_sub_pd(rC02, rB); rA1 = _mm_load_pd(pA1+2);
/*
 *    2nd row of C regs
 */
      rB = _mm_unpacklo_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC2, rC02);
      rB = _mm_unpacklo_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB); rA0 = _mm_load_pd(pA2+2);

      rB = _mm_unpackhi_pd(rB00, rB00);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpackhi_pd(rB01, rB01);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpackhi_pd(rB02, rB02);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); rA1 = _mm_load_pd(pA3+2);

      rB = _mm_unpacklo_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA0);
      rC20 = _mm_sub_pd(rC20, rB);
      rB = _mm_unpacklo_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA0);
      rC21 = _mm_sub_pd(rC21, rB);
      rB = _mm_unpacklo_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA0);
      rC22 = _mm_sub_pd(rC22, rB);

      rB = _mm_unpackhi_pd(rB20, rB20);
      rB = _mm_mul_pd(rB, rA1);
      rC20 = _mm_sub_pd(rC20, rB); _mm_store_pd(pC0+2, rC20);
      rB = _mm_unpackhi_pd(rB21, rB21);
      rB = _mm_mul_pd(rB, rA1);
      rC21 = _mm_sub_pd(rC21, rB); _mm_store_pd(pC1+2, rC21);
      rB = _mm_unpackhi_pd(rB22, rB22);
      rB = _mm_mul_pd(rB, rA1);
      rC22 = _mm_sub_pd(rC22, rB); _mm_store_pd(pC2+2, rC22);
   }
}
#elif defined(ATL_SSE2) && defined(SREAL)
   #define NRHS 4
   #define ATL_BINWRK 1
   #include <xmmintrin.h>
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=8, nu=4, ku=4.  This version is for 16 SSE regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorizations & software pipelinine of load/use.
 * Code operates on any multiple of 4 despite using MU=8.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk4(ATL_CINT M, const TYPE *A, ATL_CINT lda, 
                           TYPE *pB0, ATL_CINT ldb, TYPE *C, ATL_CINT ldc) 
{ 
   const TYPE *pA0 = A, *pA1 = A+lda, 
              *pA2 = A+((lda)<<1), *pA3=pA1+((lda)<<1); 
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1), *pC3 = pC2+ldc;
   ATL_CINT MM =  (M & 4) ? M-4 : M-8; 
   int i; 
   register __m128 rB00, rB01, rB02, rB03; 
   register __m128 rC00, rC01, rC02, rC03;
   register __m128 rC40, rC41, rC42, rC43;
   register __m128 rA0, rA1;
 
   if (M < 4)
      return;
   rB00 = _mm_load_ps(pB0);
   rB01 = _mm_load_ps(pB0+ldb);
   rB02 = _mm_load_ps(pB0+(ldb<<1));
   rB03 = _mm_load_ps(pB0+(ldb<<1)+ldb);

   rC00 = _mm_load_ps(pC0);
   rC01 = _mm_load_ps(pC1);
   rC02 = _mm_load_ps(pC2);
   rC03 = _mm_load_ps(pC3);

   rA0 = _mm_load_ps(pA0);

   for (i=0; i < MM; i += 8, pA0 += 8, pA1 += 8, pA2 += 8, pA3 += 8,
        pC0 += 8, pC1 += 8, pC2 += 8, pC3 += 8)
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);  rC40 = _mm_load_ps(pC0+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);  rC41 = _mm_load_ps(pC1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rC42 = _mm_load_ps(pC2+4);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  rA0 = _mm_load_ps(pA2);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  rC43 = _mm_load_ps(pC3+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA0+4);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
      
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);  rA1 = _mm_load_ps(pA1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);  rC00 = _mm_load_ps(pC0+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);  rC01 = _mm_load_ps(pC1+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);  rC02 = _mm_load_ps(pC2+8);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  rA0 = _mm_load_ps(pA2+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  rC03 = _mm_load_ps(pC3+8);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB);  rA1 = _mm_load_ps(pA3+4);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);  rA0 = _mm_load_ps(pA0+8);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  _mm_store_ps(pC0+4, rC40);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  _mm_store_ps(pC1+4, rC41);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);  _mm_store_ps(pC2+4, rC42);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB); _mm_store_ps(pC3+4, rC43);
   }
/* 
 * If orig M was multiple of 4 rather than 8, drain pipe over last 4 rows
 */
   if (M&4)
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA2);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
   }
   else /* drain pipe with MU=8 */
   {
      register __m128 rB;
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);  rA1 = _mm_load_ps(pA1);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);  rC40 = _mm_load_ps(pC0+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);  rC41 = _mm_load_ps(pC1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rC42 = _mm_load_ps(pC2+4);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  rA0 = _mm_load_ps(pA2);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  rC43 = _mm_load_ps(pC3+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB);  rA1 = _mm_load_ps(pA3);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC00 = _mm_sub_ps(rC00, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC01 = _mm_sub_ps(rC01, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC02 = _mm_sub_ps(rC02, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC03 = _mm_sub_ps(rC03, rB);  rA0 = _mm_load_ps(pA0+4);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC00 = _mm_sub_ps(rC00, rB);  _mm_store_ps(pC0, rC00);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC01 = _mm_sub_ps(rC01, rB);  _mm_store_ps(pC1, rC01);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC02 = _mm_sub_ps(rC02, rB);  _mm_store_ps(pC2, rC02);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC03 = _mm_sub_ps(rC03, rB); _mm_store_ps(pC3, rC03);
      
/*
 *    K=0 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);  rA1 = _mm_load_ps(pA1+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x00);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);
/*
 *    K=1 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  rA0 = _mm_load_ps(pA2+4);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0x55);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB);  rA1 = _mm_load_ps(pA3+4);
/*
 *    K=2 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC40 = _mm_sub_ps(rC40, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC41 = _mm_sub_ps(rC41, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC42 = _mm_sub_ps(rC42, rB);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xAA);
      rB = _mm_mul_ps(rB, rA0);
      rC43 = _mm_sub_ps(rC43, rB);
/*
 *    K=3 block
 */
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB00, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC40 = _mm_sub_ps(rC40, rB);  _mm_store_ps(pC0+4, rC40);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB01, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC41 = _mm_sub_ps(rC41, rB);  _mm_store_ps(pC1+4, rC41);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB02, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC42 = _mm_sub_ps(rC42, rB);  _mm_store_ps(pC2+4, rC42);
      rB = (__m128) _mm_shuffle_epi32((__m128i) rB03, 0xFF);
      rB = _mm_mul_ps(rB, rA1);
      rC43 = _mm_sub_ps(rC43, rB); _mm_store_ps(pC3+4, rC43);
   }
}
#else
   #define NRHS 4
   #define ATL_BINWRK 0
/*
 * Subtract off x0...x3 contribution to all remaining equations using a
 * rank-4 update with mu=2, nu=4, ku=4.  This version is for 32 scalar
 * registers, and assumes the scalar registers rB00..rB33 are live on entry.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable software pipelinine of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
#define ATL_rk4(M_, A_, lda_, C_, ldc_) if (M_ > 1) \
{ \
   const TYPE *pA0 = A_, *pA1 = A_+lda_, \
              *pA2 = A_+((lda_)<<1), *pA3=pA1+((lda_)<<1); \
   TYPE *pC0 = C_, *pC1 = C_+ldc_, \
               *pC2 = C_+((ldc_)<<1), *pC3=pC1+((ldc_)<<1); \
   register TYPE rC00= *pC0, rC01= *pC1, rC02 = *pC2, rC03 = *pC3; \
   register TYPE rc00, rc01, rc02, rc03; \
   register TYPE rA0 = *pA0, rA1; \
   ATL_CINT MM = M_ - 2; \
   ATL_INT i; \
 \
   for (i=0; i < MM; i += 2, pA0 += 2, pA1 += 2, pA2 += 2, pA3 += 2, \
        pC0 += 2, pC1 += 2, pC2 += 2, pC3 += 2) \
   { \
      rC00 -= rA0 * rB00; rA1 = *pA1; \
      rC01 -= rA0 * rB01; rc00 = pC0[1]; \
      rC02 -= rA0 * rB02; rc01 = pC1[1]; \
      rC03 -= rA0 * rB03; rc02 = pC2[1]; \
 \
      rC00 -= rA1 * rB10; rA0 = *pA2; \
      rC01 -= rA1 * rB11; rc03 = pC3[1]; \
      rC02 -= rA1 * rB12;  \
      rC03 -= rA1 * rB13;  \
       \
      rC00 -= rA0 * rB20; rA1 = *pA3; \
      rC01 -= rA0 * rB21; \
      rC02 -= rA0 * rB22;  \
      rC03 -= rA0 * rB23; rA0 = pA0[1]; \
       \
      rC00 -= rA1 * rB30; *pC0 = rC00; \
      rC01 -= rA1 * rB31; *pC1 = rC01; \
      rC02 -= rA1 * rB32; *pC2 = rC02; \
      rC03 -= rA1 * rB33; *pC3 = rC03; \
       \
      rc00 -= rA0 * rB00; rA1 = pA1[1]; \
      rc01 -= rA0 * rB01; rC00 = pC0[2]; \
      rc02 -= rA0 * rB02; rC01 = pC1[2]; \
      rc03 -= rA0 * rB03; rC02 = pC2[2]; \
       \
      rc00 -= rA1 * rB10; rA0 = pA2[1]; \
      rc01 -= rA1 * rB11; rC03 = pC3[2]; \
      rc02 -= rA1 * rB12; \
      rc03 -= rA1 * rB13;  \
       \
      rc00 -= rA0 * rB20; rA1 = pA3[1]; \
      rc01 -= rA0 * rB21; \
      rc02 -= rA0 * rB22;  \
      rc03 -= rA0 * rB23; rA0 = pA0[2]; \
       \
      rc00 -= rA1 * rB30; pC0[1] = rc00; \
      rc01 -= rA1 * rB31; pC1[1] = rc01; \
      rc02 -= rA1 * rB32; pC2[1] = rc02; \
      rc03 -= rA1 * rB33; pC3[1] = rc03; \
   } \
/* \
 *  Drain the C fetch/store pipe \
 */ \
   rC00 -= rA0 * rB00; rA1 = *pA1; \
   rC01 -= rA0 * rB01; rc00 = pC0[1]; \
   rC02 -= rA0 * rB02; rc01 = pC1[1]; \
   rC03 -= rA0 * rB03; rc02 = pC2[1]; \
 \
   rC00 -= rA1 * rB10; rA0 = *pA2; \
   rC01 -= rA1 * rB11; rc03 = pC3[1]; \
   rC02 -= rA1 * rB12;  \
   rC03 -= rA1 * rB13;  \
    \
   rC00 -= rA0 * rB20; rA1 = *pA3; \
   rC01 -= rA0 * rB21; \
   rC02 -= rA0 * rB22;  \
   rC03 -= rA0 * rB23; rA0 = pA0[1]; \
    \
   rC00 -= rA1 * rB30; *pC0 = rC00; \
   rC01 -= rA1 * rB31; *pC1 = rC01; \
   rC02 -= rA1 * rB32; *pC2 = rC02; \
   rC03 -= rA1 * rB33; *pC3 = rC03; \
    \
   rc00 -= rA0 * rB00; rA1 = pA1[1]; \
   rc01 -= rA0 * rB01; \
   rc02 -= rA0 * rB02; \
   rc03 -= rA0 * rB03; \
    \
   rc00 -= rA1 * rB10; rA0 = pA2[1]; \
   rc01 -= rA1 * rB11; \
   rc02 -= rA1 * rB12; \
   rc03 -= rA1 * rB13; \
    \
   rc00 -= rA0 * rB20; rA1 = pA3[1]; \
   rc01 -= rA0 * rB21; \
   rc02 -= rA0 * rB22; \
   rc03 -= rA0 * rB23; \
    \
   rc00 -= rA1 * rB30; pC0[1] = rc00; \
   rc01 -= rA1 * rB31; pC1[1] = rc01; \
   rc02 -= rA1 * rB32; pC2[1] = rc02; \
   rc03 -= rA1 * rB33; pC3[1] = rc03; \
}
#endif

#if NRHS == 3
/*
 * Solve 4x4 L with 3 RHS symbolically for unit diagonal
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4U(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L32=L_[2*(ldl_)+3]; \
/* \
 * x1 = (b1 - L10 * x0) \
 */ \
   rB10 = (rB10 - L10*rB00);  \
   rB11 = (rB11 - L10*rB01); \
   rB12 = (rB12 - L10*rB02); \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10); \
   rB21 = (rB21 - L20*rB01 - L21*rB11); \
   rB22 = (rB22 - L20*rB02 - L21*rB12); \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20); \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21); \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22); \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
}  /* complete 4x4 NRHS=3 solve block */

/*
 * Solve 4x4 L with 3 RHS symbolically
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L00=(*(L_)), L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L11=L_[ldl_+1], L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L22=L_[2*(ldl_)+2], L32=L_[2*(ldl_)+3]; \
   const RTYPE L33=L_[3*(ldl_)+3]; \
/* \
 * x0 = b0 / L00 \
 */ \
   rB00 *= L00; \
   rB01 *= L00; \
   rB02 *= L00; \
/* \
 * x1 = (b1 - L10 * x0) / L11 \
 */ \
   rB10 = (rB10 - L10*rB00) * L11;  \
   rB11 = (rB11 - L10*rB01) * L11; \
   rB12 = (rB12 - L10*rB02) * L11; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) / L22 \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10) * L22; \
   rB21 = (rB21 - L20*rB01 - L21*rB11) * L22; \
   rB22 = (rB22 - L20*rB02 - L21*rB12) * L22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) / L33 \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20) * L33; \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21) * L33; \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22) * L33; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
}  /* complete 4x4 NRHS=3 solve block */

#define  ATL_trsmU4(U_, ldu_, r_, ldr_) \
{ \
   const RTYPE U00=(*(U_)); \
   const RTYPE U01=(U_)[ldu_], U11=(U_)[ldu_+1]; \
   const RTYPE U02=(U_)[2*(ldu_)], U12= *(U_+2*(ldu_)+1),  \
               U22 = *(U_+2*(ldu_)+2); \
   const RTYPE U03 = *(U_+3*(ldu_)), U13 = *(U_+3*(ldu_)+1), \
               U23 = *(U_+3*(ldu_)+2), U33 = *(U_+3*(ldu_)+3); \
\
/* \
 * x3 = b3 / U33 \
 */ \
   rB30 *= U33; \
   rB31 *= U33; \
   rB32 *= U33; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - U23 * x3) / U22 \
 */ \
   rB20 = (rB20 - U23*rB30) * U22;  \
   rB21 = (rB21 - U23*rB31) * U22; \
   rB22 = (rB22 - U23*rB32) * U22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x1 = (b1 - U12*x2 - U13*x3) / U11 \
 */ \
   rB10 = (rB10 - U12*rB20 - U13*rB30) * U11; \
   rB11 = (rB11 - U12*rB21 - U13*rB31) * U11; \
   rB12 = (rB12 - U12*rB22 - U13*rB32) * U11; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x0 = (b0 - U01*x1 - U02*x2 - U03*x3) / U00 \
 */ \
   rB00 = (rB00 - U01*rB10 - U02*rB20 - U03*rB30) * U00; \
   rB01 = (rB01 - U01*rB11 - U02*rB21 - U03*rB31) * U00; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB02 = (rB02 - U01*rB12 - U02*rB22 - U03*rB32) * U00; \
}  /* complete M=4, N=3 solve block */
#elif NRHS == 4
/*
 * Solve 4x4 L with 4 RHS symbolically for unit diagonal
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4U(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L32=L_[2*(ldl_)+3]; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x1 = (b1 - L10 * x0) \
 */ \
   rB10 = (rB10 - L10*rB00);  \
   rB11 = (rB11 - L10*rB01); \
   rB12 = (rB12 - L10*rB02); \
   rB13 = (rB13 - L10*rB03); \
   ATL_pfl1W(r_ + ldr_ +((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10); \
   rB21 = (rB21 - L20*rB01 - L21*rB11); \
   rB22 = (rB22 - L20*rB02 - L21*rB12); \
   rB23 = (rB23 - L20*rB03 - L21*rB13); \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20); \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21); \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22); \
   rB33 = (rB33 - L30*rB03 - L31*rB13 - L32*rB23); \
}  /* complete 4x4 solve block */

/*
 * Solve 4x4 L with 4 RHS symbolically
 * Answer is output into rBxx regs, which are live on input and output
 */
#define  ATL_trsmL4(L_, ldl_, r_, ldr_) \
{ \
   const RTYPE L00=(*(L_)), L10=L_[1], L20=L_[2], L30=L_[3]; \
   const RTYPE L11=L_[ldl_+1], L21=L_[ldl_+2], L31=a[ldl_+3]; \
   const RTYPE L22=L_[2*(ldl_)+2], L32=L_[2*(ldl_)+3]; \
   const RTYPE L33=L_[3*(ldl_)+3]; \
/* \
 * x0 = b0 / L00 \
 */ \
   rB00 *= L00; \
   rB01 *= L00; \
   rB02 *= L00; \
   rB03 *= L00; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x1 = (b1 - L10 * x0) / L11 \
 */ \
   rB10 = (rB10 - L10*rB00) * L11;  \
   rB11 = (rB11 - L10*rB01) * L11; \
   rB12 = (rB12 - L10*rB02) * L11; \
   rB13 = (rB13 - L10*rB03) * L11; \
   ATL_pfl1W(r_ + ldr_ +((ldr_)<<2)); \
/* \
 * x2 = (b2 - L20*x0 - L21*x1) / L22 \
 */ \
   rB20 = (rB20 - L20*rB00 - L21*rB10) * L22; \
   rB21 = (rB21 - L20*rB01 - L21*rB11) * L22; \
   rB22 = (rB22 - L20*rB02 - L21*rB12) * L22; \
   rB23 = (rB23 - L20*rB03 - L21*rB13) * L22; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x3 = (b3 - L30*x0 - L31*x1 - L32*x2) / L33 \
 */ \
   rB30 = (rB30 - L30*rB00 - L31*rB10 - L32*rB20) * L33; \
   rB31 = (rB31 - L30*rB01 - L31*rB11 - L32*rB21) * L33; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB32 = (rB32 - L30*rB02 - L31*rB12 - L32*rB22) * L33; \
   rB33 = (rB33 - L30*rB03 - L31*rB13 - L32*rB23) * L33; \
}  /* complete 4x4 solve block */

#define  ATL_trsmU4(U_, ldu_, r_, ldr_) \
{ \
   const RTYPE U00=(*(U_)); \
   const RTYPE U01=(U_)[ldu_], U11=(U_)[ldu_+1]; \
   const RTYPE U02=(U_)[2*(ldu_)], U12= *(U_+2*(ldu_)+1),  \
               U22 = *(U_+2*(ldu_)+2); \
   const RTYPE U03 = *(U_+3*(ldu_)), U13 = *(U_+3*(ldu_)+1), \
               U23 = *(U_+3*(ldu_)+2), U33 = *(U_+3*(ldu_)+3); \
\
/* \
 * x3 = b3 / U33 \
 */ \
   rB30 *= U33; \
   rB31 *= U33; \
   rB32 *= U33; \
   rB33 *= U33; \
   ATL_pfl1W(r_ + ((ldr_)<<2)); \
/* \
 * x2 = (b2 - U23 * x3) / U22 \
 */ \
   rB20 = (rB20 - U23*rB30) * U22;  \
   rB21 = (rB21 - U23*rB31) * U22; \
   rB22 = (rB22 - U23*rB32) * U22; \
   rB23 = (rB23 - U23*rB33) * U22; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<2)); \
/* \
 * x1 = (b1 - U12*x2 - U13*x3) / U11 \
 */ \
   rB10 = (rB10 - U12*rB20 - U13*rB30) * U11; \
   rB11 = (rB11 - U12*rB21 - U13*rB31) * U11; \
   rB12 = (rB12 - U12*rB22 - U13*rB32) * U11; \
   rB13 = (rB13 - U12*rB23 - U13*rB33) * U11; \
   ATL_pfl1W(r_ + ((ldr_)<<1)+((ldr_)<<2)); \
/* \
 * x0 = (b0 - U01*x1 - U02*x2 - U03*x3) / U00 \
 */ \
   rB00 = (rB00 - U01*rB10 - U02*rB20 - U03*rB30) * U00; \
   rB01 = (rB01 - U01*rB11 - U02*rB21 - U03*rB31) * U00; \
   ATL_pfl1W(r_ + ldr_+((ldr_)<<1)+((ldr_)<<2)); \
   rB02 = (rB02 - U01*rB12 - U02*rB22 - U03*rB32) * U00; \
   rB03 = (rB03 - U01*rB13 - U02*rB23 - U03*rB33) * U00; \
}  /* complete 4x4 solve block */
#endif

@ROUT ATL_ctrsmKL_rk2
static void ATL_trsmLUN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *U, /* McxMc lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A*x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* McxNRHS workspace with good alignment */
)
{
   const TYPE ral=(*alpha), ial=alpha[1];
   const int ALPHAISREAL = (ial == ATL_rzero);
   ATL_CINT M2 = M+M, ldb2=ldb+ldb;
   #if TRU == 1
      ATL_CINT Mc = (M == 1) ? 2 : M;  /* need at least 2 for solve */
   #else
      ATL_CINT Mc = ((M+TRU-1)/TRU)*TRU;  /* make M multiple of unroll factor */
   #endif
   ATL_CINT Mc2=Mc+Mc, gap=Mc2-M2;
   ATL_INT i, j, k, mr, mr2;

/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb2)
   {
      const int nb = Mmin(NRHS, N-j);
      TYPE *w = W, *b = B;
      const TYPE *Ac = U + (Mc-2)*Mc2, *a = Ac + Mc2-4;
/*
 *    Copy NRHS RHS into aligned workspace, scaling by alpha as required
 *    Note, will have O(M) reuse
 *    For U, RHS are padded with zero at the top (U is padded with I in 
 *    same region).
 */
      for (k=0; k < nb; k++, w += Mc2-gap, b += ldb2)
      {
         for (i=0; i < gap; i++)
            w[i] = ATL_rzero;
         w += gap;
         if (ALPHAISREAL)
         {
            if (ral  != ATL_rone)
            {
               for (i=0; i < M2; i++)
                  w[i] = ral * b[i];
            }
            else
            {
               for (i=0; i < M2; i++)
                  w[i] = b[i];
            }
         }
         else /* must apply a complex alpha */
         {
            for (i=0; i < M2; i += 2)
            {
               register TYPE rb, ib;

               rb = b[i];
               ib = b[i+1];
               w[i] = rb*ral - ib*ial;
               w[i+1] = rb*ial + ib*ral;
            }
         }
      }
      for (; k < NRHS; k++, w += Mc2)
         for (i=0; i < Mc2; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these copied RHSs by looping over entire triangular mat
 */
      mr = (M == 1) ? 1 : 2;
      mr2 = mr + mr;
      w = W + Mc2-4;
      b = B + M2 - mr2;
      for (k=0; k < M; k += 2, w -= 4, a -= (Mc+1)<<2, Ac -= (Mc2<<1))
      {
//         ATL_CINT mr = Mmin(2,M-k), mr2=mr+mr;
/*
 *       Solve M=2 NRHS=NRHS TRSM symbolicly
 */
         ATL_trsmU2(a, Mc, w, Mc);
/*
 *       Write solved 2xNRHS block out to original workspace (final answer)
 */
         {
            ATL_INT ii, jj;
            TYPE *bb = b;
            const TYPE *ww = (const TYPE *) ((mr == 2) ? w : w+2);
            for (jj=0; jj < nb; jj++, ww += Mc2, bb += ldb2)
               for (ii=0; ii < mr2; ii++)
                  bb[ii] = ww[ii];
         }
/*
 *       Subtract off x0 and x1 contribution from rest of B using rank-2 update
 */
         if (M-k-2 > 0)
            ATL_rk2(Mc-k-2, Ac, Mc, w, Mc, W, Mc);
         mr = Mmin(2,M-k-2);
         mr2 = mr+mr;
         b -= mr2;
      }         /* end k-loop over U */
   }            /* end loop over RHS */
}  /* end of static func ATL_trsmLUN */

static void ATL_trsmLLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *L, /* McxMc lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* MxNRHS workspace with good alignment */
)
{
   const TYPE ral=(*alpha), ial=alpha[1];
   const int ALPHAISREAL = (ial == ATL_rzero);
   ATL_CINT M2 = M+M, ldb2=ldb+ldb;
   #if TRU == 1
      ATL_CINT Mc = (M == 1) ? 2 : M;  /* need at least 2 for solve */
   #else
      ATL_CINT Mc = ((M+TRU-1)/TRU)*TRU;  /* make M multiple of unroll factor */
   #endif
   ATL_CINT Mc2=Mc+Mc;
   ATL_INT i, j, k;

/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb2)
   {
      const int nb = Mmin(NRHS, N-j);
      TYPE *w = W, *b = B;
      const TYPE *a;
/*
 *    Copy NRHS RHS into aligned workspace, scaling by alpha as required
 *    Note, will have O(M) reuse
 */
      for (k=0; k < nb; k++, w += Mc2, b += ldb2)
      {
         if (ALPHAISREAL)
         {
            if (ral  != ATL_rone)
            {
               for (i=0; i < M2; i++)
                  w[i] = ral * b[i];
            }
            else
            {
               for (i=0; i < M2; i++)
                  w[i] = b[i];
            }
         }
         else /* must apply a complex alpha */
         {
            for (i=0; i < M2; i += 2)
            {
               register TYPE rb, ib;

               rb = b[i];
               ib = b[i+1];
               w[i] = rb*ral - ib*ial;
               w[i+1] = rb*ial + ib*ral;
            }
         }
         for (; i < Mc2; i++)
             w[i] = ATL_rzero;
      }
      for (; k < NRHS; k++, w += Mc2)
         for (i=0; i < Mc2; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these copied RHSs by looping over entire triangular mat
 */
      b = B;
      w = W;
      a = L;
      for (k=0; k < M; k += 2, b += 4, w += 4, a += (Mc+1)<<2)
      {
         ATL_CINT mr = Mmin(2,M-k), mr2=mr+mr;
/*
 *       Solve M=2 NRHS=NRHS TRSM symbolically
 */
         ATL_trsmL2(a, Mc, w, Mc);
/*
 *       Write solved 2xNRHS block out to original workspace (final answer)
 */
         {
            ATL_INT ii, jj;
            TYPE *bb = b;
            const TYPE *ww = (const TYPE *) w;
            for (jj=0; jj < nb; jj++, ww += Mc2, bb += ldb2)
               for (ii=0; ii < mr2; ii++)
                  bb[ii] = ww[ii];
         }
/*
 *       Subtract off x0 and x1 contribution from rest of B using rank-2 update
 */
         if (M-k-2 > 0)
            ATL_rk2(Mc-k-2, a+4, Mc, w, Mc, w+4, Mc);
      }         /* end k-loop over L */
   }            /* end loop over RHS */
}  /* end of static func ATL_trsmLLN */
@ROUT ATL_trsmKL_rk4
#ifdef ATL_KERN_L4U
   #define ATL_trsmLLNU Mjoin(PATL,ktrsmLLNU_rk4)
   #define trsmL4 ATL_trsmL4U  /* use unit-diag kernel */
void ATL_trsmLLNU
#else
   #define ATL_trsmLLN Mjoin(PATL,ktrsmLLN_rk4)
   #define trsmL4 ATL_trsmL4  /* use non-unit-diag kernel */
void ATL_trsmLLN
#endif
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A, /* MxM lower matrix A, diag has inverse of original diag */
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb, /* leading dim of B */
   TYPE *W        /* Mx4 workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2;

   #define lda M4
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W, *b = B;
      const TYPE *a;
/*
 *    Copy NRHS RHS to aligned workspace and scale if necessary, alpha cannot be
 *    zero, because this is handled as a special case at top
 */
      for (k=0; k < nb; k++, w += M4, b += ldb)
      {
         if (alpha != 1.0)
         {
            for (i=0; i < M; i++)
               w[i] = alpha * b[i];
         }
         else
         {
            for (i=0; i < M; i++)
               w[i] = b[i];
         }
         for (; i < M4; i++)
             w[i] = ATL_rzero;
      }
      for (; k < NRHS; k++, w += M4)
         for (i=0; i < M4; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      b = B;
      w = W;
      a = A;
      for (k=0; k < M; k += 4, b += 4, w += 4, a += (lda+1)<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
         RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         trsmL4(a, lda, b, ldb);
/*
 *       Write solved 4x4 block out to original workspace (final answer)
 *       Handle most common case with only one if
 */
         if (mr == 4 && nb == NRHS)
         {
            *b = rB00;
            b[1] = rB10;
            b[2] = rB20;
            b[3] = rB30;
            b[ldb] = rB01;
            b[ldb+1] = rB11;
            b[ldb+2] = rB21;
            b[ldb+3] = rB31;
            #if NRHS > 2
               b[ldb+ldb] = rB02;
               b[ldb+ldb+1] = rB12;
               b[ldb+ldb+2] = rB22;
               b[ldb+ldb+3] = rB32;
            #endif
            #if NRHS > 3
               b[(ldb<<1)+ldb] = rB03;
               b[(ldb<<1)+ldb+1] = rB13;
               b[(ldb<<1)+ldb+2] = rB23;
               b[(ldb<<1)+ldb+3] = rB33;
            #endif
         }
         else
         {
            switch(mr)
            {
            case 4:
               b[3] = rB30;
            case 3:
               b[2] = rB20;
            case 2:
               b[1] = rB10;
            case 1:
               *b = rB00;
            }
            if (nb > 1)
            {
               switch(mr)
               {
               case 4:
                  b[ldb+3] = rB31;
               case 3:
                  b[ldb+2] = rB21;
               case 2:
                  b[ldb+1] = rB11;
               case 1:
                  b[ldb] = rB01;
               }
               #if NRHS > 2
               if (nb > 2)
               {
                  switch(mr)
                  {
                  case 4:
                     b[ldb+ldb+3] = rB32;
                  case 3:
                     b[ldb+ldb+2] = rB22;
                  case 2:
                     b[ldb+ldb+1] = rB12;
                  case 1:
                     b[ldb+ldb] = rB02;
                  }
                  #if NRHS > 3
                  if (nb > 3)
                  {
                     switch(mr)
                     {
                     case 4:
                        b[(ldb<<1)+ldb+3] = rB33;
                     case 3:
                        b[(ldb<<1)+ldb+2] = rB23;
                     case 2:
                        b[(ldb<<1)+ldb+1] = rB13;
                     case 1:
                        b[(ldb<<1)+ldb] = rB03;
                     }
                  }
                  #endif
               }
               #endif
            }
         }
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            if (M-k-4 > 0)
            {
               *w = rB00;
               w[1] = rB10;
               w[2] = rB20;
               w[3] = rB30;
               w[M4] = rB01;
               w[M4+1] = rB11;
               w[M4+2] = rB21;
               w[M4+3] = rB31;
               #if NRHS > 2
                  w[2*M4] = rB02;
                  w[2*M4+1] = rB12;
                  w[2*M4+2] = rB22;
                  w[2*M4+3] = rB32;
               #endif
               #if NRHS > 3
                  w[3*M4] = rB03;
                  w[3*M4+1] = rB13;
                  w[3*M4+2] = rB23;
                  w[3*M4+3] = rB33;
               #endif
               ATL_rk4(M4-k-4, a+4, lda, w, M4, w+4, M4);
            }
         #else
            ATL_rk4(M4-k-4, a+4, lda, w+4, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
   #undef lda
}           /* end routine */

static void ATL_trsmLUN
(
   ATL_CINT  M,         /* size of orig triangular matrix A */
   ATL_CINT  N,         /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A, /* M4xM4 Upper matrix A, diag has inverse of original diag */
   TYPE *B,             /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb,       /* leading dim of B */
   TYPE *W              /* M4x4 workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2, mr = M4-M;
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, B += NRHS*ldb)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W, *b = B;
      const TYPE *Ac = A + (M4-4)*M4, *a = Ac + M4-4;
/*
 *    Copy NRHS RHS to aligned workspace and scale if necessary, alpha cannot be
 *    zero, because this is handled as a special case at top
 */
      for (k=0; k < nb; k++, w += M4, b += ldb)
      {
         for (i=0; i < mr; i++)
             w[i] = ATL_rzero;
         if (alpha != 1.0)
         {
            for (; i < M4; i++)
               w[i] = alpha * b[i-mr];
         }
         else
         {
            for (; i < M4; i++)
               w[i] = b[i-mr];
         }
      }
      for (; k < NRHS; k++, w += M4)
         for (i=0; i < M4; i++)
             w[i] = ATL_rzero;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      b = B + M;
      w = W + M4-4;
      for (k=0; k < M; k += 4, w -= 4, a -= (M4+1)<<2, Ac -= M4<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         b -= mr;
         ATL_trsmU4(a, M4, b, ldb);
/*
 *       Write solved 4x4 block out to original workspace (final answer)
 *       Handle most common case with only one if
 */
         if (mr == 4 && nb == NRHS)
         {
            *b = rB00;
            b[1] = rB10;
            b[2] = rB20;
            b[3] = rB30;
            b[ldb] = rB01;
            b[ldb+1] = rB11;
            b[ldb+2] = rB21;
            b[ldb+3] = rB31;
            #if NRHS > 2
               b[ldb+ldb] = rB02;
               b[ldb+ldb+1] = rB12;
               b[ldb+ldb+2] = rB22;
               b[ldb+ldb+3] = rB32;
            #endif
            #if NRHS > 3
               b[(ldb<<1)+ldb] = rB03;
               b[(ldb<<1)+ldb+1] = rB13;
               b[(ldb<<1)+ldb+2] = rB23;
               b[(ldb<<1)+ldb+3] = rB33;
            #endif
         }
         else
         {
            switch(mr)
            {
            case 1:
               *b = rB30;
               break;
            case 2:
               *b = rB20;
               b[1] = rB30;
               break;
            case 3:
               *b = rB10;
               b[1] = rB20;
               b[2] = rB30;
               break;
            case 4:
               *b = rB00;
               b[1] = rB10;
               b[2] = rB20;
               b[3] = rB30;
               break;
            }
            if (nb > 1)
            {
               switch(mr)
               {
               case 1:
                  b[ldb] = rB31;
                  break;
               case 2:
                  b[ldb] = rB21;
                  b[ldb+1] = rB31;
                  break;
               case 3:
                  b[ldb] = rB11;
                  b[ldb+1] = rB21;
                  b[ldb+2] = rB31;
                  break;
               case 4:
                  b[ldb] = rB01;
                  b[ldb+1] = rB11;
                  b[ldb+2] = rB21;
                  b[ldb+3] = rB31;
                  break;
               }
               #if NRHS > 2
               if (nb > 2)
               {
                  switch(mr)
                  {
                  case 1:
                     b[ldb+ldb] = rB32;
                     break;
                  case 2:
                     b[ldb+ldb] = rB22;
                     b[ldb+ldb+1] = rB32;
                     break;
                  case 3:
                     b[ldb+ldb] = rB12;
                     b[ldb+ldb+1] = rB22;
                     b[ldb+ldb+2] = rB32;
                     break;
                  case 4:
                     b[ldb+ldb] = rB02;
                     b[ldb+ldb+1] = rB12;
                     b[ldb+ldb+2] = rB22;
                     b[ldb+ldb+3] = rB32;
                     break;
                  }
                  #if NRHS > 3
                  if (nb > 3)
                  {
                     ATL_CINT ldb3 = ldb+(ldb<<1);
                     switch(mr)
                     {
                     case 1:
                        b[ldb3] = rB33;
                        break;
                     case 2:
                        b[ldb3] = rB23;
                        b[ldb3+1] = rB33;
                        break;
                     case 3:
                        b[ldb3] = rB13;
                        b[ldb3+1] = rB23;
                        b[ldb3+2] = rB33;
                        break;
                     case 4:
                        b[ldb3] = rB03;
                        b[ldb3+1] = rB13;
                        b[ldb3+2] = rB23;
                        b[ldb3+3] = rB33;
                        break;
                     }
                  }
                  #endif
               }
               #endif
            }
         }
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            if (M-k-4 > 0)
            {
               *w = rB00;
               w[1] = rB10;
               w[2] = rB20;
               w[3] = rB30;
               w[M4] = rB01;
               w[M4+1] = rB11;
               w[M4+2] = rB21;
               w[M4+3] = rB31;
               #if NRHS > 2
                  w[2*M4] = rB02;
                  w[2*M4+1] = rB12;
                  w[2*M4+2] = rB22;
                  w[2*M4+3] = rB32;
               #endif
               #if NRHS > 3
                  w[3*M4] = rB03;
                  w[3*M4+1] = rB13;
                  w[3*M4+2] = rB23;
                  w[3*M4+3] = rB33;
               #endif
               ATL_rk4(M4-k-4, Ac, M4, w, M4, W, M4);
            }
         #else
            ATL_rk4(M4-k-4, Ac, M4, W, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */
@ROUT ATL_trsmKR_rk4
static void ATL_trsmRLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const TYPE *A, /* M4xM4 lower matrix A, diag has inverse of original diag */
   TYPE *W        /* M4xN, on input padded B, on output, padded X */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2;

   #define lda M4
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*M4)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *a;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W;
      a = A;
      for (k=0; k < M; k += 4, w += 4, a += (lda+1)<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmL4(a, lda, w, M4);
/*
 *       Write solved 4xNRHS block out to workspace (final answer)
 */
         *w = rB00;
         w[1] = rB10;
         w[2] = rB20;
         w[3] = rB30;
         w[M4] = rB01;
         w[M4+1] = rB11;
         w[M4+2] = rB21;
         w[M4+3] = rB31;
         #if NRHS > 2
            w[M4+M4] = rB02;
            w[M4+M4+1] = rB12;
            w[M4+M4+2] = rB22;
            w[M4+M4+3] = rB32;
         #endif
         #if NRHS > 3
            w[3*M4] = rB03;
            w[3*M4+1] = rB13;
            w[3*M4+2] = rB23;
            w[3*M4+3] = rB33;
         #endif
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            ATL_rk4(M4-k-4, a+4, lda, w, M4, w+4, M4);
         #else
            ATL_rk4(M4-k-4, a+4, lda, w+4, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
   #undef lda
}           /* end routine */

static void ATL_trsmRUN
(
   ATL_CINT  M,         /* size of orig triangular matrix A */
   ATL_CINT  N,         /* number of RHS in W */
   const TYPE *A,       /* M4xM4 Upper matrix A, diag is inverted */
   TYPE *W              /* M4xN workspace with good alignment */
)
{
   int j;
   ATL_CINT M4 = ((M+3)>>2)<<2, mr = M4-M;
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*M4)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *Ac = A + (M4-4)*M4, *a = Ac + M4-4;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W + M4-4;
      for (k=0; k < M; k += 4, w -= 4, a -= (M4+1)<<2, Ac -= M4<<2)
      {
         ATL_CINT mr = Mmin(4,M-k);
         RTYPE rB00 = *w, rB10=w[1], rB20=w[2], rB30=w[3];
         RTYPE rB01=w[M4], rB11=w[M4+1], rB21=w[M4+2], rB31=w[M4+3];
         #if NRHS > 2
            RTYPE rB02=w[2*M4],rB12=w[2*M4+1],rB22=w[2*M4+2],rB32=w[2*M4+3];
         #endif
         #if NRHS > 3
            RTYPE rB03=w[3*M4],rB13=w[3*M4+1],rB23=w[3*M4+2],rB33=w[3*M4+3];
         #endif
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmU4(a, M4, w, M4);
/*
 *       Store solved 4xNRHS elts of X to workspace
 */
         *w = rB00;
         w[1] = rB10;
         w[2] = rB20;
         w[3] = rB30;
         w[M4] = rB01;
         w[M4+1] = rB11;
         w[M4+2] = rB21;
         w[M4+3] = rB31;
         #if NRHS > 2
            w[2*M4] = rB02;
            w[2*M4+1] = rB12;
            w[2*M4+2] = rB22;
            w[2*M4+3] = rB32;
         #endif
         #if NRHS > 3
            w[3*M4] = rB03;
            w[3*M4+1] = rB13;
            w[3*M4+2] = rB23;
            w[3*M4+3] = rB33;
         #endif
/*
 *       Subtract off x0-x4 contribution from rest of B using rank-4 update
 */
         #if ATL_BINWRK
            ATL_rk4(M4-k-4, Ac, M4, w, M4, W, M4);
         #else
            ATL_rk4(M4-k-4, Ac, M4, W, M4);
         #endif
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */
@ROUT ATL_ctrsmKR_rk2
static void ATL_trsmRLN
(
   ATL_CINT  M,   /* size of orig triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const TYPE *A, /* McxMc lower matrix A, diag has inverse of original diag */
   TYPE *W        /* McxN, on input padded B, on output, padded X */
)
{
   int j;
   ATL_CINT Mc = ((M+TRU-1)/TRU)*TRU, Mc2=Mc+Mc;

/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*Mc2)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *a;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W;
      a = A;
      for (k=0; k < M; k += 2, w += 4, a += (Mc+1)<<2)
      {
/*
 *       Solve M=2 NRHS=NRHS TRSM symbolically
 */
         ATL_trsmL2(a, Mc, w, Mc);
/*
 *       Subtract off x0,x1 contribution from rest of B using rank-2 update
 */
	 if (M-k-2 > 0)
            ATL_rk2(Mc-k-2, a+4, Mc, w, Mc, w+4, Mc);
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */

static void ATL_trsmRUN
(
   ATL_CINT  M,         /* size of orig triangular matrix A */
   ATL_CINT  N,         /* number of RHS in W */
   const TYPE *A,       /* McxMc Upper matrix A, diag is inverted */
   TYPE *W              /* McxN workspace with good alignment */
)
{
   int j;
   ATL_CINT Mc = ((M+TRU-1)/TRU)*TRU, mr = Mc-M, Mc2=Mc+Mc;
/*
 * Loop over RHS, NRHS RHS at a time
 */
   for (j=0; j < N; j += NRHS, W += NRHS*Mc2)
   {
      const int nb = Mmin(NRHS, N-j);
      int k, i;
      TYPE *w = W;
      const TYPE *Ac = A + (Mc-2)*Mc2, *a = Ac + Mc2-4;
/*
 *    Completely solve these RHSs by looping over entire triangular matrix
 */
      w = W + Mc2-4;
      for (k=0; k < M; k += 2, w -= 4, a -= (Mc+1)<<2, Ac -= Mc<<2)
      {
         ATL_CINT mr = Mc-k-2;
/*
 *       Solve M=4 NRHS=4 TRSM symbolically
 */
         ATL_trsmU2(a, Mc, w, Mc);
/*
 *       Subtract off x0,x1 contribution from rest of B using rank-2 update
 */
        
          if (mr)
            ATL_rk2(mr, Ac, Mc, w, Mc, W, Mc);
      }     /* end k-loop that loops through L */
   }        /* end j-loop over RHS */
}           /* end routine */
@ROUT ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
ATL_SINLINE void trU2Lc
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *U, ATL_CINT ldu, 
    TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L with conjugation,
 * Lower, right part of L is padded to ldl with I to reach ldl size
 */
{
   ATL_CINT N2=N+N, ldU=ldu+ldu, ldL=ldl+ldl;
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N2; j += 2, Uc += ldU)
   {
      TYPE *Lr = L + j;
      for (i=0; i < j; i += 2, Lr += ldL)
      {
         *Lr = Uc[i];
         Lr[1] = -Uc[i+1];
      }
      if (Diag == AtlasUnit)
      {
         *Lr = ATL_rone;
         Lr[1] = ATL_rzero;
      }
      else
      {
         *Lr = Uc[j];
         Lr[1] = -Uc[j+1];
         Mjoin(PATL,cplxinvert)(1, Lr, 1, Lr, 1);
      }
   }
/*
 * Pad left and lower portion of L if ldl > N
 */
  if (ldl > N)
  {
/* 
 *    Pad the last ldl-N rows of L with zeros in the first N columns of L
 */
      for (j=0; j < N2; j += 2, L += ldL)
      {
         for (i=N2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
  
/* 
 *    Pad the last ldl-N columns of L with the identity matrix
 */
      for (; j < ldL; j += 2, L += ldL)
      {
         L[j] = ATL_rone;
         L[j+1] = ATL_rzero;
         for (i=j+2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
   }
}
ATL_SINLINE void trU2L
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *U, ATL_CINT ldu, 
    TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L, 
 * Lower, right part of L is padded to ldl with I to reach ldl size
 */
{
   ATL_CINT N2=N+N, ldU=ldu+ldu, ldL=ldl+ldl;
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N2; j += 2, Uc += ldU)
   {
      TYPE *Lr = L + j;
      for (i=0; i < j; i += 2, Lr += ldL)
      {
         *Lr = Uc[i];
         Lr[1] = Uc[i+1];
      }
      if (Diag == AtlasUnit)
      {
         *Lr = ATL_rone;
         Lr[1] = ATL_rzero;
      }
      else
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(Uc+j), 1, Lr, 1);
   }
/*
 * Pad left and lower portion of L if ldl > N
 */
  if (ldl > N)
  {
/* 
 *    Pad the last ldl-N rows of L with zeros in the first N columns of L
 */
      for (j=0; j < N2; j += 2, L += ldL)
      {
         for (i=N2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
  
/* 
 *    Pad the last ldl-N columns of L with the identity matrix
 */
      for (; j < ldL; j += 2, L += ldL)
      {
         L[j] = ATL_rone;
         L[j+1] = ATL_rzero;
         for (i=j+2; i < ldL; i++)
            L[i] = ATL_rzero;
      }
   }
}

ATL_SINLINE void trL2U
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *L, ATL_CINT ldl, 
    TYPE *U, ATL_CINT ldu)
/*
 * reflects lower part of L into upper part of U, 
 * The upper, left part of U is padded with I to reach ldu size
 */
{
   ATL_CINT N2=N+N, ldu2=ldu+ldu, ldl2=ldl+ldl, gap = ldu2-N2;
   ATL_INT i, j;
   TYPE *u;

   if (gap)
   {
/*
 *    Pad ldu-N first columns with I
 */
      for (j=0; j < gap; j += 2, U += ldu2)
      {
         for (i=0; i < ldu2; i++)
            U[i] = ATL_rzero;
         U[j] = ATL_rone;
      }
/*
 *    Pad first ldu-N rows with zeros
 */
      u = U;
      for (j=0; j < N2; j += 2, u += ldu2)
      {
         for (i=0; i < gap; i++)
            u[i] = ATL_rzero;
      }
      U += gap;
   }
/*
 * Now transpose L into last N rows/cols of U.  Access rows of U since it
 * is known to be contiguous, so non-cont L is accessed by columns.
 */
   for (j=0; j < N2; j += 2, L += ldl2+2, U += ldu2+2)
   {
      if (Diag == AtlasUnit)
      {
         *U = ATL_rone;
         U[1] = ATL_rzero;
      }
      else
         Mjoin(PATL,cplxinvert)(1, (TYPE*)L, 1, U, 1);
      for (u=U+ldu2,i=j+2; i < N2; i += 2, u += ldu2)
      {
         *u = L[i-j];
         u[1] = L[i-j+1];
      }
   }
}
ATL_SINLINE void trL2Uc
   (enum ATLAS_DIAG Diag, ATL_CINT N, const TYPE *L, ATL_CINT ldl, 
    TYPE *U, ATL_CINT ldu)
/*
 * reflects & conjugates lower part of L into upper part of U, 
 * The upper, left part of U is padded with I to reach ldu size
 */
{
   ATL_CINT N2=N+N, ldu2=ldu+ldu, ldl2=ldl+ldl, gap = ldu2-N2;
   ATL_INT i, j;
   TYPE *u;

   if (gap)
   {
/*
 *    Pad ldu-N first columns with I
 */
      for (j=0; j < gap; j += 2, U += ldu2)
      {
         for (i=0; i < ldu2; i++)
            U[i] = ATL_rzero;
         U[j] = ATL_rone;
      }
/*
 *    Pad first ldu-N rows with zeros
 */
      u = U;
      for (j=0; j < N2; j += 2, u += ldu2)
      {
         for (i=0; i < gap; i++)
            u[i] = ATL_rzero;
      }
      U += gap;
   }
/*
 * Now transpose L into last N rows/cols of U.  Access rows of U since it
 * is known to be contiguous, so non-cont L is accessed by columns.
 */
   for (j=0; j < N2; j += 2, L += ldl2+2, U += ldu2+2)
   {
      if (Diag == AtlasUnit)
      {
         *U = ATL_rone;
         U[1] = ATL_rzero;
      }
      else
      {
         *U = *L;
         U[1] = -L[1];
         Mjoin(PATL,cplxinvert)(1, U, 1, U, 1);
      }
      for (u=U+ldu2,i=j+2; i < N2; i += 2, u += ldu2)
      {
         *u = L[i-j];
         u[1] = -L[i-j+1];
      }
   }
}
@ROUT ATL_trsmKL_rk4 ATL_trsmKR_rk4

ATL_SINLINE void trL2U
   (ATL_CINT N, const TYPE *L, ATL_CINT ldl, TYPE *U, ATL_CINT ldu)
/*
 * reflects lower part of L into upper part of U
 */
{
   const TYPE *Lc=L;
   ATL_INT i, j;

   for (j=0; j < N; j++, Lc += ldl)
   {
      TYPE *Ur = U + j;
      for (i=j; i < N; i++)
         U[j+i*ldu] = Lc[i];
   }
}

ATL_SINLINE void trU2L
   (ATL_CINT N, const TYPE *U, ATL_CINT ldu, TYPE *L, ATL_CINT ldl)
/*
 * reflects upper part of U into lower part of L
 */
{
   ATL_INT i, j;
   const TYPE *Uc = U;

   for (j=0; j < N; j++, Uc += ldu)
   {
      TYPE *Lr = L + j;
      for (i=0; i <= j; i++, Lr += ldl)
         *Lr = Uc[i];
   }
}

/*
 * Copy original U to aligned workspace, invert diagonal elts, pad wt I
 * Padding is at top of upper triangular matrix
 */
static void cpypadU
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT N4                  /* leading dim of A */
)
{
   int i, j;
   const int mr = N4-N;

   for (j=0; j < mr; j++, a += N4)
   {
      for (i=0; i < N4; i++)
         a[i] = ATL_rzero;
      a[j] = ATL_rone;
   }
   for (; j < N4; j++, a += N4, A += lda)
   {
      for (i=0; i < mr; i++)
         a[i] = ATL_rzero;
      for (; i < j; i++)
         a[i] = A[i-mr];
      a[j] = (Diag == AtlasNonUnit) ? 1.0 / A[j-mr] : ATL_rone;
   }
}

/*
 * Copy original L to aligned workspace, invert diagonal elts, pad wt I
 */
static void cpypadL
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT N4                  /* leading dim of A */

)
{
   int i, j;

   for (j=0; j < N; j++, a += N4, A += lda)
   {
      a[j] = (Diag == AtlasNonUnit) ? 1.0 / A[j] : ATL_rone;
      for (i=j+1; i < N; i++)
         a[i] = A[i];
      for (; i < N4; i++)
         a[i] = ATL_rzero;
   }
   for (; j < N4; j++, a += N4)
   {
      for (i=0; i < N4; i++)
         a[i] = ATL_rzero;
      a[j] = ATL_rone;
   }
}
@ROUT ATL_ctrsmKR_rk2
/*
 * Conjugate original U to aligned workspace, invert diagonal elts, pad wt I
 * U is padded with I at top left of matrix to fit Nc
 */
static void cpypadUc
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* upper triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to Nc with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

/* 
 * Pad Nc-N upper, left of U with I
 */
   if (Nc != N)
   {
      const int nn = Nc2-N2;
      TYPE *aa;
      for (j=0; j < nn; j += 2, a += Nc2)
      {
         for (i=0; i < Nc2; i++)
            a[i] = ATL_rzero;
         a[j] = ATL_rone;
      }
/*
 *    Now zero first few rows of each column above that actual U
 */
      aa = a;
      for (j=0; j < N; j++, aa += Nc2)
      {
         for (i=0; i < nn; i++)
            aa[i] = ATL_rzero;
      }
      a += nn;  /* a now pts to place to copy actual U */
   }
/*
 * Copy unpadded portion to U
 */
   for (j=0; j < N2; j += 2, a += Nc2, A += lda2)
   {
      for (i=0; i < j; i += 2)
      {
         a[i] = A[i];
         a[i+1] = -A[i+1];
      }
      if (Diag == AtlasNonUnit)
      {
         a[j] = A[j];
         a[j+1] = -A[j+1];
         Mjoin(PATL,cplxinvert)(1, a+j, 1, a+j, 1);
      }
      else
      {
         a[j] = ATL_rone;
         a[j+1] = ATL_rzero;
      }
   }
}
/*
 * Conjugate original L to aligned workspace, invert diagonal elts, pad wt I
 */
static void cpypadLc
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

   for (j=0; j < N; j++, a += Nc2, A += lda2)
   {
      ATL_CINT j2 = j+j;

      if (Diag == AtlasNonUnit)
      {
         a[j2] = A[j2];
	 a[j2+1] = -A[j2+1];
         Mjoin(PATL,cplxinvert)(1, a+j2, 1, a+j2, 1);
      }
      else
      {
         a[j2] = ATL_rone;
         a[j2+1] = ATL_rzero;
      }
      for (i=j2+2; i < N2; i += 2)
      {
         a[i] = A[i];
         a[i+1] = -A[i+1];
      }
      for (; i < Nc2; i++)
         a[i] = ATL_rzero;
   }
   for (; j < Nc; j++, a += Nc2)
   {
      for (i=0; i < Nc2; i++)
         a[i] = ATL_rzero;
      a[j+j] = ATL_rone;
   }
}
@ROUT ATL_ctrsmKL_rk2 ATL_ctrsmKR_rk2
/*
 * Copy original U to aligned workspace, invert diagonal elts, pad wt I
 * U is padded with I at top left of matrix to fit Nc
 */
static void cpypadU
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* upper triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to Nc with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

/* 
 * Pad Nc-N upper, left of U with I
 */
   if (Nc != N)
   {
      const int nn = Nc2-N2;
      TYPE *aa;
      for (j=0; j < nn; j += 2, a += Nc2)
      {
         for (i=0; i < Nc2; i++)
            a[i] = ATL_rzero;
         a[j] = ATL_rone;
      }
/*
 *    Now zero first few rows of each column above that actual U
 */
      aa = a;
      for (j=0; j < N; j++, aa += Nc2)
      {
         for (i=0; i < nn; i++)
            aa[i] = ATL_rzero;
      }
      a += nn;  /* a now pts to place to copy actual U */
   }
/*
 * Copy unpadded portion to U
 */
   for (j=0; j < N2; j += 2, a += Nc2, A += lda2)
   {
      for (i=0; i < j; i++)
         a[i] = A[i];
      if (Diag == AtlasNonUnit)
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(A+j), 1, a+j, 1);
      else
      {
         a[j] = ATL_rone;
         a[j+1] = ATL_rzero;
      }
   }
}
/*
 * Copy original L to aligned workspace, invert diagonal elts, pad wt I
 */
static void cpypadL
(
   enum ATLAS_DIAG Diag, 
   ATL_CINT N,                  /* size of triangular matrix A */
   const TYPE *A,               /* lower triangular matrix */
   ATL_CINT lda,                /* leading dim of A */
   TYPE *a,                     /* cpy of A, padded to N4 with I */
   ATL_CINT Nc                  /* leading dim of A, how much to pad */
)
{
   int i, j;
   ATL_CINT lda2 = lda+lda, N2=N+N, Nc2=Nc+Nc;

   for (j=0; j < N; j++, a += Nc2, A += lda2)
   {
      ATL_CINT j2 = j+j;

      if (Diag == AtlasNonUnit)
         Mjoin(PATL,cplxinvert)(1, (TYPE*)(A+j2), 1, a+j2, 1);
      else
      {
         a[j2] = ATL_rone;
         a[j2+1] = ATL_rzero;
      }
      for (i=j2+2; i < N2; i++)
         a[i] = A[i];
      for (; i < Nc2; i++)
         a[i] = ATL_rzero;
   }
   for (; j < Nc; j++, a += Nc2)
   {
      for (i=0; i < Nc2; i++)
         a[i] = ATL_rzero;
      a[j+j] = ATL_rone;
   }
}
@ROUT ATL_ctrsmKL_rk2
int Mjoin(PATL,trsmKL_rk2)   /* returns 0 on success */
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,   /* size of triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A0, /* MxM lower matrix A, diag has inverse of original diag */
   ATL_CINT  lda0,
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb  /* leading dim of B */
)
{
   void *vp;
   const TYPE *A = A0;
   TYPE *a, *w, *t=NULL;
   #if TRU == 1
      ATL_CINT Mc = (M == 1) ? 2 : M;  /* need at least 2 for solve */
   #else
      ATL_CINT Mc = ((M+TRU-1)/TRU)*TRU;  /* make M multiple of unroll factor */
   #endif
   ATL_CINT Mc2 = Mc+Mc;
   ATL_INT lda = lda0;
   int UPPER = (Uplo == AtlasUpper);

   ATL_assert(Side == AtlasLeft);
   vp = malloc(ATL_MulBySize(Mc*Mc+Mc*NRHS)+2*ATL_Cachelen);
   if (!vp)
      return(1);

   a = ATL_AlignPtr(vp);
   w = a + Mc*Mc2;
   w = ATL_AlignPtr(w);
   if (UPPER)
   {
      if (TA == AtlasNoTrans)
      {
         cpypadU(Diag, M, A, lda, a, Mc);
         ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
      }
      else if (TA == AtlasTrans)
      {
         trU2L(Diag, M, A0, lda0, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
      else  /* TA == AtlasConjTrans */
      {
         trU2Lc(Diag, M, A0, lda0, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
   }
   else  /* Lower */
   {
      if (TA == AtlasNoTrans)
      {
         cpypadL(Diag, M, A, lda, a, Mc);
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      }
      else if (TA == AtlasTrans)
      {
         trL2U(Diag, M, A, lda, a, Mc);
         ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
      }
      else  /* TA == AtlasConjTrans */
      {
         trL2Uc(Diag, M, A, lda, a, Mc);
         ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
      }
   }
   free(vp);
   return(0);
}
@ROUT ATL_trsmKL_rk4
#ifdef ATL_KERN_L4U
int Mjoin(PATL,trsmKLU_rk4)   /* returns 0 on success */
#else
int Mjoin(PATL,trsmKL_rk4)   /* returns 0 on success */
#endif
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,   /* size of triangular matrix A */
   ATL_CINT  N,   /* number of RHS in B */
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A0, /* MxM lower matrix A, diag has inverse of original diag */
   ATL_CINT  lda0,
   TYPE *B,       /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb  /* leading dim of B */
)
{
   void *vp;
   const TYPE *A = A0;
   TYPE *a, *w, *t=NULL;
   ATL_CINT M4 = ((M+3)>>2)<<2;
   ATL_INT lda = lda0;
   int UPPER = (Uplo == AtlasUpper);

   if (TA == AtlasTrans)
   {
      t = malloc(M*M*sizeof(TYPE));
      ATL_assert(t);
      if (UPPER)
         trU2L(M, A0, lda0, t, M);
      else
         trL2U(M, A0, lda0, t, M);
      UPPER = !UPPER;
      A = (const TYPE *) t;
      lda = M;
   }
   vp = malloc(sizeof(TYPE)*(M4*M4+M4*NRHS)+2*ATL_Cachelen);
   if (!vp)
      return(1);

   a = ATL_AlignPtr(vp);
   w = a + M4*M4;
   w = ATL_AlignPtr(w);
   if (!UPPER)
   {
      int j, i;
      TYPE *ap=a;
      cpypadL(Diag, M, A, lda, a, M4);
      if (t)
         free(t);
      #ifdef ATL_KERN_L4U
         ATL_trsmLLNU(M, N, alpha, a, B, ldb, w);
      #else
         ATL_trsmLLN(M, N, alpha, a, B, ldb, w);
      #endif
   }
   else  /* Uplo == AtlasUpper */
   {
      int j, i;
      const int mr = M4-M;
      TYPE *ap=a;
      cpypadU(Diag, M, A, lda, a, M4);
      if (t)
         free(t);
      ATL_trsmLUN(M, N, alpha, a, B, ldb, w);
   }
   free(vp);
   return(0);
}
@ROUT ATL_trsmKR_rk4
/* 
 * This routine computes X * op(A) = B by first computing:
 *    op(A)^T * X^T = B^T
 * and then transposing the answer back out.
 */
int Mjoin(PATL,trsmKR_rk4)
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,         /* number of RHS in B */
   ATL_CINT  N,         /* size of triangular matrix A */ 
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A,       /* MxM triangular matrix A */
   ATL_CINT  lda,
   TYPE *B,             /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb        /* leading dim of B */
)
{
   void *vp;
   TYPE *a, *w, *b;
   enum ATLAS_TRANS TRANSA;
   ATL_CINT N4 = ((N+3)>>2)<<2, nr = N4-N;
   ATL_CINT Mc = Mmin(((M+NRHS-1)/NRHS)*NRHS, 36);
   int UPPER = (Uplo == AtlasUpper);
   const int TRANS = (TA == AtlasTrans), UNIT=(Diag == AtlasUnit);
   ATL_INT i, j;

   vp = malloc((Mc*N4+N4*N4)*sizeof(TYPE) + 2*ATL_Cachelen);
   if (!vp) 
      return(1);
   a = ATL_AlignPtr(vp);
   w = a+N4*N4;
   w = ATL_AlignPtr(w);
/*
 * If matrix is already transposed, keep Uplo same and just copy to space
 * and invert the diagonal, and swap the transpose setting
 */
   if (TRANS)
   {
     if (UPPER)
        cpypadU(Diag, N, A, lda, a, N4);
     else
        cpypadL(Diag, N, A, lda, a, N4);
   }
/* 
 * If the matrix is presently in NoTranspose format, transpose it, which
 * will swap Uplo setting
 */
   else
   {
      if (UPPER)
      {
         TYPE *c=a;
         trU2L(N, A, lda, a, N4);  /* transpose A into lower-triangular a */
/*
 *       invert diagonal and pad N4-N gap at bottom of Lower matrix
 */
         for (j=0; j < N; j++, c += N4)
         {
            c[j] = (UNIT) ? ATL_rone : ATL_rone / c[j];
            for (i=N; i < N4; i++)
               c[i] = ATL_rzero;
         }
/*
 *       Pad last N4-N columns with identity matrix
 */
         for (; j < N4; j++, c += N4)
         {
            for (i=0; i < N4; i++)
               c[i] = ATL_rzero;
            c[j] = ATL_rone;
         }
      }
      else  /* matrix is currently lower */
      {
         TYPE *c=a+nr*(N4+1);
/*
 *       Transpose Lower matrix to Upper; padding goes at top of Upper matrix
 */
         trL2U(N, A, lda, c, N4);
/*
 *       Invert diagonal elements
 */
         if (UNIT)
            for (j=nr; j < N4; j++, c += N4+1)
               *c = ATL_rone;
         else
            for (j=nr; j < N4; j++, c += N4+1)
               *c = ATL_rone / *c;
            
/* 
 *       If N != N4, then we must pad the matrix
 */
         if (nr)
         {
            c = a;
/*
 *          Pad first nr cols with identity matrix
 */
            for (j=0; j < nr; j++, c += N4)
            {
               for (i=0; i < N4; i++)
                  c[i] = ATL_rzero;
               c[j] = ATL_rone;
            }
/*
 *          Pad first nr rows of remaining columns with zeros
 */
            for (; j < N4; j++, c += N4)
            {
               for (i=0; i < nr; i++)
                  c[i] = ATL_rzero;
            }
         }
      }
      UPPER = !UPPER;  /* transposition swaps UPLO setting */
   }
/*
 * a now contains correctly padded A^T, so now loop over Mc rows of RHS;
 * Mc is set to a value near 32 so we can get some TLB reuse when doing
 * the transpose of the RHS vectors from row-access to column-access
 */
   b = (UPPER) ? w + nr : w;
   for (i=0; i < M; i += Mc, B += Mc)
   {
      const int mr = Mmin(M-i, Mc);
      Mjoin(PATL,gemoveT)(N, mr, alpha, B, ldb, b, N4);
      if (UPPER)
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w, N4);
         ATL_trsmRUN(N, mr, a, w);
      }
      else
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w+N, N4);
         ATL_trsmRLN(N, mr, a, w);
      }
      Mjoin(PATL,gemoveT)(mr, N, ATL_rone, b, N4, B, ldb);
   }
   free(vp);
   return(0);
}
@ROUT ATL_ctrsmKR_rk2
/* 
 * This routine computes X * op(A) = B by first computing:
 *    op(A)^T * X^T = B^T
 * and then transposing the answer back out.
 */
int Mjoin(PATL,trsmKR_rk2)
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,         /* number of RHS in B */
   ATL_CINT  N,         /* size of triangular matrix A */ 
   const SCALAR alpha,  /* scale factor for B */
   const TYPE *A,       /* MxM triangular matrix A */
   ATL_CINT  lda,
   TYPE *B,             /* on input, B, on output X, of A x = b */
   ATL_CINT  ldb        /* leading dim of B */
)
{
   void *vp;
   TYPE *a, *w, *b;
   const TYPE one[2] = {ATL_rone, ATL_rzero};
   enum ATLAS_TRANS TRANSA;
   ATL_CINT Nc = ((N+TRU-1)/TRU)*TRU, nr = Nc-N, Nc2=Nc+Nc, nr2 = nr+nr;
   ATL_CINT Mc = Mmin(((M+NRHS-1)/NRHS)*NRHS, 36), Mc2 = Mc+Mc;
   int UPPER = (Uplo == AtlasUpper);
   const int UNIT=(Diag == AtlasUnit);
   ATL_INT i, j;

   vp = malloc(ATL_MulBySize(Nc*(Mc+Nc)) + 2*ATL_Cachelen);
   if (!vp) 
      return(1);
   a = ATL_AlignPtr(vp);
   w = a+Nc*Nc2;
   w = ATL_AlignPtr(w);
/*
 * If matrix is already transposed, keep Uplo same and just copy to space
 * and invert the diagonal, and swap the transpose setting
 */
   if (TA == AtlasTrans)
   {
     if (UPPER)
        cpypadU(Diag, N, A, lda, a, Nc);
     else
        cpypadL(Diag, N, A, lda, a, Nc);
      TA = AtlasNoTrans;
   }
/*
 * If matrix is conjugate transposed, keep Uplo same and just conjugate copy 
 * to space and invert the diagonal, and swap the transpose setting
 */
   else if (TA == AtlasConjTrans)
   {
     if (UPPER)
        cpypadUc(Diag, N, A, lda, a, Nc);
     else
        cpypadLc(Diag, N, A, lda, a, Nc);
      TA = AtlasNoTrans;
   }
/* 
 * If the matrix is presently in NoTranspose format, transpose it, which
 * will swap Uplo setting
 */
   else
   {
      if (UPPER)
         trU2L(Diag, N, A, lda, a, Nc);  /* transpose A into lower-triang a */
/*
 *       Transpose Lower matrix to Upper; padding goes at top of Upper matrix
 */
      else  /* matrix is currently lower */
         trL2U(Diag, N, A, lda, a, Nc);
      UPPER = !UPPER;  /* transposition swaps UPLO setting */
   }
/*
 * a now contains correctly padded A^T, so now loop over Mc rows of RHS;
 * Mc is set to a value near 32 so we can get some TLB reuse when doing
 * the transpose of the RHS vectors from row-access to column-access
 */
   b = (UPPER) ? w + nr2 : w;
   for (i=0; i < M; i += Mc, B += Mc2)
   {
      const int mr = Mmin(M-i, Mc);
      Mjoin(PATL,gemoveT)(N, mr, alpha, B, ldb, b, Nc);
      if (UPPER)
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w, Nc);
         ATL_trsmRUN(N, mr, a, w);
      }
      else
      {
         if (nr)
            Mjoin(PATL,gezero)(nr, mr, w+N+N, Nc);
         ATL_trsmRLN(N, mr, a, w);
      }
      Mjoin(PATL,gemoveT)(mr, N, one, b, Nc, B, ldb);
   }
   free(vp);
   return(0);
}
@ROUT ATL_trsmK_LLNU_a1
#include "atlas_misc.h"
#include "atlas_simd.h"

#if defined(ATL_VLEN) && ATL_VLEN > 1
void Mjoin(PATL,trsmK_LLNU_a1)
(
   int N,         /* size of lower triangular matrix A */
   int R,         /* NRHS */
   const TYPE *L, /* NxN lower triangular array to solve with */
   ATL_CSZT ldl,
   TYPE *B,       /* NxR array of X on input, solved to B on output */
   ATL_CSZT ldb
)
{
   int j, i, r;
   const int ldt = ldl+1;
   ATL_CSZT vlmskb = (ATL_VLENb-1);
   TYPE *Bc = B;
   ATL_VTYPE vnone;
   const double none=ATL_rnone;
   ATL_vbcast(vnone, &none);
  
   for (r=0; r < R; r++, Bc += ldb)   /* loop over RHS */
   {
      const TYPE *t=L;
      int nr, nv;
/*
 *    Peel to align b
 */
      for (j=0; j < N-ATL_VLEN; j++, t += ldl)
      {
         int nv;
         const register TYPE b0 = -Bc[j];
         for (i=j+1; i < N && ((size_t)(Bc+i)&vlmskb); i++)
             Bc[i] += b0 * t[i];
         nr = N-i;
         nv = nr & (~(ATL_VLEN-1));
/*
 *       Do the vectorized iterations
 */
         if (nv)
         {
            ATL_VTYPE b0, t0, x0;
            ATL_vbcast(b0, Bc+j);
            ATL_vmul(b0, b0, vnone);
            nr -= nv;
            nv += i;
            for (; i < nv; i += ATL_VLEN)
            {
                ATL_vuld(t0, t+i);
                ATL_vld(x0, Bc+i);
                ATL_vmac(x0, b0, t0);
                ATL_vst(Bc+i, x0);
            }
         }
         for (; i < N; i++)
             Bc[i] += b0 * t[i];
      }
      for (; j < N; j++, t += ldl)
      {
         const register TYPE b0 = -Bc[j];
         for (i=j+1; i < N; i++)
             Bc[i] += b0 * t[i];
      }
   }
}
#else
void Mjoin(PATL,trsmK_LLNU_a1)
(
   int N,         /* size of lower triangular matrix A */
   int R,         /* NRHS */
   const TYPE *L, /* NxN lower triangular array to solve with */
   ATL_CSZT ldl,
   TYPE *B,       /* NxR array of X on input, solved to B on output */
   ATL_CSZT ldb
)
{
   int j, i, r;
   ATL_CSZT ldt = ldl+1;
   TYPE *Bc = B;
  
   for (r=0; r < R; r++, Bc += ldb)   /* loop over RHS */
   {
      const TYPE *t=L;
      for (j=0; j < N; j++, t += ldl)
      {
         const register TYPE b0 = -Bc[j];
         for (i=j+1; i < N; i++)
             Bc[i] += b0 * t[i];
      }
   }
}
#endif
@ROUT ATL_trsmKL_amm
//   #define DEBUG 1
@extract -b @(topd)/cw.inc lang=C -def cwdate 2015
#include "atlas_trsmK.h"
#ifdef DEBUG
   #include "atlas_tst.h"
#endif
/*
 * Handles Left version of trsm for Lower,NoTrans or Upper,Trans
 *
 * On input, L has already be copied to a special format.  If nmblks=CEIL(M/b),
 * then L has nmblks row-panels, but since L is Lower triangular, the first
 * row panel has only one block (triangular), the next row panel contains
 * 2 blocks (first square, second triangular), and the last row panel
 * has nmblks-1 square blocks, followed by one triangular blocks.
 * Each row panel grabs blocks from L, and ends in the diagonal/triangular blk.
 * Square blocks are in the access-major storage required by the provided amm,
 * and have been scaled by -1 during the copy (so that they can be used to
 * subtract off contributions from the solved coefficients in prior triangular
 * blocks).
 * Triangular blocks are presently in block-major storage, with the diagonal
 * entries already inverted for NonUnit (and ignored for Unit), so that we
 * can call trsKL_rk4 directly.  Later, we need to make sure these guys
 * are multiples of 4, and then avoid the malloc/copy in trsmK_rk4.
 *
 * Workspace (w) must have room to store all blocks of B necessary to compute
 * one col-panel of R and one block of c.  Each block is size incw.  
 * incw >= b*NB, where extra length can be added to preserve alignment.
 * So, total size is: (nmblks-1)*incw, incw >= b*NB.
 */
@beginskip
int Mjoin(PATL,trsmKL_rk4)
   (enum ATLAS_SIDE, enum ATLAS_UPLO, enum ATLAS_TRANS, enum ATLAS_DIAG,
    ATL_CINT, ATL_CINT, const SCALAR, const TYPE*, ATL_CINT, TYPE*, ATL_CINT);
#define trsmK(Di_, m_, n_, al_, a_, lda_, b_, ldb_) \
{ \
   ATL_assert(!Mjoin(PATL,trsmKL_rk4)(AtlasLeft, AtlasLower, AtlasNoTrans, Di_,\
                                     m_, n_, al_, a_, lda_, b_, ldb_)); \
}
@endskip
void Mjoin(PATL,ktrsmLLN_rk4)
   (ATL_CINT M, ATL_CINT N, const SCALAR alp, const TYPE *A, TYPE *B,
    ATL_CINT ldb, TYPE *W);
#define trsmK(Di_, m_, n_, al_, a_, b_, ldb_, w_) \
   Mjoin(PATL,ktrsmLLN_rk4)(m_, n_, al_, a_, b_, ldb_, w_)
int Mjoin(PATL,trsmKL_amm)
(
   amminfo_t *mmp,      /* chosen amm kernel info */
   enum ATLAS_DIAG Diag,
   ATL_CUINT mb0,       /* sizeof first triangular block */
   ATL_CUINT nmblks,    /* CEIL(M/mb); M = (nmblks-1)*b+mb0 */
   ATL_CUINT nnblks,    /* FLOOR(N/nb) */
   ATL_CUINT nr,        /* mod(N/nb) */
   const SCALAR alpha,  /* scale factor for rhs */
   const TYPE *L,       /* L, already copied to row-panel storage as above */
   size_t incL,         /* distance between blocks in L */
   TYPE *R,             /* ptr to col-major rhs (B on input, X on output */
   size_t ldr,          /* leading dim of RHS matrix R */
   ATL_CUINT nmu,       /* nmu = mb/mu, assume mod(mb/mu) == 0 */
   ATL_CUINT nnu,       /* nnu = NB/nu, assume mod(NB/nu) == 0 */
   TYPE *c,             /* bxNB with trailing readable space for amm */
   TYPE *W,             /* aligned workspace for amm's B blocks */
   size_t incW          /* stride between b*NB blocks */
)
{
   const int b=mmp->mb, NB=mmp->nb;
   const size_t incR = ldr*NB - (mb0+(nmblks-1)*b);
   ATL_CUINT ku=mmp->ku, ldl0=(mb0+3)&(~3);
   ATL_UINT kb0;
   ATL_UINT r;
   TYPE *pR = R;
   cm2am_t b2blk=mmp->b2blk;       /* col2blk for B, wt alpha=1 */
   ablk2cmat_t blk2c=mmp->Cblk2cm; /* blk2col for C, wt beta of alpha above */
   ammkern_t amm_b0=mmp->amm_b0, amm_b1=mmp->amm_b1;

   if (mb0 != b)
   {
      if (!(mmp->flag & 1))
         amm_b0 = mmp->amm_k1_b0;
      else if (mb0 < mmp->kbmin || (mb0/ku)*ku != mb0)
         amm_b0 = mmp->amm_k1_b0;
   }
/*
 * Loop over the full column-panels of RHS
 */
   for (r=0; r < nnblks; r++, pR += incR)
   {
      ATL_INT i;
      const TYPE *pLb = L+incL;  /* ptr to L's block, loops along rowpan */
/*
 *    First row-panel has no gemm updates
 */
      #ifdef DEBUG
         i = (mb0+3)&(~3);
         Mjoin(PATL,geprint)("L0", i, i, L, i);
      #endif
      trsmK(Diag, mb0, NB, alpha, L, pR, ldr, c);
      #ifdef DEBUG
         Mjoin(PATL,geprint)("B~", mb0, NB, pR, ldr);
      #endif
      if (nmblks > 1)
         b2blk(mb0, NB, ATL_rone, pR, ldr, W); /* put B0 in amm's B storage */
      pR += mb0;
      for (i=1; i < nmblks; i++)     /* loop over rowpans of L and the */
      {                              /* blks in the colpan of B they update */
         TYPE *w = W;
         ammkern_t amm=amm_b0;
         ATL_INT j, kb=mb0;
 
         for (j=0; j != i; j++)      /* apply gemm updates to C */
         {
            const TYPE *LN = pLb + incL;
            TYPE *wN = w + incW;
            amm(nmu, nnu, kb, pLb, w, c, LN, wN, c);
            pLb = LN;
            w = wN;
            amm = amm_b1;
            kb = b;
         }
/*
 *       Subtract all solved co-efficients from RHSs using accumulated GEMM
 *       updates.  We apply alpha here, so the triangular solve will use alpha=1
 */
         blk2c(b, NB, ATL_rone, c, alpha, pR, ldr);
/*
 *       Now complete solve for X on this block of RHS with TRSM
 */
         #ifdef DEBUG
            Mjoin(PATL,geprint)("Lx", b, b, pLb, b);
         #endif
         trsmK(Diag, b, NB, ATL_rone, pLb, pR, ldr, c);
         pLb += incL;
/*
 *       If this isn't the last row-panel, I'll need the final answer copied
 *       to amm's B storage for subtracting these equations' contributions out
 */
         if (i != nmblks-1)
            b2blk(b, NB, ATL_rone, pR, ldr, w); /* put Bi in amm's B storage */
         pR += b;
      }
   }
/*
 * Is there a partial NB block at end?  This block must handle padding for both
 * B and C.
 */
   if (nr)
   {
      const int nu=mmp->nu, nnu=(nr+nu-1)/nu, NB=nnu*nu;
      ATL_INT i;
      const TYPE *pLb = L+incL;  /* ptr to L's block, loops along rowpan */
/*
 *    First row-panel has no gemm updates
 */
      #ifdef DEBUG
         i = (mb0+3)&(~3);
         Mjoin(PATL,geprint)("L0", i, i, L, i);
      #endif
      trsmK(Diag, mb0, nr, alpha, L, pR, ldr, c);
      #ifdef DEBUG
         Mjoin(PATL,geprint)("B~", mb0, NB, pR, ldr);
      #endif
      if (nmblks > 1)
         b2blk(mb0, nr, ATL_rone, pR, ldr, W); /* put B0 in amm's B storage */
      pR += mb0;
      for (i=1; i < nmblks; i++)     /* loop over rowpans of L and the */
      {                              /* blks in the colpan of B they update */
         TYPE *w = W;
         ammkern_t amm=amm_b0;
         ATL_INT j, kb=mb0;
 
         for (j=0; j != i; j++)      /* apply gemm updates to C */
         {
            const TYPE *LN = pLb + incL;
            TYPE *wN = w + incW;
            amm(nmu, nnu, kb, pLb, w, c, LN, wN, c);
            pLb = LN;
            w = wN;
            amm = amm_b1;
            kb = b;
         }
/*
 *       Subtract all solved co-efficients from RHSs using accumulated GEMM
 *       updates.  We apply alpha here, so the triangular solve will use alpha=1
 */
         blk2c(b, nr, ATL_rone, c, alpha, pR, ldr);
/*
 *       Now complete solve for X on this block of RHS with TRSM
 */
         #ifdef DEBUG
            Mjoin(PATL,geprint)("Lx", b, b, pLb, b);
         #endif
         trsmK(Diag, b, nr, ATL_rone, pLb, pR, ldr, c);
         pLb += incL;
/*
 *       If this isn't the last row-panel, I'll need the final answer copied
 *       to amm's B storage for subtracting these equations' contributions out
 */
         if (i != nmblks-1)
            b2blk(b, nr, ATL_rone, pR, ldr, w); /* put Bi in amm's B storage */
         pR += b;
      }
   }
   return(0);
}

@ROUT ATL_trsmK_L2blk
#include "atlas_trsmK.h"
/*
 * Take column major L, put it in block-major/amm form for use in trsm_amm.
 * In particular, diagonal blocks are copied and padded to match rank-4-based
 * trsm kernel, while blocks below the diagonal are copied to amm format.
 */
void Mjoin(PATL,trsmK_L2blk)
(
   enum ATLAS_DIAG Diag,
   const int mb0,   /* size of first triangular block */
   const int b,     /* size of all other blocks */
   int nmblks,      /* number of blocks of size b (not incl mb0) */
   const TYPE *L,
   size_t ldl,
   TYPE *W,
   size_t incW,
   cm2am_t a2blk    /* copy rect blks to amm storage */
)
{
   const int ldW0=(mb0+3)&(~3), ldW=(b+3)&(~3);
   const size_t incL = ldl*b, incL0 = ldl*mb0;
   ATL_UINT i;

   #ifdef DEBUG
      ATL_assert(incW >= ldW0*ldW0 && incW >= ldW*ldW);
   #endif

   Mjoin(PATL,trcpypad4L)(Diag, mb0, L, ldl, W, ldW0);
   L += mb0;
   W += incW;
   for (i=1; i < nmblks; i++)
   {
      const TYPE *l=L;
      ATL_UINT j, kb=mb0;

      a2blk(mb0, b, ATL_rnone, l, ldl, W);
      l += incL0; W += incW;

      for (j=1; j < i; j++, l += incL, W += incW)
         a2blk(b, b, ATL_rnone, l, ldl, W);

      Mjoin(PATL,trcpypad4L)(Diag, b, l, ldl, W, ldW);
      W += incW;
      L += b;
   }
}
@ROUT ATL_trsmKL_amm
#define L2blk Mjoin(PATL,trsmK_L2blk)
int Mjoin(PATL,trsmL_amm)
(
   enum ATLAS_SIDE Side,
   enum ATLAS_UPLO Uplo,
   enum ATLAS_TRANS TA,
   enum ATLAS_DIAG Diag,
   ATL_CINT  M,        /* size of triangular matrix A */
   ATL_CINT  N,        /* number of RHS in B */
   const SCALAR alpha, /* scale factor for B */
   const TYPE *T,      /* MxM triangular matrix A */
   ATL_CINT  ldt,
   TYPE *R,            /* on input, B, on output X, of A x = b */
   ATL_CINT  ldr       /* leading dim of R */
)
{
   amminfo_t mminf;
   ATL_UINT nmblks, nsqblks, nnblks, nb, nmu, nnu, mu, nu, nr;
   size_t wrksz, szL, szB, szC, incL, incW;
   void *vp=NULL;
   TYPE *pC, *W, *L;
   ATL_INT i;
   int b, mb0;
   #ifdef DEBUG
      int *eBV=NULL;
      TYPE *myB;

      myB = malloc(M*N*sizeof(TYPE));
      ATL_assert(myB);
      Mjoin(PATL,geprint)("Lg", M, M, T, ldt);
      Mjoin(PATL,gecopy)(M, N, R, ldr, myB, M);
      if (Diag == AtlasUnit)
         Mjoin(PATL,reftrsmLLNU)(M, N, alpha, T, ldt, myB, M);
      else
         Mjoin(PATL,reftrsmLLNN)(M, N, alpha, T, ldt, myB, M);
      Mjoin(PATL,geprint)("Xg", M, N, myB, M);
   #endif
   if (Side == AtlasRight)
      return(1);
   if (Uplo == AtlasUpper && TA == AtlasNoTrans)
      return(2);
   if (Uplo == AtlasLower && TA == AtlasTrans)
      return(3);
   #if 0
   if (M < 24)   /* tiny triangles should just use trsmKL_rk4 directly */
      return(4); /* since our B can't be much bigger than 4 */
   if (N < 4)
      return(5);
   #endif
   if (alpha == 0.0)
      i = 0;
   else if (alpha == 1.0)
      i = 1;
   else
      i = (alpha == -1.0) ? -1 : 2;
   b = Mjoin(PATL,GetTrsmInfo)(&mminf, i, TA, M, N, alpha);
   if (b < 4)
      return(Mjoin(PATL,trsmKL_rk4)(Side, Uplo, TA, Diag, M, N, alpha, 
                                    T, ldt, R, ldr));
      
   nb = mminf.nb;
   mu = mminf.mu;
   nu = mminf.nu;
   nmblks = ((M+b-1)/b);
   nnblks = N / nb;
   nr = N - nnblks*nb;
   nsqblks = ((nmblks-1)*nmblks)>>1; /* # of square blks below diag */
   incL = (b+3)&(~3);                /* triang blks must be multiples of 4 */
   incL *= incL;
   incW = b * nb;
   szL = incL * (nsqblks + nmblks);  /* don't store upper blks of matrix */
   szB = (nmblks-1) * incW;          /* last blk not copied to B form */
   szC = incW;                       /* C just 1 block in size */
   wrksz = 3*ATL_Cachelen + ATL_MulBySize(szL+szB+szC);
   if (wrksz < ATL_MaxMalloc)
      vp = malloc(wrksz);
   if (!vp)
      return(4);
   pC = ATL_AlignPtr(vp);
   W = pC + szC;            /* W is workspace for amm's B */
   W = ATL_AlignPtr(W);
   L = W + szB;             /* L is workspace for triangular matrix */
   L = ATL_AlignPtr(L);     /* put it last so last diag blk is amm buffer */
   i = nmblks*b;
   if (i == M)
      mb0 = b;
   else
      mb0 = M - i + b;
   nmu = b / mu;
   nnu = nb / nu;
   L2blk(Diag, mb0, b, nmblks, T, ldt, L, incL, mminf.a2blk);
   ATL_assert(!Mjoin(PATL,trsmKL_amm)(&mminf, Diag, mb0, nmblks, nnblks,
                                      nr, alpha, L, incL, R, ldr, nmu, nnu,
                                      pC, W, incW));
   #ifdef DEBUG
      ATL_assert(nmu*mu == b);
      ATL_assert(b*(nmblks-1)+mb0 == M);
      i = (M+3)&(~3);
      eBV = Mjoin(PATL,cmpmatBV)(0, (1.0*M)*M*1e-15, M, N, myB, M, R, ldr);
      Mjoin(PATL,geprint)("Xc", M, N, R, ldr);
      Mjoin(PATL,gediff)(M, N, R, ldr, myB, M, myB, M);
      ATL_print2dBV(M, N, eBV);
      free(eBV);
      free(myB);
   #endif
   free(vp);
   return(0);
}
