@ROUT ATL_damm12x4x1_aarch64-A53.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwauth "Rakib Hasan"
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm12x4x1_aarch64-A53.S
@ROUT ATL_damm12x4x6_aarch64-A57.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015 -define cwauth "Rakib Hasan"
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm12x4x6_aarch64-A57.S
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
   @define mu @4@
   @define nu @4@
@ROUT ATL_dammm3x3x256_sse2.S
   @define mu @6@
   @define nu @3@
@ROUT ATL_dammm4x3x6_arm64.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014 
@extract -b @(topd)/kernel/ClintWhaley/ATL_dammm4x3x6_arm64.S
@ROUT ATL_damm5x5x2_aarch64.S
/*
 * This file adapted & tuned by Dave Nuechterlein to work on ARM64 from 
 * R. Clint Whaley's original ATL_damm5x5x2_arm.S
 */
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014 -define cwauth "Clint Whaley" -def contrib "Dave Nuechterlein"
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm5x5x2_aarch64.S
@ROUT ATL_dammm12x6x2_vsx.S
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
@extract -b @(topd)/kernel/ClintWhaley/ATL_dammm12x6x2_vsx.S
@endextract
@ROUT ATL_ammm12x3x1_fma3.S
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
@extract -b @(topd)/kernel/ClintWhaley/ATL_ammm12x3x1_fma3.S
@ROUT ATL_samm4x6x2b_aarch64.S
/*
 * This file written & tuned to use ARM64 SIMD  by Dave Nuechterlein using
 * R. Clint Whaley's original ATL_samm4x6x2_arm.S as a template.
 */
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014 -define cwauth "Dave Nuechterlein" -def contrib "R. Clint Whaley"
@extract -b @(topd)/kernel/ClintWhaley/ATL_samm4x6x2b_aarch64.S
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
#define vmovapd vmovaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
@ROUT ATL_dammm2x4x1_sse2.S
#define KK      %rdx
#define KK0     %r11
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pA0     %r13
#define incAm   %r14
#define pfB     %r15
#define FSIZE 6*8
@ROUT ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
@ROUT ATL_dammm2x4x256_sse2.S
#define pfC     %r13
@ROUT ATL_dammm3x3x256_sse2.S

#define rA0     %xmm0
#define rA1     %xmm1
#define rA2     %xmm2
#define rB0     %xmm3
#define rB1     %xmm4
#define rB2     %xmm5
#define rC00    %xmm6
#define rC10    %xmm7
#define rC20    %xmm8
#define rC01    %xmm9
#define rC11    %xmm10
#define rC21    %xmm11
#define rC02    %xmm12
#define rC12    %xmm13
#define rC22    %xmm14
#define rm0     %xmm15
#define FSIZE 4*8
@ROUT ATL_dammm2x4x1_sse2.S
#define FSIZE 6*8
@ROUT ATL_dammm2x4x256_sse2.S
#define FSIZE 4*8
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S

#define rA0     %xmm0
#define rA1     %xmm1
#define rB0     %xmm2
#define rB1     %xmm3
#define rB2     %xmm4
#define rB3     %xmm5
#define rC00    %xmm6
#define rC10    %xmm7
#define rC01    %xmm8
#define rC11    %xmm9
#define rC02    %xmm10
#define rC12    %xmm11
#define rC03    %xmm12
#define rC13    %xmm13
#define rm0     %xmm14
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S 
#define unpckhpd movhlps
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
/* #define movddup pshufd $0x44, */
#ifndef pref
   #define pref prefetcht1
#endif
#ifndef prefB
   #define prefB prefetcht1
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
   movq    %r13, 24(%rsp)
@ROUT ATL_dammm2x4x1_sse2.S 
   movq    %r14, 32(%rsp)
   movq    %r15, 40(%rsp)
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pf      /* pf = pBn */
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm3x3x256_sse2.S
   cmp pf, pB                   /* if (pBn == pB) */
   CMOVE pfB, pf                /* if (pBn == pB) pfB = pAn */
   CMOVEq FSIZE+24(%rsp), pfB   /* if (pbN == pB) pfB = pCn */
@ROUT ATL_dammm2x4x256_sse2.S
   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
@beginskip
   cmp $0, pf
   je BADPFB
   cmp pf, pB
   je BADPFB
.local PFSETUP
PFSETUP:
   movq FSIZE+8(%rsp), pfB
   cmp $0, pfB
   je BADPF_2
   cmp pf, pA
   je BADPF_2
PFSETUP_2:
@endskip
   mov $8*@(mu)*@(nu), incPF
@ROUT ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
/*
 * Extend range of small operands by starting at -128
 */
@ROUT ATL_dammm3x3x256_sse2.S `         movddup (pB), rC00`
   sub $-128, pA
   sub $-128, pB
   mov $KB*@(mu)*8, incAm           /* incAm = KB*MU*size */
@ROUT ATL_dammm3x3x256_sse2.S
   sub $-128, pf
   sub $-128, pC
   sub $-128, pfB
@ROUT ATL_dammm2x4x1_sse2.S
   mov KK, incAm                /* incAm = K */
   shl $5, incAm                /* incAm = K*MU*size = K*4*8 = K*32 = K << 5 */
   mov KK, KK0
   mov pA, pA0
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
   movq pB, pB0

   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
@ROUT ATL_dammm3x3x256_sse2.S
         movapd  -128(pA), rA0
         movapd rC00, rC10
         mulpd rA0, rC00
         movapd rC10, rC20
         movapd  -112(pA), rA1
         mulpd rA1, rC10
         movapd  -96(pA), rA2
         mulpd rA2, rC20
         movddup -120(pB), rC01
         movapd rC01, rC11
         mulpd rA0, rC01
         movapd rC11, rC21
         mulpd rA1, rC11
         mulpd rA2, rC21
         movddup -112(pB), rC02
         movapd rC02, rC12
         mulpd rA0, rC02
         #if KB > 1
            movddup -104(pB), rB0
         #endif
         movapd rC12, rC22
         mulpd rA1, rC12
            prefC -128(pC)
         mulpd rA2, rC22

ALIGN8
@iexp ao -80 0 +
@iexp bo -96 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movapd @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            movapd rA0, rm0
            mulpd rB0, rm0
            addpd rm0, rC00
            movapd @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            movapd rA1, rm0
            mulpd rB0, rm0
            addpd rm0, rC10
            movapd @(ao)(pA), rA2
   @iexp ao @(ao) 16 +
            movapd rA2, rm0
            mulpd rA2, rB0
            addpd rB0, rC20
   
            movddup @(bo)(pB), rB1
   @iexp bo @(bo) 8 +
            movapd rA0, rm0
            mulpd rB1, rm0
            addpd rm0, rC01
            movddup @(bo)(pB), rB2
   @iexp bo @(bo) 8 +
            movapd rA1, rm0
            mulpd rB1, rm0
            addpd rm0, rC11
            #if KB > @(k)
               movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 +
            #endif
            mulpd rA2, rB1
            addpd rB1, rC21
   
   @iif k = 2
            prefC -64(pC)
   @endiif
   @iif k = 3
               #if KB < 78
                  pref -128(pf)
               #endif
   @endiif
   @iif k = 4
               #if KB < 72
               prefB -128(pfB)
               #endif
   @endiif
            mulpd rB2, rA0
            addpd rA0, rC02
   @iif k = 3
@skip               pref 64-128(pf)
   @endiif
            mulpd rB2, rA1
            addpd rA1, rC12
   @iif k = 4
               #if KB < 72
               prefB (pfB)
               #endif
   @endiif
   @iif k = 3
               #if KB < 78
               pref 128(pf)
               #endif
   @endiif
            mulpd rB2, rA2
            addpd rA2, rC22
   @iif k = 2
            prefC (pC)
   @endiif
   @iif k = 3
               #if KB < 78
                  add incPF, pf
               #endif
   @endiif
   @iif k = 4
               #if KB < 72
                  add incPF, pfB
               #endif
   @endiif
         #endif
@endiwhile
/*
 *       Bring in C if necessary, and store out final answer
 */
         add $KB*@(nu)*8, pB
         #if defined(BETA1) || defined(BETAN1)
            BETCOP -128(pC), rC00
            movapd rC00, -128(pC)
               movddup -128(pB), rC00
            BETCOP 16-128(pC), rC10
            movapd rC10, 16-128(pC)
            BETCOP 32-128(pC), rC20
            movapd rC20, 32-128(pC)
            BETCOP 48-128(pC), rC01
            movapd rC01, 48-128(pC)
            BETCOP 64-128(pC), rC11
            movapd rC11, 64-128(pC)
            BETCOP 80-128(pC), rC21
            movapd rC21, 80-128(pC)
            BETCOP 96-128(pC), rC02
            movapd rC02, 96-128(pC)
            BETCOP 112-128(pC), rC12
            movapd rC12, 112-128(pC)
            BETCOP (pC), rC22
            movapd rC22, (pC)
         #else
            movapd rC00, -128(pC)
               movddup -128(pB), rC00
            movapd rC10, 16-128(pC)
            movapd rC20, 32-128(pC)
            movapd rC01, 48-128(pC)
            movapd rC11, 64-128(pC)
            movapd rC21, 80-128(pC)
            movapd rC02, 96-128(pC)
            movapd rC12, 112-128(pC)
            movapd rC22, (pC)
         #endif
         add $144, pC
      sub $1, nnu
      jnz MNLOOP
               movddup -128(pB0), rC00
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP
@ROUT ATL_dammm2x4x256_sse2.S
         movapd -128(pB), rB1
         movddup rB1, rB0
         movapd -128(pA), rC00

         movapd rC00, rC01
         mulpd rB0, rC00
         unpckhpd rB1, rB1
         movapd -112(pA), rC10
         movapd rC10, rC11
         mulpd rB0, rC10

         movapd -112(pB), rB3
         movddup rB3, rB2
         movapd rC01, rC02
         mulpd rB1, rC01
         unpckhpd rB3, rB3
         movapd rC11, rC12
         mulpd rB1, rC11
         #if KB > 1
            movapd -96(pB), rB1
         #else
            pref (pf)
         #endif

         #if KB > 1
            movapd -96(pA), rA0
         #else
            pref 64(pf)
         #endif
         movapd rC02, rC03
         mulpd rB2, rC02
         movapd rC12, rC13
         mulpd rB2, rC12
         #if KB > 1
            movddup rB1, rB0
         #else
            add incPF, pf
         #endif

         mulpd rB3, rC03
         prefC (pC)
         mulpd rB3, rC13
         prefC 64(pC)
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -80 0 +
@iexp bo -80 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movapd rB0, rm0
            mulpd rA0, rm0
            addpd rm0, rC00
            movapd @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            mulpd rA1, rB0
            addpd rB0, rC10
            unpckhpd rB1, rB1

            movapd rB1, rm0
            mulpd rA0, rm0
            addpd rm0, rC01
            movapd @(bo)(pB), rB3
   @iexp bo @(bo) 16 +
            mulpd rA1, rB1
            addpd rB1, rC11
            movddup rB3, rB2

            movapd rB2, rm0
            mulpd rA0, rm0
            addpd rm0, rC02
            unpckhpd rB3, rB3
            mulpd rA1, rB2
            addpd rB2, rC12

            #if KB > @(k)
               movapd @(bo)(pB), rB1
   @iexp bo @(bo) 16 +
            #elif KB == @(k)
               pref (pf)
            #endif
            mulpd rB3, rA0
            addpd rA0, rC03
            #if KB > @(k)
               movapd @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #elif KB == @(k)
               pref (pfC)
               add incPF, pfC
            #endif
            mulpd rA1, rB3
            addpd rB3, rC13
            #if KB > @(k)
               movddup rB1, rB0
            #elif KB == @(k)
               prefetcht1 64(pfB)
               add incPF, pfB
               add incPF, pf
            #endif
         #endif   
@endiwhile
@ROUT ATL_dammm2x4x1_sse2.S
         movapd (pB), rB1
         movddup rB1, rB0
         movapd (pA), rC00

         movapd rC00, rC01
         mulpd rB0, rC00
         unpckhpd rB1, rB1
         movapd 16(pA), rC10
         movapd rC10, rC11
         mulpd rB0, rC10

         movapd 16(pB), rB3
         movddup rB3, rB2
         movapd rC01, rC02
         mulpd rB1, rC01
         pref (pf)
         movapd rC11, rC12
         mulpd rB1, rC11
         unpckhpd rB3, rB3

         pref 64(pf)
         movapd rC02, rC03
         mulpd rB2, rC02
         movapd rC12, rC13
         mulpd rB2, rC12

         prefC (pC)
         mulpd rB3, rC03
         add $32, pB
         prefC 64(pC)
         mulpd rB3, rC13
         add incPF, pf
         add $32, pA
         sub $1, KK
         jz DONEK

         KLOOP:
            movapd (pB), rB1
            movddup rB1, rB0
            movapd (pA), rA0
            movapd rB0, rm0
            mulpd rA0, rm0
            addpd rm0, rC00
            movapd 16(pA), rA1
            movapd rB0, rm0
            mulpd rA1, rm0
            addpd rm0, rC10
            unpckhpd rB1, rB1

            movapd 16(pB), rB3
            movapd rB1, rm0
            mulpd rA0, rm0
            addpd rm0, rC01
            movddup rB3, rB2
            movapd rB1, rm0
            mulpd rA1, rm0
            addpd rm0, rC11
            unpckhpd rB3, rB3

            movapd rB2, rm0
            mulpd rA0, rm0
            addpd rm0, rC02
            add $32, pA
            movapd rB2, rm0
            mulpd rA1, rm0
            addpd rm0, rC12
            add $32, pB

            movapd rB3, rm0
            mulpd rA0, rm0
            addpd rm0, rC03
            movapd rB3, rm0
            mulpd rA1, rm0
            addpd rm0, rC13
         sub $1, KK
         jnz KLOOP
         DONEK:
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movapd rC00, (pC)
            BETCOP 16(pC), rC10
            movapd rC10, 16(pC)
            BETCOP 32(pC), rC01
            movapd rC01, 32(pC)
            BETCOP 48(pC), rC11
            movapd rC11, 48(pC)
            BETCOP 64(pC), rC02
            movapd rC02, 64(pC)
            BETCOP 80(pC), rC12
            movapd rC12, 80(pC)
            BETCOP 96(pC), rC03
            movapd rC03, 96(pC)
            BETCOP 112(pC), rC13
            movapd rC13, 112(pC)
         #else
            movapd rC00, (pC)
            movapd rC10, 16(pC)
            movapd rC01, 32(pC)
            movapd rC11, 48(pC)
            movapd rC02, 64(pC)
            movapd rC12, 80(pC)
            movapd rC03, 96(pC)
            movapd rC13, 112(pC)
         #endif
         sub $-128, pC
@ROUT ATL_dammm2x4x1_sse2.S 
         mov KK0, KK
         mov pA0, pA
@ROUT ATL_dammm2x4x256_sse2.S
         add $KB*4*8, pB
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
@ROUT ATL_dammm2x4x1_sse2.S
      add incAm, pA0
      mov pA0, pA
@ROUT ATL_dammm2x4x256_sse2.S
      add incAm, pA
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
   sub $1, nmu
   jnz MNLOOP

@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S
   movq    24(%rsp), %r13
@ROUT ATL_dammm2x4x1_sse2.S
   movq    32(%rsp), %r14
   movq    40(%rsp), %r15
@ROUT ATL_dammm2x4x1_sse2.S ATL_dammm2x4x256_sse2.S ATL_dammm3x3x256_sse2.S
   add $FSIZE, %rsp
   ret
@beginskip
/*
 * Can't use next B as pointer as usual, see if we can use next A
 */
.local BADPFB
BADPFB:
   movq 16(%rsp), pf
   cmp $0, pf
   je BADPFAB
   cmp pf, pA
   jne PFSETUP
/*
 * Can't use A or B as ptr, try C
 */
.local BADPFAB
BADPFAB:
   movq 24(%rsp), pf
   cmp $0, pf
   je BADPFABC
   cmp pf, pA
   jne PFSETUP
/*
 * No next block, so just fetch this time's A ahead
 */
.local BADPFABC
BADPFABC:
#ifndef KB
   #define KB 40
#endif
#ifndef MB
   #define MB KB
#endif
   lea KB*MB*4(pA), pf
   jmp PFSETUP
BADPF_2:
   movq 24(%rsp), pfB
   jmp PFSETUP_2
@endskip
@ROUT ATL_dammm_nb8_avx.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"

#define pA %rcx
#define pB %r8
#define pC %r9
#define rA00    %ymm0
#define rA10    %ymm1
#define rA01    %ymm2
#define rA11    %ymm3
#define rA02    %ymm4
#define rA12    %ymm5
#define rA03    %ymm6
#define rA13    %ymm7
#define rB0     %ymm8
#define rB1     %ymm9
#define rC00    %ymm10
#define rC10    %ymm11
#define rC01    %ymm12
#define rC11    %ymm13
#define rb0     %ymm14
#define rb0_x   %xmm14
#define rb1     %ymm15
#define rb1_x   %xmm15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
ALIGN16
.global ATL_asmdecor(ATL_USERMM)
ATL_asmdecor(ATL_USERMM):
   vmovapd (pA), rA00
   movapd 16(pA), rA10
   movapd 32(pA), rA01
   movapd 48(pA), rA11
   movapd 64(pA), rA02
   movapd 80(pA), rA12
   movapd 96(pA), rA03
   movapd 112(pA), rA13

/*
 * ======================================================
 * rC00 = rA00 * rB00 + rA01*rB10 + rA02*rB20 + rA03*rB30
 * rC10 = rA10 * rB00 + rA11*rB10 + rA12*rB20 + rA13*rB30
 * rC01 = rA00 * rB01 + rA01*rB11 + rA02*rB21 + rA03*rB31
 * rC11 = rA10 * rB01 + rA11*rB11 + rA12*rB21 + rA13*rB31
 * ======================================================
 */
/*
 * rC00 = rA00 * rB0
 * rC10 = rA10 * rB0
 * rC01 = rA00 * rB1
 * rC11 = rA10 * rB1
 */
   #if defined(BETA0) || defined(BETAN1)
      movddup (pB), rC10
      movapd rC10, rC00
      mulpd rA00, rC00
      movddup 32(pB), rC11
      mulpd rA10, rC10
      movapd rC11, rC01
      mulpd rA00, rC01
      mulpd rA10, rC11
   #else
      movddup (pB), rB0
      movddup 32(pB), rB1
      movapd rB0, m0
      mulpd rA00, m0
      movapd (pC), rC00
      addpd m0, rC00
      mulpd rA10, rB0
      movapd 16(pC), rC10
      addpd rB0, rC10
      movapd rB1, m0
      mulpd rA00, m0
      movapd 32(pC), rC01
      addpd m0, rC01
      mulpd rA10, rB1
      movapd 48(pC), rC11
      addpd rB1, rC11
   #endif
/*
 * rC00 = rA01*rB10 
 * rC10 = rA11*rB10 
 * rC01 = rA01*rB11 
 * rC11 = rA11*rB11 
 */
   movddup 8(pB), rB0
   movapd rB0, m0
   mulpd rA01, m0
   addpd m0, rC00
   mulpd rA11, rB0
   addpd rB0, rC10
   movddup 40(pB), rB1
   movapd rB1, m0
   mulpd rA01, m0
   addpd m0, rC01
   mulpd rA11, rB1
   addpd rB1, rC11
/*
 * rC00 = rA02*rB20
 * rC10 = rA12*rB20
 * rC01 = rA02*rB21
 * rC11 = rA12*rB21
 */
   movddup 16(pB), rB0
   movapd rA02, m0
   mulpd rB0, m0
   addpd m0, rC00
   mulpd rA12, rB0
   addpd rB0, rC10
   movapd rA02, m0
   movddup 48(pB), rB1
   mulpd rB1, m0
   addpd m0, rC01
   mulpd rA12, rB1
   addpd rB1, rC11
/*
 * rC00 = rA03*rB30
 * rC10 = rA13*rB30
 * rC01 = rA03*rB31
 * rC11 = rA13*rB31
 */
   movddup 24(pB), rB0
   movapd rA03, m0
   mulpd rB0, m0
   addpd m0, rC00
   #ifdef BETAN1
      subpd (pC), rC00
   #endif
   movapd rC00, (pC)
   mulpd rA13, rB0
   addpd rB0, rC10
   #ifdef BETAN1
      subpd 16(pC), rC10
   #endif
   movapd rC10, 16(pC)
   movapd rA03, m0
   movddup 56(pB), rB1
   mulpd rB1, m0
   addpd m0, rC01
   #ifdef BETAN1
      subpd 32(pC), rC01
   #endif
   movapd rC01, 32(pC)
   mulpd rA13, rB1
   addpd rB1, rC11
   #ifdef BETAN1
      subpd 48(pC), rC11
   #endif
   movapd rC11, 48(pC)

/*
 * rC00 = rA00 * rB0
 * rC10 = rA10 * rB0
 * rC01 = rA00 * rB1
 * rC11 = rA10 * rB1
 */
   #if defined(BETA0) || defined(BETAN1)
      movddup 64(pB), rC10
      movapd rC10, rC00
      mulpd rA00, rC00
      movddup 96(pB), rC11
      mulpd rA10, rC10
      movapd rC11, rC01
      mulpd rA00, rC01
      mulpd rA10, rC11
   #else
      movddup 64(pB), rB0
      movddup 96(pB), rB1
      movapd rB0, m0
      mulpd rA00, m0
      movapd 64(pC), rC00
      addpd m0, rC00
      mulpd rA10, rB0
      movapd 80(pC), rC10
      addpd rB0, rC10
      movapd rB1, m0
      mulpd rA00, m0
      movapd 96(pC), rC01
      addpd m0, rC01
      mulpd rA10, rB1
      movapd 112(pC), rC11
      addpd rB1, rC11
   #endif
/*
 * rC00 = rA01*rB10 
 * rC10 = rA11*rB10 
 * rC01 = rA01*rB11 
 * rC11 = rA11*rB11 
 */
   movddup 72(pB), rB0
   movapd rB0, m0
   mulpd rA01, m0
   addpd m0, rC00
   mulpd rA11, rB0
   addpd rB0, rC10
   movddup 104(pB), rB1
   movapd rB1, m0
   mulpd rB1, rA01
   addpd rA01, rC01
   mulpd rA11, rB1
   addpd rB1, rC11
/*
 * rC00 = rA02*rB20
 * rC10 = rA12*rB20
 * rC01 = rA02*rB21
 * rC11 = rA12*rB21
 */
   movddup 80(pB), rB0
   movapd rA02, m0
   mulpd rB0, m0
   addpd m0, rC00
   mulpd rA12, rB0
   addpd rB0, rC10
   movddup 112(pB), rB1
   mulpd rB1, rA02
   addpd rA02, rC01
   mulpd rA12, rB1
   addpd rB1, rC11
/*
 * rC00 = rA03*rB30
 * rC10 = rA13*rB30
 * rC01 = rA03*rB31
 * rC11 = rA13*rB31
 */
   movddup 88(pB), rB0
   movapd rA03, m0
   mulpd rB0, m0
   addpd m0, rC00
   #ifdef BETAN1
      subpd 64(pC), rC00
   #endif
   movapd rC00, 64(pC)
   mulpd rA13, rB0
   addpd rB0, rC10
   #ifdef BETAN1
      subpd 80(pC), rC10
   #endif
   movapd rC10, 80(pC)
   movddup 120(pB), rB1
   mulpd rB1, rA03
   addpd rA03, rC01
   #ifdef BETAN1
      subpd 96(pC), rC01
   #endif
   movapd rC01, 96(pC)
   mulpd rA13, rB1
   addpd rB1, rC11
   #ifdef BETAN1
      subpd 112(pC), rC11
   #endif
   movapd rC11, 112(pC)
   ret

@ROUT ATL_dammm_nb4_sse2.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"

#define pA %rcx
#define pB %r8
#define pC %r9
#define m0      %xmm0
#define rA00    %xmm1
#define rA10    %xmm2
#define rA01    %xmm3
#define rA11    %xmm4
#define rA02    %xmm5
#define rA12    %xmm6
#define rA03    %xmm7
#define rA13    %xmm8
#define rB0     %xmm9
#define rB1     %xmm10
#define rC00    %xmm11
#define rC10    %xmm12
#define rC01    %xmm13
#define rC11    %xmm14
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
ALIGN16
.global ATL_asmdecor(ATL_USERMM)
ATL_asmdecor(ATL_USERMM):
   movapd (pA), rA00
   movapd 16(pA), rA10
   movapd 32(pA), rA01
   movapd 48(pA), rA11
   movapd 64(pA), rA02
   movapd 80(pA), rA12
   movapd 96(pA), rA03
   movapd 112(pA), rA13

/*
 * ======================================================
 * rC00 = rA00 * rB00 + rA01*rB10 + rA02*rB20 + rA03*rB30
 * rC10 = rA10 * rB00 + rA11*rB10 + rA12*rB20 + rA13*rB30
 * rC01 = rA00 * rB01 + rA01*rB11 + rA02*rB21 + rA03*rB31
 * rC11 = rA10 * rB01 + rA11*rB11 + rA12*rB21 + rA13*rB31
 * ======================================================
 */
/*
 * rC00 = rA00 * rB0
 * rC10 = rA10 * rB0
 * rC01 = rA00 * rB1
 * rC11 = rA10 * rB1
 */
   #if defined(BETA0) || defined(BETAN1)
      movddup (pB), rC10
      movapd rC10, rC00
      mulpd rA00, rC00
      movddup 32(pB), rC11
      mulpd rA10, rC10
      movapd rC11, rC01
      mulpd rA00, rC01
      mulpd rA10, rC11
   #else
      movddup (pB), rB0
      movddup 32(pB), rB1
      movapd rB0, m0
      mulpd rA00, m0
      movapd (pC), rC00
      addpd m0, rC00
      mulpd rA10, rB0
      movapd 16(pC), rC10
      addpd rB0, rC10
      movapd rB1, m0
      mulpd rA00, m0
      movapd 32(pC), rC01
      addpd m0, rC01
      mulpd rA10, rB1
      movapd 48(pC), rC11
      addpd rB1, rC11
   #endif
/*
 * rC00 = rA01*rB10 
 * rC10 = rA11*rB10 
 * rC01 = rA01*rB11 
 * rC11 = rA11*rB11 
 */
   movddup 8(pB), rB0
   movapd rB0, m0
   mulpd rA01, m0
   addpd m0, rC00
   mulpd rA11, rB0
   addpd rB0, rC10
   movddup 40(pB), rB1
   movapd rB1, m0
   mulpd rA01, m0
   addpd m0, rC01
   mulpd rA11, rB1
   addpd rB1, rC11
/*
 * rC00 = rA02*rB20
 * rC10 = rA12*rB20
 * rC01 = rA02*rB21
 * rC11 = rA12*rB21
 */
   movddup 16(pB), rB0
   movapd rA02, m0
   mulpd rB0, m0
   addpd m0, rC00
   mulpd rA12, rB0
   addpd rB0, rC10
   movapd rA02, m0
   movddup 48(pB), rB1
   mulpd rB1, m0
   addpd m0, rC01
   mulpd rA12, rB1
   addpd rB1, rC11
/*
 * rC00 = rA03*rB30
 * rC10 = rA13*rB30
 * rC01 = rA03*rB31
 * rC11 = rA13*rB31
 */
   movddup 24(pB), rB0
   movapd rA03, m0
   mulpd rB0, m0
   addpd m0, rC00
   #ifdef BETAN1
      subpd (pC), rC00
   #endif
   movapd rC00, (pC)
   mulpd rA13, rB0
   addpd rB0, rC10
   #ifdef BETAN1
      subpd 16(pC), rC10
   #endif
   movapd rC10, 16(pC)
   movapd rA03, m0
   movddup 56(pB), rB1
   mulpd rB1, m0
   addpd m0, rC01
   #ifdef BETAN1
      subpd 32(pC), rC01
   #endif
   movapd rC01, 32(pC)
   mulpd rA13, rB1
   addpd rB1, rC11
   #ifdef BETAN1
      subpd 48(pC), rC11
   #endif
   movapd rC11, 48(pC)

/*
 * rC00 = rA00 * rB0
 * rC10 = rA10 * rB0
 * rC01 = rA00 * rB1
 * rC11 = rA10 * rB1
 */
   #if defined(BETA0) || defined(BETAN1)
      movddup 64(pB), rC10
      movapd rC10, rC00
      mulpd rA00, rC00
      movddup 96(pB), rC11
      mulpd rA10, rC10
      movapd rC11, rC01
      mulpd rA00, rC01
      mulpd rA10, rC11
   #else
      movddup 64(pB), rB0
      movddup 96(pB), rB1
      movapd rB0, m0
      mulpd rA00, m0
      movapd 64(pC), rC00
      addpd m0, rC00
      mulpd rA10, rB0
      movapd 80(pC), rC10
      addpd rB0, rC10
      movapd rB1, m0
      mulpd rA00, m0
      movapd 96(pC), rC01
      addpd m0, rC01
      mulpd rA10, rB1
      movapd 112(pC), rC11
      addpd rB1, rC11
   #endif
/*
 * rC00 = rA01*rB10 
 * rC10 = rA11*rB10 
 * rC01 = rA01*rB11 
 * rC11 = rA11*rB11 
 */
   movddup 72(pB), rB0
   movapd rB0, m0
   mulpd rA01, m0
   addpd m0, rC00
   mulpd rA11, rB0
   addpd rB0, rC10
   movddup 104(pB), rB1
   movapd rB1, m0
   mulpd rB1, rA01
   addpd rA01, rC01
   mulpd rA11, rB1
   addpd rB1, rC11
/*
 * rC00 = rA02*rB20
 * rC10 = rA12*rB20
 * rC01 = rA02*rB21
 * rC11 = rA12*rB21
 */
   movddup 80(pB), rB0
   movapd rA02, m0
   mulpd rB0, m0
   addpd m0, rC00
   mulpd rA12, rB0
   addpd rB0, rC10
   movddup 112(pB), rB1
   mulpd rB1, rA02
   addpd rA02, rC01
   mulpd rA12, rB1
   addpd rB1, rC11
/*
 * rC00 = rA03*rB30
 * rC10 = rA13*rB30
 * rC01 = rA03*rB31
 * rC11 = rA13*rB31
 */
   movddup 88(pB), rB0
   movapd rA03, m0
   mulpd rB0, m0
   addpd m0, rC00
   #ifdef BETAN1
      subpd 64(pC), rC00
   #endif
   movapd rC00, 64(pC)
   mulpd rA13, rB0
   addpd rB0, rC10
   #ifdef BETAN1
      subpd 80(pC), rC10
   #endif
   movapd rC10, 80(pC)
   movddup 120(pB), rB1
   mulpd rB1, rA03
   addpd rA03, rC01
   #ifdef BETAN1
      subpd 96(pC), rC01
   #endif
   movapd rC01, 96(pC)
   mulpd rA13, rB1
   addpd rB1, rC11
   #ifdef BETAN1
      subpd 112(pC), rC11
   #endif
   movapd rC11, 112(pC)
   ret
@ROUT ATL_damm2x12x2_sse2.S ATL_damm2x12x256_sse2.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
#ifndef KB
   #define KB 0
#endif
@ROUT ATL_damm2x12x2_sse2.S
/*
 * innermost (K-) loop items get priority on 1st 7 regs
 */
#define pA      %rcx
#define pB      %rdi
#define incB    %rax
#define KK      %rdx
/*
 * Second (N-) loop items get next level of priority on good regs
 */
#define pC      %rbp
#define pfA     %rsi
#define pfB     %rbx
#define K1      %r8
#define incPF   %r9
#define nnu     %r10
#define incAm   %r11
#define KK0     %r15
/*
 * Outer- (M-) loop variables assigned to any regs
 */
#define nmu     %r12
#define pB0     %r13
#define nnu0    %r14
@ROUT ATL_damm2x12x256_sse2.S
/*
 * innermost (K-) loop items get priority on 1st 7 regs
 */
#define pA      %rcx
#define pB      %rdi
#define i256    %rax
#define i768    %rdx   /* 3 * 256 */
#define i1280   %rsi   /* 5 * 256 */
#define i1792   %rbx   /* 7 * 256 */
/*
 * Second (N-) loop items get next level of priority on good regs
 */
#define pC      %rbp
#define pfA     %r12
#define pfB     %r8
#define incPF   %r9
#define nnu     %r10
@skip #define incAm   %r11
/*
 * Outer- (M-) loop variables assigned to any regs
 */
#define nmu     %r13
#define pB0     %r14
#define nnu0    %r15
@ROUT ATL_damm2x12x2_sse2.S ATL_damm2x12x256_sse2.S
/*
 * floating point registers
 */
#define a0      %xmm0
#define b0      %xmm1
#define rC0     %xmm2
#define rC1     %xmm3
#define rC2     %xmm4
#define rC3     %xmm5
#define rC4     %xmm6
#define rC5     %xmm7
#define rC6     %xmm8
#define rC7     %xmm9
#define rC8     %xmm10
#define rC9     %xmm11
#define rC10    %xmm12
#define rC11    %xmm13
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

#define FSIZE 6*8
#ifndef prefA
   #define prefA prefetcht0
#endif
#ifndef prefB
   #define prefB prefetcht0
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
   movq    %r14, 32(%rsp)
   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq FSIZE+24(%rsp), pfB
   mov $2*12*8, incPF           /* incPF = mu*nu*sizeof */
/*
 * Extend range of 1-byte offsets  by starting at -128
 */
@ROUT ATL_damm2x12x256_sse2.S `   sub $-128, pA`
   sub $-128, pB
   sub $-128, pC
   sub $-128, pfA
   sub $-128, pfB
   movq pB, pB0
@ROUT ATL_damm2x12x2_sse2.S
#if KB == 0
/*
 * Make KK even, and store if we have an extra iteration in K1
 */
   mov $2, KK0  /* KK0 sets 2nd bit, used only when K==3 */
   mov $1, K1
   and KK, K1   /* K1 1 for odd K, else 0 */
   add K1, KK0  /* KK0 has both bits set appropriately for K==3 case */
   cmp $3, KK   /* is K == 3? */
   CMOVE KK0, K1/* bit 0: even/odd, bit1: set only if K == 3 */
   sub K1, KK   /* KK now guaranteed even */
#elif KB%2
      sub $1, KK  /* make loop variable even */
#endif
/*
 * incAm = KB*MU*sizeof = K*2*8 = K*16
 */
   shl $4, KK                   /* KK = K*MU*sizeof = K*2*8 = K*16 */
   mov KK, incAm                /* incAm = K*16 */
   mov $192, incB
@ROUT ATL_damm2x12x256_sse2.S
@beginskip
@BEGINPROC decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
@ENDPROC
@endskip
@BEGINPROC doref p io r
   @define io0 @@(io)@
   @mif "pB = p
      @define mv @   movddup@
   @endmif
   @mif "pB ! p
      @define mv @   movapd@
   @endmif
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 128
         @(mv) @(of)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 384
         @(mv) @(of)(@(p),i256), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 640
         @(mv) @(of)(@(p),i256,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 896
         @(mv) @(of)(@(p),i768), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1152
         @(mv) @(of)(@(p),i256,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1408
         @(mv) @(of)(@(p),i1280), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1664
         @(mv) @(of)(@(p),i768,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1920
         @(mv) @(of)(@(p),i1792), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2176
         @(mv) @(of)(@(p),i256,8), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2432
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2688
         @(mv) @(of)(@(p),i1280,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2944
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3200
         @(mv) @(of)(@(p),i768,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3456
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3712
         @(mv) @(of)(@(p),i1792,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 4992
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 5248
         @(mv) @(of)(@(p),i1280,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 6016
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 6272
         @(mv) @(of)(@(p),i768,8), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 7040
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 7296
         @(mv) @(of)(@(p),i1792,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 10112
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 7296
         @(mv) @(of)(@(p),i1280,8), @(r)
      @endextract
   @endiif
@SKIP default case just indexes ptr
         @(mv) @(io0)(@(p)), @(r)
   @undef io0
   @undef of
   @undef mv
@ENDPROC
   mov $256, i256
   lea (i256, i256,2), i768
   lea (i256, i256,4), i1280
   lea (i256, i768,2), i1792
   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
      .local NLOOP
      NLOOP:
*/
/*
 *       Peel first iteration of K loop to initialize rCx
 */
         movapd -128(pA), a0
         movddup -128(pB), rC0
         mulpd a0, rC0
            prefC -128(pC)
         movddup -120(pB), rC1
         mulpd a0, rC1
         movddup -112(pB), rC2
         mulpd a0, rC2
         movddup -104(pB), rC3
         mulpd a0, rC3
         movddup -96(pB), rC4
         mulpd a0, rC4
         movddup -88(pB), rC5
         mulpd a0, rC5
         movddup -80(pB), rC6
         mulpd a0, rC6
         movddup -72(pB), rC7
         mulpd a0, rC7
         movddup -64(pB), rC8
         mulpd a0, rC8
         movddup -56(pB), rC9
         mulpd a0, rC9
         movddup -48(pB), rC10
         mulpd a0, rC10
         movddup -40(pB), rC11
         mulpd a0, rC11
/*
 *       Fully unrolled K-loop
 */
@iexp ao -112 0 +
@iexp bo -32 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            @callproc doref pA @(ao) a0
            @iexp ao @(ao) 16 +
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC0
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            @iif @(k) = 1
               prefC -64(pC)
            @endiif
            @iif @(k) = 2
               prefC (pC)
            @endiif
            @iif @(k) = 3
            prefA -128(pfA)
            @endiif
            @iif @(k) = 4
            prefA -64(pfA)
            @endiif
            @iif @(k) = 5
            prefA (pfA)
            @endiif
            @iif @(k) = 6
            prefB -128(pfB)
            @endiif
            @iif @(k) = 7
            prefB -64(pfB)
            @endiif
            @iif @(k) = 8
            prefB (pfB)
            @endiif
            mulpd a0, b0
            addpd b0, rC1
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC2
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC3
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC4
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC5
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC6
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC7
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC8
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC9
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC10
            @callproc doref pB @(bo) b0
            @iexp bo @(bo) 8 +
            mulpd a0, b0
            addpd b0, rC11
         #endif
@endiwhile
         add incPF, pfA
         add incPF, pfB
/*
 *       Write answer back out to C
 */
         #ifdef BETA0
            movapd rC0, -128(pC)
            movapd rC1, -112(pC)
            movapd rC2, -96(pC)
            movapd rC3, -80(pC)
            movapd rC4, -64(pC)
            movapd rC5, -48(pC)
            movapd rC6, -32(pC)
            movapd rC7, -16(pC)
            movapd rC8, (pC)
            movapd rC9, 16(pC)
            movapd rC10, 32(pC)
            movapd rC11, 48(pC)
/*
 *          Add running sum in rCx with original C, then store back out
 */
         #else
            BETCOP -128(pC), rC0
            movapd rC0, -128(pC)
            BETCOP -112(pC), rC1
            movapd rC1, -112(pC)
            BETCOP -96(pC), rC2
            movapd rC2, -96(pC)
            BETCOP -80(pC), rC3
            movapd rC3, -80(pC)
            BETCOP -64(pC), rC4
            movapd rC4, -64(pC)
            BETCOP -48(pC), rC5
            movapd rC5, -48(pC)
            BETCOP -32(pC), rC6
            movapd rC6, -32(pC)
            BETCOP -16(pC), rC7
            movapd rC7, -16(pC)
            BETCOP (pC), rC8
            movapd rC8, (pC)
            BETCOP 16(pC), rC9
            movapd rC9, 16(pC)
            BETCOP 32(pC), rC10
            movapd rC10, 32(pC)
            BETCOP 48(pC), rC11
            movapd rC11, 48(pC)
         #endif
         add $12*2*8, pC   /* pC += NU*VECLEN*sizeof */
         add $KB*12*8, pB     /* pB += K*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP

      mov nnu0, nnu
      mov pB0, pB
      add $KB*2*8, pA    /* pA += KB*MU*size */
   sub $1, nmu
   jnz MNLOOP
@ROUT ATL_damm2x12x2_sse2.S
   add KK, pA                   /* pA += K*MU (ptr starts 1 past last loc */
   neg KK
   mov KK, KK0

   nop ; nop ; nop ; nop ; nop
   .local MNLOOP
   MNLOOP:
/*
      .local NLOOP
      NLOOP:
*/
         #if KB == 0
            bt $1, K1   /* If K==3, use special-case code that avoids loop */
            jc KISTHREE
         #endif
/*
 *       Peel first iteration of K loop to initialize rCx
 */
         movapd (pA,KK), a0
         movddup -128(pB), rC0
         mulpd a0, rC0
            prefC -128(pC)
         movddup -120(pB), rC1
         mulpd a0, rC1
            prefC -64(pC)
         movddup -112(pB), rC2
         mulpd a0, rC2
            prefC (pC)
         movddup -104(pB), rC3
         mulpd a0, rC3
            prefA -128(pfA)
         movddup -96(pB), rC4
         mulpd a0, rC4
            prefA -64(pfA)
         movddup -88(pB), rC5
         mulpd a0, rC5
            prefA (pfA)
         movddup -80(pB), rC6
         mulpd a0, rC6
            prefB -128(pfB)
         movddup -72(pB), rC7
         mulpd a0, rC7
            prefB -64(pfB)
         movddup -64(pB), rC8
         mulpd a0, rC8
            prefB (pfB)
         movddup -56(pB), rC9
         mulpd a0, rC9
@skip            prefC -128(pfC)
         movddup -48(pB), rC10
         mulpd a0, rC10
         movddup -40(pB), rC11
         mulpd a0, rC11
      #if KB == 0
         cmp $0, KK
         jz KLOOPDONE
      #endif

         #if KB > 1 || KB == 0
            movapd 16(pA,KK), a0
            movddup -32(pB), b0
            mulpd a0, b0
            addpd b0, rC0
            movddup -24(pB), b0
            mulpd a0, b0
            addpd b0, rC1
               #if KB > 2
                  add $32, KK
               #endif
            movddup -16(pB), b0
            mulpd a0, b0
            addpd b0, rC2
               add incPF, pfA
            movddup -8(pB), b0
            mulpd a0, b0
            addpd b0, rC3
               add incPF, pfB
            movddup (pB), b0
            mulpd a0, b0
            addpd b0, rC4
            movddup 8(pB), b0
            mulpd a0, b0
            addpd b0, rC5
            movddup 16(pB), b0
            mulpd a0, b0
            addpd b0, rC6
            movddup 24(pB), b0
            mulpd a0, b0
            addpd b0, rC7
            movddup 32(pB), b0
            mulpd a0, b0
            addpd b0, rC8
            movddup 40(pB), b0
            mulpd a0, b0
            addpd b0, rC9
            movddup 48(pB), b0
            mulpd a0, b0
            addpd b0, rC10
            movddup 56(pB), b0
            add incB, pB
            mulpd a0, b0
            addpd b0, rC11
            #if KB == 0
               add $32, KK
               jz KLOOPDONE
            #endif
         #endif

         #if KB >= 4 || KB == 0
         .local KLOOP
         KLOOP:
            movapd (pA,KK), a0
            movddup -128(pB), b0
            mulpd a0, b0
            addpd b0, rC0
            movddup -120(pB), b0
            mulpd a0, b0
            addpd b0, rC1
            movddup -112(pB), b0
            mulpd a0, b0
            addpd b0, rC2
            movddup -104(pB), b0
            mulpd a0, b0
            addpd b0, rC3
            movddup -96(pB), b0
            mulpd a0, b0
            addpd b0, rC4
            movddup -88(pB), b0
            mulpd a0, b0
            addpd b0, rC5
            movddup -80(pB), b0
            mulpd a0, b0
            addpd b0, rC6
            movddup -72(pB), b0
            mulpd a0, b0
            addpd b0, rC7
            movddup -64(pB), b0
            mulpd a0, b0
            addpd b0, rC8
            movddup -56(pB), b0
            mulpd a0, b0
            addpd b0, rC9
            movddup -48(pB), b0
            mulpd a0, b0
            addpd b0, rC10
            movddup -40(pB), b0
            mulpd a0, b0
            addpd b0, rC11

            movapd 16(pA,KK), a0
            movddup -32(pB), b0
            mulpd a0, b0
            addpd b0, rC0
            movddup -24(pB), b0
            mulpd a0, b0
            addpd b0, rC1
            movddup -16(pB), b0
            mulpd a0, b0
            addpd b0, rC2
            movddup -8(pB), b0
            mulpd a0, b0
            addpd b0, rC3
            movddup (pB), b0
            mulpd a0, b0
            addpd b0, rC4
            movddup 8(pB), b0
            mulpd a0, b0
            addpd b0, rC5
            movddup 16(pB), b0
            mulpd a0, b0
            addpd b0, rC6
            movddup 24(pB), b0
            mulpd a0, b0
            addpd b0, rC7
            movddup 32(pB), b0
            mulpd a0, b0
            addpd b0, rC8
            movddup 40(pB), b0
            mulpd a0, b0
            addpd b0, rC9
            movddup 48(pB), b0
            mulpd a0, b0
            addpd b0, rC10
            movddup 56(pB), b0
            mulpd a0, b0
            addpd b0, rC11
            add incB, pB
         add $32, KK
         jnz KLOOP
         #endif
         #if KB == 0
            bt $0, K1    /* set CF if odd # of K */
            jc ONEEXTRAKIT
            .local KLOOPDONE
            KLOOPDONE:
         #elif KB%2 && KB != 1
            movapd (pA,KK), a0
            movddup -128(pB), b0
            mulpd a0, b0
            addpd b0, rC0
            movddup -120(pB), b0
            mulpd a0, b0
            addpd b0, rC1
            movddup -112(pB), b0
            mulpd a0, b0
            addpd b0, rC2
            movddup -104(pB), b0
            mulpd a0, b0
            addpd b0, rC3
            movddup -96(pB), b0
            mulpd a0, b0
            addpd b0, rC4
            movddup -88(pB), b0
            mulpd a0, b0
            addpd b0, rC5
            movddup -80(pB), b0
            mulpd a0, b0
            addpd b0, rC6
            movddup -72(pB), b0
            mulpd a0, b0
            addpd b0, rC7
            movddup -64(pB), b0
            mulpd a0, b0
            addpd b0, rC8
            movddup -56(pB), b0
            mulpd a0, b0
            addpd b0, rC9
            movddup -48(pB), b0
            mulpd a0, b0
            addpd b0, rC10
            movddup -40(pB), b0
            mulpd a0, b0
            addpd b0, rC11
         #endif
/*
 *       Write answer back out to C
 */
         #ifdef BETA0
            movapd rC0, -128(pC)
            movapd rC1, -112(pC)
            movapd rC2, -96(pC)
            movapd rC3, -80(pC)
            movapd rC4, -64(pC)
            movapd rC5, -48(pC)
            movapd rC6, -32(pC)
            movapd rC7, -16(pC)
            movapd rC8, (pC)
            movapd rC9, 16(pC)
            movapd rC10, 32(pC)
            movapd rC11, 48(pC)
/*
 *          Add running sum in rCx with original C, then store back out
 */
         #else
            BETCOP -128(pC), rC0
            movapd rC0, -128(pC)
            BETCOP -112(pC), rC1
            movapd rC1, -112(pC)
            BETCOP -96(pC), rC2
            movapd rC2, -96(pC)
            BETCOP -80(pC), rC3
            movapd rC3, -80(pC)
            BETCOP -64(pC), rC4
            movapd rC4, -64(pC)
            BETCOP -48(pC), rC5
            movapd rC5, -48(pC)
            BETCOP -32(pC), rC6
            movapd rC6, -32(pC)
            BETCOP -16(pC), rC7
            movapd rC7, -16(pC)
            BETCOP (pC), rC8
            movapd rC8, (pC)
            BETCOP 16(pC), rC9
            movapd rC9, 16(pC)
            BETCOP 32(pC), rC10
            movapd rC10, 32(pC)
            BETCOP 48(pC), rC11
            movapd rC11, 48(pC)
         #endif
         mov KK0, KK
         add incB, pC   /* pC += 12*8*2 = 192 */
      sub $1, nnu
      jnz MNLOOP

      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA    /* pA += KB*KU*MU*size */
   sub $1, nmu
   jnz MNLOOP
@ROUT ATL_damm2x12x2_sse2.S ATL_damm2x12x256_sse2.S
/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
   movq    32(%rsp), %r14
   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_damm2x12x2_sse2.S
/*
 * This code executed only for odd K
 */
#if KB == 0
KISTHREE:
/*
 *       Peel first 2 iteration of K to initialize rCx
 */
         movapd (pA,KK), a0
         movddup -128(pB), rC0
         mulpd a0, rC0
            prefC -128(pC)
         movddup -120(pB), rC1
         mulpd a0, rC1
            prefC -64(pC)
         movddup -112(pB), rC2
         mulpd a0, rC2
            prefC (pC)
         movddup -104(pB), rC3
         mulpd a0, rC3
            prefA -128(pfA)
         movddup -96(pB), rC4
         mulpd a0, rC4
            prefA -64(pfA)
         movddup -88(pB), rC5
         mulpd a0, rC5
            prefA (pfA)
         movddup -80(pB), rC6
         mulpd a0, rC6
            prefB -128(pfB)
         movddup -72(pB), rC7
         mulpd a0, rC7
            prefB -64(pfB)
         movddup -64(pB), rC8
         mulpd a0, rC8
            prefB (pfB)
         movddup -56(pB), rC9
         mulpd a0, rC9
         movddup -48(pB), rC10
         mulpd a0, rC10
         movddup -40(pB), rC11
         mulpd a0, rC11

         movapd 16(pA,KK), a0
         movddup -32(pB), b0
         mulpd a0, b0
         addpd b0, rC0
         movddup -24(pB), b0
         mulpd a0, b0
         addpd b0, rC1
         movddup -16(pB), b0
         mulpd a0, b0
         addpd b0, rC2
            add incPF, pfA
         movddup -8(pB), b0
         mulpd a0, b0
         addpd b0, rC3
            add incPF, pfB
         movddup (pB), b0
         mulpd a0, b0
         addpd b0, rC4
         movddup 8(pB), b0
         mulpd a0, b0
         addpd b0, rC5
         movddup 16(pB), b0
         mulpd a0, b0
         addpd b0, rC6
         movddup 24(pB), b0
         mulpd a0, b0
         addpd b0, rC7
         movddup 32(pB), b0
         mulpd a0, b0
         addpd b0, rC8
         movddup 40(pB), b0
         mulpd a0, b0
         addpd b0, rC9
         movddup 48(pB), b0
         add $32, KK
         mulpd a0, b0
         addpd b0, rC10
         movddup 56(pB), b0
         add incB, pB
         mulpd a0, b0
         addpd b0, rC11
/*
 *       Then fall thru into last K iteration to finish K==3
 */
.local ONEEXTRAKIT
ONEEXTRAKIT:
            movapd (pA,KK), a0
            movddup -128(pB), b0
            mulpd a0, b0
            addpd b0, rC0
            movddup -120(pB), b0
            mulpd a0, b0
            addpd b0, rC1
            movddup -112(pB), b0
            mulpd a0, b0
            addpd b0, rC2
            movddup -104(pB), b0
            mulpd a0, b0
            addpd b0, rC3
            movddup -96(pB), b0
            mulpd a0, b0
            addpd b0, rC4
            movddup -88(pB), b0
            mulpd a0, b0
            addpd b0, rC5
            movddup -80(pB), b0
            mulpd a0, b0
            addpd b0, rC6
            movddup -72(pB), b0
            mulpd a0, b0
            addpd b0, rC7
            movddup -64(pB), b0
            mulpd a0, b0
            addpd b0, rC8
            movddup -56(pB), b0
            mulpd a0, b0
            addpd b0, rC9
            movddup -48(pB), b0
            mulpd a0, b0
            addpd b0, rC10
            movddup -40(pB), b0
            mulpd a0, b0
            addpd b0, rC11
            jmp KLOOPDONE
#endif
@ROUT ATL_damm12x3x256_avx.S ATL_samm24x3x256_avx.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
#ifndef KB
   #define KB 0
#endif
/*
 * innermost (K-) loop items get priority on 1st 7 regs
 */
#define pA      %rcx
#define pB      %rdi
#define i256    %rax
#define i768    %rdx   /* 3 * 256 */
#define i1280   %rsi   /* 5 * 256 */
#define i1792   %rbx   /* 7 * 256 */
#define i2304   %r11
/*
 * Second (N-) loop items get next level of priority on good regs
 */
#define pC      %rbp
#define pfA     %r8
#define pfB     %r9
#define incPF   %r10
#define nnu     %r12
/*
 * Outer- (M-) loop variables assigned to any regs
 */
#define nmu     %r13
#define pB0     %r14
#define nnu0    %r15
/*
 * floating point registers
 */
#define m0   %ymm0
#define rA0  %ymm1
#define rA1  %ymm2
#define rA2  %ymm3
#define rB0  %ymm4
#define rB1  %ymm5
#define rB2  %ymm6
#define rC00 %ymm7
#define rC10 %ymm8
#define rC20 %ymm9
#define rC01 %ymm10
#define rC11 %ymm11
#define rC21 %ymm12
#define rC02 %ymm13
#define rC12 %ymm14
#define rC22 %ymm15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

#define FSIZE 6*8
#ifndef prefA
   #define prefA prefetcht0
#endif
#ifndef prefB
   #define prefB prefetcht0
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
@ROUT ATL_samm24x3x256_avx.S
#ifdef BETAN1
   #define BETCOP vsubps
#else
   #define BETCOP vaddps
#endif
#define vmovapd vmovaps
#define vmulpd vmulps
#define vaddpd vaddps
#define vbroadcastsd vbroadcastss
@define sz @4@
@ROUT ATL_damm12x3x256_avx.S
#ifdef BETAN1
   #define BETCOP vsubpd
#else
   #define BETCOP vaddpd
#endif
#define vmovapd vmovaps
@define sz @8@
@ROUT ATL_damm12x3x256_avx.S ATL_samm24x3x256_avx.S
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
   movq    %r14, 32(%rsp)
   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq FSIZE+24(%rsp), pfB
   mov $2*12*@(sz), incPF           /* incPF = mu*nu*sizeof */
/*
 * Extend range of 1-byte offsets  by starting at -128
 */
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   sub $-128, pfA
   sub $-128, pfB
   movq pB, pB0
@BEGINPROC doref p io r
   @define io0 @@(io)@
   @mif "pB = p
      @ROUT ATL_damm12x3x256_avx.S `@define mv @   vbroadcastsd@`
      @ROUT ATL_samm24x3x256_avx.S `@define mv @   vbroadcastss@`
   @endmif
   @mif "pB ! p
      @ROUT ATL_damm12x3x256_avx.S `@define mv @   vmovapd@`
      @ROUT ATL_samm24x3x256_avx.S `@define mv @   vmovaps@`
   @endmif
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 128
         @(mv) @(of)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 384
         @(mv) @(of)(@(p),i256), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 640
         @(mv) @(of)(@(p),i256,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 896
         @(mv) @(of)(@(p),i768), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1152
         @(mv) @(of)(@(p),i256,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1408
         @(mv) @(of)(@(p),i1280), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1664
         @(mv) @(of)(@(p),i768,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1920
         @(mv) @(of)(@(p),i1792), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2176
         @(mv) @(of)(@(p),i256,8), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2432
         @(mv) @(of)(@(p),i2304), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2688
         @(mv) @(of)(@(p),i1280,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2944
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3200
         @(mv) @(of)(@(p),i768,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3456
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3712
         @(mv) @(of)(@(p),i1792,2), @(r)
      @endextract
   @endiif
   @iif io0 < 4736
      @iif io0 > 4479
         @iexp io @(io0) -4480 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i2304,2), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i2304,2), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 5248
      @iif io0 > 4991
         @iexp io @(io0) -4992 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1280,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1280,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 6272
      @iif io0 > 6015
         @iexp io @(io0) -6016 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i768,8), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i768,8), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 7296
      @iif io0 > 7039
         @iexp io @(io0) -7040 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1792,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1792,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 7296
      @iif io0 > 7039
         @iexp io @(io0) -7040 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1792,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1792,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 9344
      @iif io0 > 9087
         @iexp io @(io0) -9088 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i2304,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i2304,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 10368
      @iif io0 > 10111
         @iexp io @(io0) -10112 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1280,8), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1280,8), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
@SKIP default case just indexes ptr
         @(mv) @(io0)(@(p)), @(r)
   @undef io0
   @undef of
   @undef mv
@ENDPROC
   mov $256, i256
   lea (i256, i256,2), i768
   lea (i256, i256,4), i1280
   lea (i256, i768,2), i1792
   lea (i256, i256,8), i2304
   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
      .local NLOOP
      NLOOP:
*/
/*
 *       Peel first iteration of K loop to initialize rCx
 */
@iexp bo -128 0 +
         vmovapd -128(pA), rA0
         @callproc doref pB @(bo) rB0
         @iexp bo @(bo) @(sz) +
         vmulpd rA0, rB0, rC00
         vmovapd -96(pA), rA1
         vmulpd rA1, rB0, rC10
         vmovapd -64(pA), rA2
         vmulpd rA2, rB0, rC20
         @callproc doref pB @(bo) rB1
         @iexp bo @(bo) @(sz) +
         vmulpd rA0, rB1, rC01
         @callproc doref pB @(bo) rB2
         @iexp bo @(bo) @(sz) +
         vmulpd rA1, rB1, rC11
         #if KB > 1
         @callproc doref pB @(bo) rB0
         @iexp bo @(bo) @(sz) +
         #endif
         vmulpd rA2, rB1, rC21
         #if KB > 1
         @callproc doref pB @(bo) rB1
         @iexp bo @(bo) @(sz) +
         #endif
         vmulpd rA0, rB2, rC02
         #if KB > 1
            vmovapd -32(pA), rA0
         #endif
         vmulpd rA1, rB2, rC12
         #if KB > 1
            vmovapd (pA), rA1
         #endif
         vmulpd rA2, rB2, rC22
         #if KB > 1
            vmovapd 32(pA), rA2
         #endif
/*
 *       Fully unrolled K-loop
 */
@iexp ao 64 0 +
@skip @iexp bo -88 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            @callproc doref pB @(bo) rB2
            @iexp bo @(bo) @(sz) +
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
            @iif @(k) = 1
               prefC -128(pC)
            @endiif
            @iif @(k) = 2
               prefC (pC)
            @endiif
            @iif @(k) = 3
               prefC 128(pC)
            @endiif
            @iif @(k) = 4
            prefA -128(pfA)
            @endiif
            @iif @(k) = 5
            prefA -64(pfA)
            @endiif
            @iif @(k) = 6
            prefA (pfA)
            @endiif
            @iif @(k) = 7
            prefB -128(pfB)
            @endiif
            @iif @(k) = 8
            prefB -64(pfB)
            @endiif
            @iif @(k) = 9
            prefB (pfB)
            @endiif
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
            #if KB > @(k)
               @callproc doref pB @(bo) rB0
               @iexp bo @(bo) @(sz) +
            #endif

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
            @iif @(k) = 1
               prefC -64(pC)
            @endiif
            @iif @(k) = 2
               prefC 64(pC)
            @endiif
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
            #if KB > @(k)
               @callproc doref pB @(bo) rB1
               @iexp bo @(bo) @(sz) +
            #endif
      
            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
            #if KB > @(k)
               @callproc doref pA @(ao) rA0
               @iexp ao @(ao) 32 +
            #endif
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
            #if KB > @(k)
               @callproc doref pA @(ao) rA1
               @iexp ao @(ao) 32 +
            #endif
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
            #if KB > @(k)
               @callproc doref pA @(ao) rA2
               @iexp ao @(ao) 32 +
            #endif

         #endif
@endiwhile
         add incPF, pfA
         add incPF, pfB
/*
 *       Write answer back out to C
 */
         #ifdef BETA0
            vmovapd rC00, -128(pC)
            vmovapd rC10, -96(pC)
            vmovapd rC20, -64(pC)
            vmovapd rC01, -32(pC)
            vmovapd rC11, (pC)
            vmovapd rC21, 32(pC)
            vmovapd rC02, 64(pC)
            vmovapd rC12, 96(pC)
            vmovapd rC22, 128(pC)
/*
 *          Add running sum in rCx with original C, then store back out
 */
         #else
            BETCOP -128(pC), rC00, rC00
            vmovapd rC00, -128(pC)
            BETCOP -96(pC), rC10, rC10
            vmovapd rC10, -96(pC)
            BETCOP -64(pC), rC20, rC20
            vmovapd rC20, -64(pC)
            BETCOP -32(pC), rC01, rC01
            vmovapd rC01, -32(pC)
            BETCOP (pC), rC11, rC11
            vmovapd rC11, (pC)
            BETCOP 32(pC), rC21, rC21
            vmovapd rC21, 32(pC)
            BETCOP 64(pC), rC02, rC02
            vmovapd rC02, 64(pC)
            BETCOP 96(pC), rC12, rC12
            vmovapd rC12, 96(pC)
            BETCOP 128(pC), rC22, rC22
            vmovapd rC22, 128(pC)
         #endif
         add $12*3*8, pC        /* pC += MU*NU*sizeof */
         add $KB*3*@(sz), pB        /* pB += K*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP

      mov nnu0, nnu
      mov pB0, pB
      add $KB*12*8, pA          /* pA += KB*MU*size */
   sub $1, nmu
   jnz MNLOOP
/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
   movq    32(%rsp), %r14
   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
#ifndef KB
   #define KB 0
#endif
/*
 * innermost (K-) loop items get priority on 1st 7 regs
 */
#define pA      %rcx
#define pB      %rdi
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
#define KK0     %rax
#define KK      %rdx
#define incA    %rsi
#define incB    %rbx
#define pA0     %r11
@ROUT ATL_damm6x3x256_sse3.S
#define i256    %rax
#define i768    %rdx   /* 3 * 256 */
#define i1280   %rsi   /* 5 * 256 */
#define i1792   %rbx   /* 7 * 256 */
#define i2304   %r11
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
/*
 * Second (N-) loop items get next level of priority on good regs
 */
#define pC      %rbp
#define pfA     %r8
#define pfB     %r9
#define incPF   %r10
#define nnu     %r12
/*
 * Outer- (M-) loop variables assigned to any regs
 */
#define nmu     %r13
#define pB0     %r14
#define nnu0    %r15
/*
 * floating point registers
 */
#define m0   %xmm0
#define rA0  %xmm1
#define rA1  %xmm2
#define rA2  %xmm3
#define rB0  %xmm4
#define rB1  %xmm5
#define rB2  %xmm6
#define rC00 %xmm7
#define rC10 %xmm8
#define rC20 %xmm9
#define rC01 %xmm10
#define rC11 %xmm11
#define rC21 %xmm12
#define rC02 %xmm13
#define rC12 %xmm14
#define rC22 %xmm15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

#define FSIZE 6*8
#ifndef prefA
   #define prefA prefetcht0
#endif
#ifndef prefB
   #define prefB prefetcht2
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
#define movapd movaps
@define sz @8@
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
   movq    %r14, 32(%rsp)
   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq FSIZE+24(%rsp), pfB
   mov $6*3*@(sz), incPF           /* incPF = mu*nu*sizeof */
/*
 * Extend range of 1-byte offsets  by starting at -128
 */
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   sub $-128, pfA
   sub $-128, pfB
   movq pB, pB0
@BEGINPROC doref p io r
   @define io0 @@(io)@
   @mif "pB = p
      @define mv @   movddup@
   @endmif
   @mif "pB ! p
      @define mv @   movapd@
   @endmif
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io < 128
         @(mv) @(of)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 384
         @(mv) @(of)(@(p),i256), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 640
         @(mv) @(of)(@(p),i256,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 896
         @(mv) @(of)(@(p),i768), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1152
         @(mv) @(of)(@(p),i256,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1408
         @(mv) @(of)(@(p),i1280), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1664
         @(mv) @(of)(@(p),i768,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 1920
         @(mv) @(of)(@(p),i1792), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2176
         @(mv) @(of)(@(p),i256,8), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2432
         @(mv) @(of)(@(p),i2304), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2688
         @(mv) @(of)(@(p),i1280,2), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 2944
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3200
         @(mv) @(of)(@(p),i768,4), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3456
         @(mv) @(io0)(@(p)), @(r)
      @endextract
   @endiif
@skip   @callproc decio
   @iexp io @(io) -256 +
   @undef of
   @iif io = 0
      @define of @@
   @endiif
   @iif io ! 0
      @define of @@(io)@
   @endiif
   @iif io0 < 3712
         @(mv) @(of)(@(p),i1792,2), @(r)
      @endextract
   @endiif
   @iif io0 < 4736
      @iif io0 > 4479
         @iexp io @(io0) -4480 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i2304,2), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i2304,2), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 5248
      @iif io0 > 4991
         @iexp io @(io0) -4992 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1280,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1280,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 6272
      @iif io0 > 6015
         @iexp io @(io0) -6016 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i768,8), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i768,8), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 7296
      @iif io0 > 7039
         @iexp io @(io0) -7040 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1792,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1792,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 7296
      @iif io0 > 7039
         @iexp io @(io0) -7040 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1792,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1792,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 9344
      @iif io0 > 9087
         @iexp io @(io0) -9088 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i2304,4), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i2304,4), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
   @iif io0 < 10368
      @iif io0 > 10111
         @iexp io @(io0) -10112 +
         @iexp io @(io) -128 +
         @iif io = 0
         @(mv) (@(p),i1280,8), @(r)
         @endiif
         @iif io ! 0
         @(mv) @(io)(@(p),i1280,8), @(r)
         @endiif
         @endextract
      @endiif
   @endiif
@SKIP default case just indexes ptr
         @(mv) @(io0)(@(p)), @(r)
   @undef io0
   @undef of
   @undef mv
@ENDPROC
@ROUT ATL_damm6x3x4_sse3.S
   mov pA, pA0
   mov KK, KK0
   mov $192, incA
   mov $96, incB
@ROUT ATL_damm6x3x256_sse3.S
   mov $256, i256
   lea (i256, i256,2), i768
   lea (i256, i256,4), i1280
   lea (i256, i768,2), i1792
   lea (i256, i256,8), i2304
@iexp bo -128 0 +
   @callproc doref pB @(bo) rB0
   @iexp bo @(bo) @(sz) +
   @callproc doref pB @(bo) rB1
   @iexp bo @(bo) @(sz) +
   ALIGN8
@ROUT ATL_damm6x3x4_sse3.S
@iexp bo -128 0 +
   @callproc doref pB @(bo) rB0
   @iexp bo @(bo) @(sz) +
   @callproc doref pB @(bo) rB1
   @iexp bo @(bo) @(sz) +
   nop ; nop ; nop ; nop ; nop
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
   .local MNLOOP
   MNLOOP:
/*
      .local NLOOP
      NLOOP:
*/
/*
 *       Peel first iteration of K loop to initialize rCx
 */
         movapd -128(pA), rC02
         movapd rC02, rC00
         mulpd rB0, rC00
         movapd -112(pA), rC12
         movapd rC12, rC10
         mulpd rB0, rC10
         movapd -96(pA), rC22
         movapd rC22, rC20
         mulpd rB0, rC20

         movapd rC02, rC01
         mulpd rB1, rC01
         @callproc doref pB @(bo) rB2
         @iexp bo @(bo) @(sz) +
         movapd rC12, rC11
         mulpd rB1, rC11
            prefC -128(pC)
         movapd rC22, rC21
         mulpd rB1, rC21

         mulpd rB2, rC02
         #if KB > 1
         @callproc doref pB @(bo) rB0
         @iexp bo @(bo) @(sz) +
         #endif
         mulpd rB2, rC12
         #if KB > 1
         @callproc doref pB @(bo) rB1
         @iexp bo @(bo) @(sz) +
         #endif
         mulpd rB2, rC22

@ROUT ATL_damm6x3x4_sse3.S
         movapd -80(pA), rA0
         movapd rA0, m0
         mulpd rB0, m0
         addpd m0, rC00
         movapd -64(pA), rA1
         movapd rA1, m0
         mulpd rB0, m0
         addpd m0, rC10
         movapd -48(pA), rA2
         mulpd rA2, rB0
         addpd rB0, rC20

            prefC 64(pC)
         movapd rA0, m0
         mulpd rB1, m0
         addpd m0, rC01
            prefA -128(pfA)
         movapd rA1, m0
         mulpd rB1, m0
         addpd m0, rC11
         mulpd rA2, rB1
         addpd rB1, rC21

         movddup -88(pB), rB2
         mulpd rB2, rA0
         addpd rA0, rC02
            movddup -80(pB), rB0
         mulpd rB2, rA1
         addpd rA1, rC12
            movddup -72(pB), rB1
         mulpd rB2, rA2
         addpd rA2, rC22

         movapd -32(pA), rA0
         movapd rA0, m0
         mulpd rB0, m0
         addpd m0, rC00
         movapd -16(pA), rA1
         movapd rA1, m0
         mulpd rB0, m0
         addpd m0, rC10
         movapd (pA), rA2
         mulpd rA2, rB0
         addpd rB0, rC20

            prefA -64(pfA)
         movapd rA0, m0
         mulpd rB1, m0
         addpd m0, rC01
            prefA (pfA)
         movapd rA1, m0
         mulpd rB1, m0
         addpd m0, rC11
         mulpd rA2, rB1
         addpd rB1, rC21

         movddup -64(pB), rB2
         mulpd rB2, rA0
         addpd rA0, rC02
            movddup -56(pB), rB0
         mulpd rB2, rA1
         addpd rA1, rC12
            movddup -48(pB), rB1
         mulpd rB2, rA2
         addpd rA2, rC22

         movapd 16(pA), rA0
         movapd rA0, m0
         mulpd rB0, m0
         addpd m0, rC00
         movapd 32(pA), rA1
         movapd rA1, m0
         mulpd rB0, m0
         addpd m0, rC10
         movapd 48(pA), rA2
         mulpd rA2, rB0
         addpd rB0, rC20

            prefB -128(pfB)
         movapd rA0, m0
         mulpd rB1, m0
         addpd m0, rC01
            prefB -64(pfB)
         movapd rA1, m0
         mulpd rB1, m0
         addpd m0, rC11
            prefB (pfB)
         mulpd rA2, rB1
         addpd rB1, rC21

         movddup -40(pB), rB2
         add incB, pB
         mulpd rB2, rA0
         addpd rA0, rC02
            movddup -128(pB), rB0
         mulpd rB2, rA1
         addpd rA1, rC12
            movddup -120(pB), rB1
         mulpd rB2, rA2
            add incA, pA
         addpd rA2, rC22
         sub $4, KK
         jz KLOOPDONE

         KLOOP:
            movapd -128(pA), rA0
            movapd rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            movapd -112(pA), rA1
            movapd rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            movapd -96(pA), rA2
            mulpd rA2, rB0
            addpd rB0, rC20

            movapd rA0, m0
            mulpd rB1, m0
            addpd m0, rC01
            movapd rA1, m0
            mulpd rB1, m0
            addpd m0, rC11
            mulpd rA2, rB1
            addpd rB1, rC21

            movddup -112(pB), rB2
            mulpd rB2, rA0
            addpd rA0, rC02
               movddup -104(pB), rB0
            mulpd rB2, rA1
            addpd rA1, rC12
               movddup -96(pB), rB1
            mulpd rB2, rA2
            addpd rA2, rC22

            movapd -80(pA), rA0
            movapd rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            movapd -64(pA), rA1
            movapd rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            movapd -48(pA), rA2
            mulpd rA2, rB0
            addpd rB0, rC20

            movapd rA0, m0
            mulpd rB1, m0
            addpd m0, rC01
            movapd rA1, m0
            mulpd rB1, m0
            addpd m0, rC11
            mulpd rA2, rB1
            addpd rB1, rC21

            movddup -88(pB), rB2
            mulpd rB2, rA0
            addpd rA0, rC02
               movddup -80(pB), rB0
            mulpd rB2, rA1
            addpd rA1, rC12
               movddup -72(pB), rB1
            mulpd rB2, rA2
            addpd rA2, rC22

            movapd -32(pA), rA0
            movapd rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            movapd -16(pA), rA1
            movapd rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            movapd (pA), rA2
            mulpd rA2, rB0
            addpd rB0, rC20

            movapd rA0, m0
            mulpd rB1, m0
            addpd m0, rC01
            movapd rA1, m0
            mulpd rB1, m0
            addpd m0, rC11
            mulpd rA2, rB1
            addpd rB1, rC21

            movddup -64(pB), rB2
            mulpd rB2, rA0
            addpd rA0, rC02
               movddup -56(pB), rB0
            mulpd rB2, rA1
            addpd rA1, rC12
               movddup -48(pB), rB1
            mulpd rB2, rA2
            addpd rA2, rC22

            movapd 16(pA), rA0
            movapd rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            movapd 32(pA), rA1
            movapd rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            movapd 48(pA), rA2
            mulpd rA2, rB0
            addpd rB0, rC20

            movapd rA0, m0
            mulpd rB1, m0
            addpd m0, rC01
            movapd rA1, m0
            mulpd rB1, m0
            addpd m0, rC11
               add incA, pA
            mulpd rA2, rB1
            addpd rB1, rC21

            movddup -40(pB), rB2
            mulpd rB2, rA0
            addpd rA0, rC02
               movddup -32(pB), rB0
            mulpd rB2, rA1
            addpd rA1, rC12
               movddup -24(pB), rB1
            mulpd rB2, rA2
            add incB, pB
            addpd rA2, rC22
         sub $4, KK
         jnz KLOOP
         KLOOPDONE:
@ROUT ATL_damm6x3x256_sse3.S
/*
 *       Fully unrolled K-loop
 */
@define vsz @16@
@iexp ao -80 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            @callproc doref pA @(ao) rA0
            @iexp ao @(ao) @(vsz) +
            movapd rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            @callproc doref pA @(ao) rA1
            @iexp ao @(ao) @(vsz) +
            movapd rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            @callproc doref pA @(ao) rA2
            @iexp ao @(ao) @(vsz) +
            mulpd rA2, rB0
            addpd rB0, rC20

            @iif @(k) = 1
               prefC -64(pC)
            @endiif
            @iif @(k) = 2
               prefC (pC)
            @endiif
            @iif @(k) = 3
               prefA -128(pfA)
            @endiif
            @iif @(k) = 4
               prefA -64(pfA)
            @endiif
            @iif @(k) = 5
               prefA (pfA)
            @endiif
            @iif @(k) = 6
               prefB -128(pfB)
            @endiif
            @iif @(k) = 7
               prefB -64(pfB)
            @endiif
            @iif @(k) = 8
               prefB (pfB)
            @endiif
            movapd rA0, m0
            mulpd rB1, m0
            addpd m0, rC01
            movapd rA1, m0
            mulpd rB1, m0
            addpd m0, rC11
            mulpd rA2, rB1
            addpd rB1, rC21

            @callproc doref pB @(bo) rB2
            @iexp bo @(bo) @(sz) +
            mulpd rB2, rA0
            addpd rA0, rC02
            #if KB > @(k)
            @callproc doref pB @(bo) rB0
            @iexp bo @(bo) @(sz) +
            #endif
            mulpd rB2, rA1
            addpd rA1, rC12
            #if KB > @(k)
            @callproc doref pB @(bo) rB1
            @iexp bo @(bo) @(sz) +
            #endif
            mulpd rB2, rA2
            addpd rA2, rC22
         #endif
@endiwhile
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
         add incPF, pfA
         add incPF, pfB
/*
 *       Write answer back out to C
 */
         #ifdef BETA0
            movapd rC00, -128(pC)
            movapd rC10, -112(pC)
            movapd rC20, -96(pC)
            movapd rC01, -80(pC)
            movapd rC11, -64(pC)
            movapd rC21, -48(pC)
            movapd rC02, -32(pC)
            movapd rC12, -16(pC)
            movapd rC22, (pC)
/*
 *          Add running sum in rCx with original C, then store back out
 */
         #else
            BETCOP -128(pC), rC00
            movapd rC00, -128(pC)
            BETCOP -112(pC), rC10
            movapd rC10, -112(pC)
            BETCOP -96(pC), rC20
            movapd rC20, -96(pC)
            BETCOP -80(pC), rC01
            movapd rC01, -80(pC)
            BETCOP -64(pC), rC11
            movapd rC11, -64(pC)
            BETCOP -48(pC), rC21
            movapd rC21, -48(pC)
            BETCOP -32(pC), rC02
            movapd rC02, -32(pC)
            BETCOP -16(pC), rC12
            movapd rC12, -16(pC)
            BETCOP (pC), rC22
            movapd rC22, (pC)
         #endif
@ROUT ATL_damm6x3x256_sse3.S `         add $KB*3*@(sz), pB        /* pB += K*NU*sizeof */`
         add $6*3*8, pC        /* pC += MU*NU*sizeof */
         movddup -128(pB), rB0
      sub $1, nnu
         movddup -120(pB), rB1
@ROUT ATL_damm6x3x4_sse3.S `         mov KK0, KK`
@ROUT ATL_damm6x3x4_sse3.S `         mov pA0, pA`
      jnz MNLOOP

         movddup -128(pB0), rB0
      mov nnu0, nnu
         movddup -120(pB0), rB1
      mov pB0, pB
@ROUT ATL_damm6x3x4_sse3.S
      lea (KK, KK, 4), pA      /* pA = 5*K */
      add KK, pA               /* pA = 6*K = MU*K */
      shl $3, pA               /* pA = MU*K*sizeof */
      add pA0, pA              /* pA = A + MU*K*sizeof */
      mov pA, pA0
@ROUT ATL_damm6x3x256_sse3.S
      add $KB*6*8, pA          /* pA += KB*MU*size */
@ROUT ATL_damm6x3x256_sse3.S ATL_damm6x3x4_sse3.S
   sub $1, nmu
   jnz MNLOOP
/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
   movq    32(%rsp), %r14
   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_samm16x4x1_av.c
#include "atlas_misc.h"
#define ATL_NoFakePF
#include "atlas_prefetch.h"

#ifndef ATL_CSZT
   #define ATL_CSZT const size_t
#endif
void ATL_USERMM
(
   ATL_CSZT nmus,
   ATL_CSZT nnus,
   ATL_CSZT K,
   const TYPE *pA,    /* 4*KB*nmus-length access-major array of A */
   const TYPE *pB,    /* 1*KB*nnus-length access-major array of B */
   TYPE *pC,          /* 4*1*nnus*nmus-length access-major array of C */
   const TYPE *pAn,   /* next block of A */
   const TYPE *pBn,   /* next block of B */
   const TYPE *pCn    /* next block of C */
)
/*
 * Very basic 4x4 KU=1 AltiVec kernel
 */
{
   const TYPE *pB0 = pB, *pA0 = pA;
   const TYPE *pfA, *pfB;
   size_t incPF, i, j, k;
   vector float vA0, vA1, vA2, vA3, vB0, vB1, vB2, vB3;
   vector float  vC00, vC10, vC20, vC30, vC01, vC11, vC21, vC31,
                 vC02, vC12, vC22, vC32, vC03, vC13, vC23, vC33;
   #ifndef ATL_NoIEEE /* turn on java/ieee mode */
         const vector int izero   =  VECTOR_INITI(0,0,0,0);
         vec_mtvscr(izero);
   #endif

  if (pAn != pA)
   {
      pfA = pAn;
      incPF = (nmus*4*K) / (nmus * nnus);
      pfB = (pBn != pB) ? pBn : pCn;

   }
   else if (pCn != pC)
   {
      pfA = pCn;
      incPF = (nmus*4*nnus*1) / (nmus * nnus);
      pfB = pBn;
   }
   else if (pBn != pB)
   {
      pfA = pBn;
      pfB = pBn +((nmus*4*nnus*1)>>1);
      incPF = (K*nnus*1*sizeof(TYPE)) / ((nmus * nnus)<<1);
   }
   else
   {
      pfA = pA + nmus*4*(K>>1);
      incPF = (nmus*4*K) / (nmus * nnus);
   }
   for (i=0; i < nmus; i++)
   {
      for (j=0; j < nnus; j++)
      {
/*
 *       Peel K=0 iteration to prefetch
 */
         vC33 = vec_xor(vC33, vC33);
         vB3 = vec_ld(0, pB);
         ATL_pfl1W(pC);
         ATL_pfl1W(pC+32);
         vB0 = vec_splat(vB3, 0);
         vB1 = vec_splat(vB3, 1);
         vB2 = vec_splat(vB3, 2);
         vB3 = vec_splat(vB3, 3);

         vA0 = vec_ld(0, pA);
         vA1 = vec_ld(0, pA+4);
         vA2 = vec_ld(0, pA+8);
         vA3 = vec_ld(0, pA+12);

         vC00 = vec_madd(vA0, vB0, vC33);
         ATL_pfl1R(pfA);
         vC10 = vec_madd(vA1, vB0, vC33);
         vC20 = vec_madd(vA2, vB0, vC33);
         vC30 = vec_madd(vA3, vB0, vC33);
         vC01 = vec_madd(vA0, vB1, vC33);
         vC11 = vec_madd(vA1, vB1, vC33);
         vC21 = vec_madd(vA2, vB1, vC33);
         vC31 = vec_madd(vA3, vB1, vC33);
            vB1 = vec_ld(0, pB+4);
         vC02 = vec_madd(vA0, vB2, vC33);
            vB0 = vec_splat(vB1, 0);
         vC12 = vec_madd(vA1, vB2, vC33);
         vC22 = vec_madd(vA2, vB2, vC33);
         vC32 = vec_madd(vA3, vB2, vC33);
            vB2 = vec_splat(vB1, 2);
         vC03 = vec_madd(vA0, vB3, vC33);
            vA0 = vec_ld(0, pA+16);
         vC13 = vec_madd(vA1, vB3, vC33);
            vA1 = vec_ld(0, pA+20);
         vC23 = vec_madd(vA2, vB3, vC33);
            vA2 = vec_ld(0, pA+24);
         vC33 = vec_madd(vA3, vB3, vC33);
            vB3 = vec_splat(vB1, 3);
         pA += 32;
         pB += 8;
/*
 *       Handle remaining K its with rolled loop (compiler can unroll easily)
 */
      #if KB != 0
         for (k=1; k < KB; k++)
      #else
         for (k=1; k < K; k++)
      #endif
         {
               vA3 = vec_ld(0, pA-4);
            vC00 = vec_madd(vA0, vB0, vC00);
            vC10 = vec_madd(vA1, vB0, vC10);
            vC20 = vec_madd(vA2, vB0, vC20);
            vC30 = vec_madd(vA3, vB0, vC30);
               vB1 = vec_splat(vB1, 1);
            vC01 = vec_madd(vA0, vB1, vC01);
            vC11 = vec_madd(vA1, vB1, vC11);
            vC21 = vec_madd(vA2, vB1, vC21);
            vC31 = vec_madd(vA3, vB1, vC31);
               vB1 = vec_ld(0, pB);
            vC02 = vec_madd(vA0, vB2, vC02);
               vB0 = vec_splat(vB1, 0);
            vC12 = vec_madd(vA1, vB2, vC12);
            vC22 = vec_madd(vA2, vB2, vC22);
            vC32 = vec_madd(vA3, vB2, vC32);
               vB2 = vec_splat(vB1, 2);
            vC03 = vec_madd(vA0, vB3, vC03);
            vC13 = vec_madd(vA1, vB3, vC13);
               vA0 = vec_ld(0, pA);
               vA1 = vec_ld(0, pA+4);
            vC23 = vec_madd(vA2, vB3, vC23);
            vC33 = vec_madd(vA3, vB3, vC33);
               vA2 = vec_ld(0, pA+8);
               vB3 = vec_splat(vB1, 3);
            pA += 16;
            pB += 4;
         }
         #ifdef BETA0
           vec_st(vC00, 0, pC);
           vec_st(vC10, 0, pC+4);
           vec_st(vC20, 0, pC+8);
           vec_st(vC30, 0, pC+12);
           vec_st(vC01, 0, pC+16);
           vec_st(vC11, 0, pC+20);
           vec_st(vC21, 0, pC+24);
           vec_st(vC31, 0, pC+28);
           vec_st(vC02, 0, pC+32);
           vec_st(vC12, 0, pC+36);
           vec_st(vC22, 0, pC+40);
           vec_st(vC32, 0, pC+44);
           vec_st(vC03, 0, pC+48);
           vec_st(vC13, 0, pC+52);
           vec_st(vC23, 0, pC+56);
           vec_st(vC33, 0, pC+60);
         #else
            #ifdef BETAN1
               #define VEC_ADD vec_sub
            #else
               #define VEC_ADD vec_add
            #endif
            vA0 = vec_ld(0, pC);
            vA1 = vec_ld(0, pC+4);
            vA2 = vec_ld(0, pC+8);
            vA3 = vec_ld(0, pC+12);
            vC00 = VEC_ADD(vC00, vA0);
            vC10 = VEC_ADD(vC10, vA1);
            vC20 = VEC_ADD(vC20, vA2);
            vC30 = VEC_ADD(vC30, vA3);
            vec_st(vC00, 0, pC);
            vec_st(vC10, 0, pC+4);
            vec_st(vC20, 0, pC+8);
            vec_st(vC30, 0, pC+12);
            vB0 = vec_ld(0, pC+16);
            vB1 = vec_ld(0, pC+20);
            vB2 = vec_ld(0, pC+24);
            vB3 = vec_ld(0, pC+28);
            vC01 = VEC_ADD(vC01, vB0);
            vC11 = VEC_ADD(vC11, vB1);
            vC21 = VEC_ADD(vC21, vB2);
            vC31 = VEC_ADD(vC31, vB3);
            vec_st(vC01, 0, pC+16);
            vec_st(vC11, 0, pC+20);
            vec_st(vC21, 0, pC+24);
            vec_st(vC31, 0, pC+28);
            vA0 = vec_ld(0, pC+32);
            vA1 = vec_ld(0, pC+36);
            vA2 = vec_ld(0, pC+40);
            vA3 = vec_ld(0, pC+44);
            vC02 = VEC_ADD(vC02, vA0);
            vC12 = VEC_ADD(vC12, vA1);
            vC22 = VEC_ADD(vC22, vA2);
            vC32 = VEC_ADD(vC32, vA3);
            vec_st(vC02, 0, pC+32);
            vec_st(vC12, 0, pC+36);
            vec_st(vC22, 0, pC+40);
            vec_st(vC32, 0, pC+44);
            vB0 = vec_ld(0, pC+48);
            vB1 = vec_ld(0, pC+52);
            vB2 = vec_ld(0, pC+56);
            vB3 = vec_ld(0, pC+60);
            vC03 = VEC_ADD(vC03, vB0);
            vC13 = VEC_ADD(vC13, vB1);
            vC23 = VEC_ADD(vC23, vB2);
            vC33 = VEC_ADD(vC33, vB3);
            vec_st(vC03, 0, pC+48);
            vec_st(vC13, 0, pC+52);
            vec_st(vC23, 0, pC+56);
            vec_st(vC33, 0, pC+60);
         #endif
         pA = pA0;
         pC += 16*4;  /* MU * NU */
         pB -= 4;
      }
      pA0 += 16*K;     /* MU*K */
      pA = pA0;
      pB = pB0;
   }
}
@ROUT ATL_damm12x3x1_avx.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

#define FSIZE 6*8
#define PFADIST 448
#define PFBDIST 448
#ifndef prefA
   #define prefA prefetcht0
#endif
#ifndef prefB
   #define prefB prefetcht0
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#if defined(SREAL) || defined(SCPLX)
   #define vmovapd vmovaps
   #define vsubpd vsubps
   #define vaddpd vaddps
   #define vmulpd vmulps
   #define vbroadcastsd vbroadcastss
   #define SZ 4
#else
   #define vmovapd vmovaps
   #define SZ 8
#endif
#ifdef BETAN1
   #define BETCOP vsubpd
#else
   #define BETCOP vaddpd
#endif
/*
 * floating point registers
 */
#define m0   %ymm0
#define rA0  %ymm1
#define rA1  %ymm2
#define rA2  %ymm3
#define rB0  %ymm4
#define rB1  %ymm5
#define rB2  %ymm6
#define rC00 %ymm7
#define rC10 %ymm8
#define rC20 %ymm9
#define rC01 %ymm10
#define rC11 %ymm11
#define rC21 %ymm12
#define rC02 %ymm13
#define rC12 %ymm14
#define rC22 %ymm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
@skip #define incAk   %r9   /* set after mov r9 to pC () */
#define incBk   %r8   /* set after mov r8 to pB (rax) */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
@skip #define pfB     %rbp
#define r96     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r9 /* %r14 */
#define pB0     %r14
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub  $FSIZE, %rsp
   movq %rbp, (%rsp)
   movq %rbx, 8(%rsp)
   movq %r12, 16(%rsp)
   movq %r13, 24(%rsp)
   movq %r14, 32(%rsp)
@skip   movq %r15, 40(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
      prefetcht0 (pA)
   mov %rsi, nnu
   mov %r8, pB
      prefetcht0 64(pA)
   mov %r9, pC
   mov nnu, nnu0
   mov $96, r96
      prefetcht0 128(pA)
@skip   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+16(%rsp), pfA     /* pfA = pBn */
   cmp pfA, pB                  /* if (pBn == pB) */
   CMOVEq FSIZE+8(%rsp), pfA    /*    pfA = pAn */
   cmp pfA, pA                  /* if (pAn == pA) */
   CMOVEq FSIZE+24(%rsp), pfA   /*    pfA = pCn */
   sub $-128, pfA
@skip   sub $-128, pfB
   sub $-128, pC                 /* extend range of 1-byte offsets */
      prefetcht0 192(pA)
/*
 * Set constants
 */
   mov $12*3*8, incPF           /* incPF = mu*nu*sizeof */
   mov $3*SZ, incBk             /* 24 = NU*sizeof = 3*8 = 24 */
      prefetcht0 256(pA)
@skip   mov $96, incAk               /* 96 = MU*sizeof = 12*8 = 96 */
   mov pB, pB0
/*
 * incAm = MU*sizeof*K = 12*8*K = 3*32*K
 */
   lea (KK, KK, 2), KK          /* KK = 3*K */
      prefetcht0 (pB)
   shl $5, KK                   /* KK = 3*32*K = 12*8*K = MU*sizeof*K */
   mov KK, incAm                /* incAm = MU*sizeof*K */
      prefetcht0 64(pB)
   add KK, pA                   /* pA[-kk] will access A */
   neg KK                       /* KK = -NU*sizeof*K */
   mov KK, KK0
   vbroadcastsd (pB), rB0
   ALIGN32
   MNLOOP:
/*
 *          First peeled iteration gets us to preloading next iter's data for
 *          loop while only doing 1 load/flop.  It does no adds to avoid
 *          having to zero the C registers.
 */

            vmovapd (pA,KK), rA0
            vmulpd rB0, rA0, rC00
            vmovapd 32(pA,KK), rA1
            vmulpd rB0, rA1, rC10
            vmovapd 64(pA,KK), rA2
            vmulpd rB0, rA2, rC20

            vbroadcastsd SZ(pB), rB1
            vmulpd rB1, rA0, rC01
            vbroadcastsd 2*SZ(pB), rB2
            vmulpd rB1, rA1, rC11
               vbroadcastsd 3*SZ(pB), rB0
            vmulpd rB1, rA2, rC21
               vbroadcastsd 4*SZ(pB), rB1
            add incBk, pB

            vmulpd rB2, rA0, rC02
               vmovapd 96(pA,KK), rA0
            vmulpd rB2, rA1, rC12
               vmovapd 128(pA,KK), rA1
            vmulpd rB2, rA2, rC22
               vmovapd 160(pA,KK), rA2
            add r96, KK
            jz KDONE_NOFINALPEEL
/*
 *          Stop loop on iteration early to set up for C, so 2nd peel is
 *          from end (bottom) of loop
 */
               prefC -128(pC)
            add incBk, pB
               prefC -64(pC)
            add r96, KK
            jz KLOOPDRAIN
/*
 *          Now peel a 3rd iteration (already peeled one from top and bottom)
 *          in order to do some prefetch.  This peel is the exact code from
 *          loop, with some prefetch commands added.
 */
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            vbroadcastsd -SZ(pB), rB2
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
               prefA -128(pfA)
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
               vbroadcastsd (pB), rB0

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
               prefA (pfA)
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
               prefC (pC)
               prefC 64(pC)
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
               vbroadcastsd SZ(pB), rB1
            add incBk, pB

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
               vmovapd (pA,KK), rA0
               add incPF, pfA
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
               vmovapd 32(pA,KK), rA1
@skip               add incPF, pfB
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
               vmovapd 64(pA,KK), rA2
            add r96, KK
            jz KLOOPDRAIN
               prefC 32(pC,r96)
/*
 *        Finally, start actual loop
 */
          ALIGN32
          KLOOP:
               prefetcht0 PFBDIST(pB)
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            vbroadcastsd -SZ(pB), rB2
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
               prefetcht0 PFADIST(pA,KK)
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
               vbroadcastsd (pB), rB0

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
               prefetcht0 PFADIST+64(pA,KK)
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
               vbroadcastsd SZ(pB), rB1
            add incBk, pB

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
               vmovapd (pA,KK), rA0
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
               vmovapd 32(pA,KK), rA1
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
               vmovapd 64(pA,KK), rA2
            add r96, KK
         jnz KLOOP
/* 
 *       Last iteration peeled off bottom to allow store of C; this should
 *       strongly improve BETA=0, but may hurt BETA=1.  This kernel written
 *       primarily for K-cleanup, which is always BETA=0, so do it.
 */
.local KLOOPDRAIN
KLOOPDRAIN:
#ifdef BETAN1
   #define VCOP vsubpd
#else
   #define VCOP vaddpd
#endif
            vbroadcastsd -SZ(pB), rB2
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            #ifndef BETA0
               VCOP  -128(pC), rC00, rC00
            #endif
            vmovapd rC00, -128(pC)
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
            #ifndef BETA0
               VCOP  -96(pC), rC10, rC10
            #endif
            vmovapd rC10, -96(pC)
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
            #ifndef BETA0
               VCOP  -64(pC), rC20, rC20
            #endif
            vmovapd rC20, -64(pC)

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
            #ifndef BETA0
               VCOP  -32(pC), rC01, rC01
            #endif
            vmovapd rC01, -32(pC)
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
            #ifndef BETA0
               VCOP  (pC), rC11, rC11
            #endif
            vmovapd rC11, (pC)
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
            #ifndef BETA0
               VCOP  32(pC), rC21, rC21
            #endif
            vmovapd rC21, 32(pC)

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
            #ifndef BETA0
               VCOP  64(pC), rC02, rC02
            #endif
            vmovapd rC02, 64(pC)
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
            #ifndef BETA0
               VCOP  96(pC), rC12, rC12
            #endif
            vmovapd rC12, 96(pC)
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
            #ifndef BETA0
               VCOP  32(pC,r96), rC22, rC22
            #endif
            vmovapd rC22, 32(pC,r96)
.local KLOOPDONE
KLOOPDONE:
         mov KK0, KK
         vbroadcastsd (pB), rB0
         add incPF, pC                /* pC += MU*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      vbroadcastsd (pB0), rB0
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA                     /* pA += MU*sizeof*K */
   sub $1, nmu
   jnz MNLOOP
/* DONE: */
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
   movq 16(%rsp), %r12
   movq 24(%rsp), %r13
   movq 32(%rsp), %r14
@skip   movq 40(%rsp), %r15
   add  $FSIZE, %rsp
   ret
/*
 * got answers in rCxx, just need to apply them to memory
 */
KDONE_NOFINALPEEL:
/*
 *       Write answer back out to C
 */
         #ifdef BETA0
            vmovapd rC00, -128(pC)
            vmovapd rC10, -96(pC)
            vmovapd rC20, -64(pC)
            vmovapd rC01, -32(pC)
            vmovapd rC11, (pC)
            vmovapd rC21, 32(pC)
            vmovapd rC02, 64(pC)
            vmovapd rC12, 96(pC)
            vmovapd rC22, 128(pC)
/*
 *          Add running sum in rCx with original C, then store back out
 */
         #else
            BETCOP -128(pC), rC00, rC00
            vmovapd rC00, -128(pC)
            BETCOP -96(pC), rC10, rC10
            vmovapd rC10, -96(pC)
            BETCOP -64(pC), rC20, rC20
            vmovapd rC20, -64(pC)
            BETCOP -32(pC), rC01, rC01
            vmovapd rC01, -32(pC)
            BETCOP (pC), rC11, rC11
            vmovapd rC11, (pC)
            BETCOP 32(pC), rC21, rC21
            vmovapd rC21, 32(pC)
            BETCOP 64(pC), rC02, rC02
            vmovapd rC02, 64(pC)
            BETCOP 96(pC), rC12, rC12
            vmovapd rC12, 96(pC)
            BETCOP 128(pC), rC22, rC22
            vmovapd rC22, 128(pC)
         #endif
         jmp KLOOPDONE
@ROUT ATL_damm12x3x2_avx.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#include "atlas_asm.h"
#ifndef KB 
   #define KB 0
#endif
#if (KB/2)*2 != KB
   #error "KB must be a multiple of 2!"
#endif
   
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

#define FSIZE 6*8

#if defined(SREAL) || defined(SCPLX)
   #define vmovapd vmovaps
   #define vsubpd vsubps
   #define vaddpd vaddps
   #define vmulpd vmulps
   #define vbroadcastsd vbroadcastss
   #define SZ 4
#else
   #define vmovapd vmovaps
   #define SZ 8
#endif
#ifdef BETAN1
   #define VCOP vsubpd
#else
   #define VCOP vaddpd
#endif
/*
 * floating point registers
 */
#define m0   %ymm0
#define rA0  %ymm1
#define rA1  %ymm2
#define rA2  %ymm3
#define rB0  %ymm4
#define rB1  %ymm5
#define rB2  %ymm6
#define rC00 %ymm7
#define rC10 %ymm8
#define rC20 %ymm9
#define rC01 %ymm10
#define rC11 %ymm11
#define rC21 %ymm12
#define rC02 %ymm13
#define rC12 %ymm14
#define rC22 %ymm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pC      %rbx  /* set after mov rsi to nnu () */
#define nnu     %rsi  /* comes in as rsi */
#define r128    %rbp
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r8   /* comes in as rdi */
#define incAm   %r9 
#define nnu0    %r10
#define pB0     %r11
/*
 * Prefetch definitions
 */
#define PFADIST 448
#define PFBDIST 128
#define prefA(pA_) prefetcht0 pA_
#define prefB(pB_) prefetcht0 pB_
#define prefC(pC_) prefetcht0 pC_
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub  $FSIZE, %rsp
   movq %rbp, (%rsp)
   nop
   nop
   movq %rbx, 8(%rsp)
#if 0
   movq %r12, 16(%rsp)
   movq %r13, 24(%rsp)
   movq %r14, 32(%rsp)
   movq %r15, 40(%rsp)
#endif
/*
 * Load paramaters
 */
   mov %r8, pB
   prefA(64(pA))
   mov %rdi, nmu
   mov $128, r128
   prefA(64(pA,r128))
   mov %r9, pC
   mov nnu, nnu0
   sub $-128, pC                 /* extend range of 1-byte offsets */
/*
 * Set constants
 */
   mov pB, pB0
/*
 * incAm = MU*sizeof*K = 12*8*K = 3*32*K
 */
   lea (KK, KK, 2), KK          /* KK = 3*K */
   shl $5, KK                   /* KK = 3*32*K = 12*8*K = MU*sizeof*K */
   mov KK, incAm                /* incAm = MU*sizeof*K */
   prefA((pA,r128,2))
   sub $3*2*32, KK              /* stop 1 iteration early for final peel */
   add KK, pA                   /* pA[-kk] will access A */
   neg KK                       /* KK = -NU*sizeof*K */
   mov KK, KK0
   vbroadcastsd (pB), rB0
   ALIGN32
   MNLOOP:
/*
 *          First peeled K=0 iteration gets to preloading next iter's data for
 *          loop while only doing 1 load/flop.  It does no adds to avoid
 *          having to zero the C registers.
 */

            vmovapd (pA,KK), rA0
            vmulpd rB0, rA0, rC00
            vmovapd 32(pA,KK), rA1
            vmulpd rB0, rA1, rC10
            vmovapd 64(pA,KK), rA2
            vmulpd rB0, rA2, rC20

            vbroadcastsd SZ(pB), rB1
            vmulpd rB1, rA0, rC01
            vbroadcastsd 2*SZ(pB), rB2
            vmulpd rB1, rA1, rC11
               vbroadcastsd 3*SZ(pB), rB0
            vmulpd rB1, rA2, rC21
               vbroadcastsd 4*SZ(pB), rB1

            vmulpd rB2, rA0, rC02
               vmovapd 96(pA,KK), rA0
            vmulpd rB2, rA1, rC12
               vmovapd 128(pA,KK), rA1
            vmulpd rB2, rA2, rC22
               vmovapd 160(pA,KK), rA2
               add $96, KK
/*
 *          For K=1 iteration of peel, prefetch C
 */
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            vbroadcastsd 5*SZ(pB), rB2
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
               prefC((pC))
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
               vbroadcastsd 6*SZ(pB), rB0

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
               prefC(64(pC))
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
               vbroadcastsd 7*SZ(pB), rB1
            add $6*SZ, pB
            add $96, KK

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
               vmovapd (pA,KK), rA0
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
               vmovapd 32(pA,KK), rA1
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
               vmovapd 64(pA,KK), rA2
/*
 *        Finally, start actual loop
 */
          ALIGN32
          KLOOP:
               prefB(PFBDIST(pB))
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            vbroadcastsd 2*SZ(pB), rB2
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
               prefA(PFADIST(pA,KK))
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
               vbroadcastsd 3*SZ(pB), rB0

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
               prefA(PFADIST+64(pA,KK))
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
               add $96, KK
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
               vbroadcastsd 4*SZ(pB), rB1

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
               vmovapd (pA,KK), rA0
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
               vmovapd 32(pA,KK), rA1
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
               vmovapd 64(pA,KK), rA2
/*
 *          K = 1 iteration
 */
            vmulpd rB0, rA0, m0
            vaddpd rC00, m0, rC00
            vbroadcastsd 5*SZ(pB), rB2
            vmulpd rB0, rA1, m0
            vaddpd rC10, m0, rC10
               prefA(PFADIST+128(pA,KK))
            vmulpd rB0, rA2, m0
            vaddpd rC20, m0, rC20
               vbroadcastsd 6*SZ(pB), rB0

            vmulpd rB1, rA0, m0
            vaddpd rC01, m0, rC01
               prefB(3*KB*8(pB))
            vmulpd rB1, rA1, m0
            vaddpd rC11, m0, rC11
            vmulpd rB1, rA2, m0
            vaddpd rC21, m0, rC21
               vbroadcastsd 7*SZ(pB), rB1
            add $6*SZ, pB

            vmulpd rB2, rA0, m0
            vaddpd rC02, m0, rC02
               vmovapd 96(pA,KK), rA0
            vmulpd rB2, rA1, m0
            vaddpd rC12, m0, rC12
               vmovapd 96+32(pA,KK), rA1
            vmulpd rB2, rA2, m0
            vaddpd rC22, m0, rC22
               vmovapd 96+64(pA,KK), rA2
            add $96, KK
         jnz KLOOP
/*
 *       Last iteration peeled off bottom to allow store of C; this should
 *       strongly improve BETA=0, but may hurt BETA=1.  This kernel written
 *       primarily for K-cleanup, which is always BETA=0, so do it.
 */
         vmulpd rB0, rA0, m0
         vaddpd rC00, m0, rC00
         vbroadcastsd 2*SZ(pB), rB2
         vmulpd rB0, rA1, m0
         vaddpd rC10, m0, rC10
            prefA(PFADIST(pA))
         vmulpd rB0, rA2, m0
         vaddpd rC20, m0, rC20
            vbroadcastsd 3*SZ(pB), rB0

         vmulpd rB1, rA0, m0
         vaddpd rC01, m0, rC01
            prefA(PFADIST+64(pA))
         vmulpd rB1, rA1, m0
         vaddpd rC11, m0, rC11
         vmulpd rB1, rA2, m0
         vaddpd rC21, m0, rC21
            vbroadcastsd 4*SZ(pB), rB1

         vmulpd rB2, rA0, m0
         vaddpd rC02, m0, rC02
            vmovapd 96(pA), rA0
         vmulpd rB2, rA1, m0
         vaddpd rC12, m0, rC12
            vmovapd 96+32(pA), rA1
         vmulpd rB2, rA2, m0
         vaddpd rC22, m0, rC22
            vmovapd 96+64(pA), rA2
/*
 *       K = 1 iteration
 */
         vmulpd rB0, rA0, m0
         vaddpd rC00, m0, rC00
         #ifndef BETA0
            VCOP -128(pC), rC00, rC00
         #endif
         vmovapd rC00, -128(pC)
         vbroadcastsd 5*SZ(pB), rB2
         vmulpd rB0, rA1, m0
         vaddpd rC10, m0, rC10
         #ifndef BETA0
            VCOP -96(pC), rC10, rC10
         #endif
         vmovapd rC10, -96(pC)
            prefA(PFADIST+96+128(pA))
         vmulpd rB0, rA2, m0
         vaddpd rC20, m0, rC20
         #ifndef BETA0
            VCOP -64(pC), rC20, rC20
         #endif
         vmovapd rC20, -64(pC)
            vbroadcastsd 6*SZ(pB), rB0

         vmulpd rB1, rA0, m0
         vaddpd rC01, m0, rC01
         #ifndef BETA0
            VCOP -32(pC), rC01, rC01
         #endif
         vmovapd rC01, -32(pC)
            prefB(3*KB*8(pB))
         vmulpd rB1, rA1, m0
         vaddpd rC11, m0, rC11
         #ifndef BETA0
            VCOP (pC), rC11, rC11
         #endif
         vmovapd rC11, (pC)
         vmulpd rB1, rA2, m0
         vaddpd rC21, m0, rC21
         #ifndef BETA0
            VCOP 32(pC), rC21, rC21
         #endif
         vmovapd rC21, 32(pC)
         add $6*SZ, pB

         vmulpd rB2, rA0, m0
         vaddpd rC02, m0, rC02
         #ifndef BETA0
            VCOP 64(pC), rC02, rC02
         #endif
         vmovapd rC02, 64(pC)
         vmulpd rB2, rA1, m0
         vaddpd rC12, m0, rC12
         #ifndef BETA0
            VCOP 96(pC), rC12, rC12
         #endif
         vmovapd rC12, 96(pC)
         vmulpd rB2, rA2, m0
         vaddpd rC22, m0, rC22
         #ifndef BETA0
            VCOP (pC,r128), rC22, rC22
         #endif
         vmovapd rC22, (pC,r128)

         mov KK0, KK
         vbroadcastsd (pB), rB0
         add $12*3*8, pC                /* pC += MU*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      vbroadcastsd (pB0), rB0
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA                     /* pA += MU*sizeof*K */
   sub $1, nmu
   jnz MNLOOP
#if 0
.local DONE
DONE: 
#endif
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
#if 0
   movq 16(%rsp), %r12 
   movq 24(%rsp), %r13
   movq 32(%rsp), %r14
   movq 40(%rsp), %r15
#endif
   add  $FSIZE, %rsp
   ret
@ROUT ATL_damm16x2_kb4_avx.S
#include "atlas_asm.h"
#define m0      %ymm0
#define rA0     %ymm1
#define rA1     %ymm2
#define rA2     %ymm3
#define rA3     %ymm4
#define rB0     %ymm5
#define rB1     %ymm6
#define rb1     %ymm7
#define rC00    %ymm8
#define rC10    %ymm9
#define rC20    %ymm10
#define rC30    %ymm11
#define rC01    %ymm12
#define rC11    %ymm13
#define rC21    %ymm14
#define rC31    %ymm15
#ifdef BETA1
   #define ADDC(r, p) vaddpd p, r, r
#elif defined(BETA0)
   #define ADDC(r, p) 
#else
   #define ADDC(r, p) vsubpd p, r, r
#endif

#define NMU %rdi
#define NNU %rsi
#define pA  %rcx
#define pB  %rdx
#define pC  %r9
#define pB0 %r8
#define NNU0 %rax
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
   mov pB0, pB
   mov NNU, NNU0
   MLOOP:
      vbroadcastsd (pB), rB0
      vmovapd (pA), rA0
      NLOOP:
/*
 *       K == 1
*/
         vmulpd rA0, rB0, rC00
         ADDC(rC00, (pC))
         vmovapd 32(pA), rA1
         vmulpd rA1, rB0, rC10
         ADDC(rC10, 32(pC))
         vmovapd 64(pA), rA2
         vmulpd rA2, rB0, rC20
         ADDC(rC20, 64(pC))
         vmovapd 96(pA), rA3
         vmulpd rA3, rB0, rC30
         ADDC(rC30, 96(pC))
         vbroadcastsd 8(pB), rB1
         vmulpd rA0, rB1, rC01
         ADDC(rC01, 128(pC))
            vbroadcastsd 16(pB), rB0
         vmulpd rA1, rB1, rC11
         ADDC(rC11, 160(pC))
            vmovapd 128(pA), rA0
         vmulpd rA2, rB1, rC21
         ADDC(rC21, 192(pC))
            vmovapd 160(pA), rA1
         vmulpd rA3, rB1, rC31
         ADDC(rC31, 224(pC))
            vmovapd 192(pA), rA2
/*
 *       K == 2
 */
         vmulpd rA0, rB0, m0
         vaddpd m0, rC00, rC00
            vmovapd 224(pA), rA3
         vmulpd rA1, rB0, m0
         vaddpd m0, rC10, rC10
            vbroadcastsd 24(pB), rB1
         vmulpd rA2, rB0, m0
         vaddpd m0, rC20, rC20
         vmulpd rA3, rB0, m0
         vaddpd m0, rC30, rC30
            vbroadcastsd 32(pB), rB0
         vmulpd rA0, rB1, m0
         vaddpd m0, rC01, rC01
            vmovapd 256(pA), rA0
         vmulpd rA1, rB1, m0
         vaddpd m0, rC11, rC11
            vmovapd 288(pA), rA1
         vmulpd rA2, rB1, m0
         vaddpd m0, rC21, rC21
            vmovapd 320(pA), rA2
         vmulpd rA3, rB1, m0
         vaddpd m0, rC31, rC31
            vmovapd 352(pA), rA3
/*
 *       K == 3
 */
         vmulpd rA0, rB0, m0
         vaddpd m0, rC00, rC00
            vbroadcastsd 40(pB), rB1
         vmulpd rA1, rB0, m0
         vaddpd m0, rC10, rC10
            vbroadcastsd 56(pB), rb1
         vmulpd rA2, rB0, m0
         vaddpd m0, rC20, rC20
         vmulpd rA3, rB0, m0
         vaddpd m0, rC30, rC30
            vbroadcastsd 48(pB), rB0
         vmulpd rA0, rB1, m0
         vaddpd m0, rC01, rC01
            vmovapd 384(pA), rA0
         vmulpd rA1, rB1, m0
         vaddpd m0, rC11, rC11
            vmovapd 416(pA), rA1
         vmulpd rA2, rB1, m0
         vaddpd m0, rC21, rC21
            vmovapd 448(pA), rA2
         vmulpd rA3, rB1, m0
         vaddpd m0, rC31, rC31
            vmovapd 480(pA), rA3
/*
 *       K == 4
 */
         vmulpd rA0, rB0, m0
         vaddpd m0, rC00, rC00
         vmovapd rC00, (pC)
         vmulpd rA1, rB0, m0
         vaddpd m0, rC10, rC10
         vmovapd rC10, 32(pC)
         vmulpd rA2, rB0, m0
         vaddpd m0, rC20, rC20
         vmovapd rC20, 64(pC)
         vmulpd rA3, rB0, m0
         vaddpd m0, rC30, rC30
         vmovapd rC30, 96(pC)
            vbroadcastsd 64(pB), rB0
         add $64, pB
         vmulpd rA0, rb1, m0
         vaddpd m0, rC01, rC01
         vmovapd rC01, 128(pC)
            vmovapd (pA), rA0
         vmulpd rA1, rb1, m0
         vaddpd m0, rC11, rC11
         vmovapd rC11, 160(pC)
         vmulpd rA2, rb1, m0
         vaddpd m0, rC21, rC21
         vmovapd rC21, 192(pC)
         vmulpd rA3, rb1, m0
         vaddpd m0, rC31, rC31
         vmovapd rC31, 224(pC)
         add $256, pC
      sub $1, NNU
      jnz NLOOP

      add $512, pA
      mov pB0, pB
      mov NNU0, NNU
   sub $1, NMU
   jnz MLOOP
/* DONE: */
   ret
@ROUT ATL_damm6x3x2_sse3.S ATL_sammm8x4x2_sse.S
   @extract -b @(topd)/gen.inc what=crsetup
   @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
@ROUT ATL_sammm8x4x2_sse.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_sammm8x4x2_sse.S
@ROUT ATL_damm6x3x2_sse3.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm6x3x2_sse3.S
@ROUT ATL_damm6x4x1_fma3.S ATL_damm12x4x1_fma3.S ATL_damm12x4x2_fma3.S @\
      ATL_samm24x4x2_fma3.S
   @extract -b @(topd)/gen.inc what=crsetup
   @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
@ROUT ATL_damm8x6x2_fma3.S
   @extract -b @(topd)/cw.inc lang=c -define cwdate 2015
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm8x6x2_fma3.S
@ROUT ATL_damm6x4x1_fma3.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm6x4x1_fma3.S
@ROUT ATL_samm24x4x2_fma3.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_samm24x4x2_fma3.S
@ROUT ATL_damm12x4x2_fma3.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm12x4x2_fma3.S
@ROUT ATL_damm12x4x1_fma3.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm12x4x1_fma3.S
@ROUT ATL_amm6x1x1_x87.S ATL_damm24x1x8_sse2.S ATL_damm24x1x1_sse2.S @\
      ATL_amm4x2x4_kb4.c
   @extract -b @(topd)/gen.inc what=crsetup
   @extract -b @(topd)/cw.inc lang=c -define cwdate 2012
@ROUT ATL_amm4x2x4_kb4.c
   @extract -b @(topd)/kernel/ClintWhaley/ATL_amm4x2x4_kb4.c
@ROUT ATL_amm6x1x1_x87.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_amm6x1x1_x87.S
@ROUT ATL_damm24x1x8_sse2.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm24x1x8_sse2.S
@ROUT ATL_damm24x1x1_sse2.S
   @extract -b @(topd)/kernel/ClintWhaley/ATL_damm24x1x1_sse2.S
@ROUT ATL_damm4x4x2rp_arm.S ATL_samm4x6x2_arm.S ATL_damm5x5x2_arm.S @\
      ATL_damm5x5x2_armpf.S ATL_damm5x5x1_armpf.S ATL_samm4x6x1_arm.S @\
      ATL_damm3x4x1_armpf.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2012
#ifndef ATL_GAS_ARM
   #error "This routine requires GAS/ARM assembly"
#endif
@ROUT ATL_damm4x4x2rp_arm.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm4x4x2rp_arm.S
@ROUT ATL_samm4x6x2_arm.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_samm4x6x2_arm.S
@ROUT ATL_samm4x6x1_arm.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_samm4x6x1_arm.S
@ROUT ATL_damm5x5x2_arm.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm5x5x2_arm.S
@ROUT ATL_damm5x5x2_armpf.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm5x5x2_armpf.S
@ROUT ATL_damm5x5x1_armpf.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm5x5x1_armpf.S
@ROUT ATL_damm3x4x1_armpf.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm3x4x1_armpf.S
@ROUT ATL_damm8x2x256_sse3.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#ifndef prefA
   #define prefA prefetcht0
#endif
#ifndef prefB
   #define prefB prefetcht2
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#elif defined(BETA1)
   #define BETCOP addpd
#endif
#define MOVAPD movaps
#define UNPCKHPD movhlps

#define m0      %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rA2     %xmm3
#define rA3     %xmm4
#define rB0     %xmm5
#define rB1     %xmm6
#define rb1     %xmm7
#define rC00    %xmm8
#define rC10    %xmm9
#define rC20    %xmm10
#define rC30    %xmm11
#define rC01    %xmm12
#define rC11    %xmm13
#define rC21    %xmm14
#define rC31    %xmm15

#define pA      %rcx
#define pB      %rdx
#define pC      %rbp
#define pfA     %rax
#define pfB     %rbx
#define pfC     %rdi
#define nnu     %rsi

#define incPF   %r8
#define incA    %r9
#define incB    %r10
#define pA0     %r11
#define nnu0    %r12
#define r256    %r13
#define nmu     %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
   #define FSIZE 6*8
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, (%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
   movq    %r14, 32(%rsp)
/*
 * Load parameters
 */
   mov  %rdi, nmu
   mov  %rsi, nnu0
   mov  %r8, pB
   mov  %r9, pC
   movq FSIZE+8(%rsp), pfA
   movq FSIZE+16(%rsp), pfB
   movq FSIZE+24(%rsp), pfC
   mov  $256, r256
   mov  $8*8*2, incPF
/*
 * Add 128 to ptrs to maximize unrolling range
 */
   add incPF, pA
   add incPF, pB
   mov pA, pA0
   mov pB, pB0
   mov nnu0, nnu
   mov $2*KB*8, incB   /* incB = nu*KB*sizeof */
   mov $8*KB*8, incA   /* incA = mu*KB*sizeof */

#if 0
   MLOOP:
      mov nnu0, nnu
      NLOOP:
#else
   ALIGN16
   .local MNLOOP
   MNLOOP:
#endif
/*
 *       Unroll first iteration to avoid zeroing rCxx
 */
         MOVAPD -128(pB), rB1
         movddup rB1, rB0
         MOVAPD -128(pA), rC00
         MOVAPD rC00, rC01
         mulpd rB0, rC00
         MOVAPD -112(pA), rC10
         MOVAPD rC10, rC11
         mulpd rB0, rC10
         MOVAPD -96(pA), rC20
         MOVAPD rC20, rC21
         mulpd rB0, rC20
         MOVAPD -80(pA), rC30
         MOVAPD rC30, rC31
         mulpd rB0, rC30


         UNPCKHPD rB1, rB1
         #if KB > 1
            MOVAPD -112(pB), rb1
         #endif
         mulpd rB1, rC01
         #if KB > 1
            movddup rb1, rB0
            MOVAPD -64(pA), rA0
         #endif
         mulpd rB1, rC11
         #if KB > 1
            MOVAPD -48(pA), rA1
         #endif
         mulpd rB1, rC21
         #if KB > 1
            MOVAPD -32(pA), rA2
         #endif
         mulpd rB1, rC31
         #if KB > 1
            UNPCKHPD rb1, rb1
         #endif

         #if KB > 1
               MOVAPD -16(pA), rA3
            MOVAPD rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            #if KB > 2
               MOVAPD -96(pB), rB1
            #endif
            MOVAPD rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            MOVAPD rA2, m0
            mulpd rB0, m0
            addpd m0, rC20
            UNPCKHPD rb1, rb1
            mulpd rA3, rB0
            addpd rB0, rC30
            #if KB > 2
               movddup rB1, rB0
            #endif
   
            mulpd rb1, rA0
            addpd rA0, rC01
            #if KB > 2
               MOVAPD (pA), rA0
            #endif
            mulpd rb1, rA1
            addpd rA1, rC11
            #if KB > 2
               MOVAPD 16(pA), rA1
            #endif
            mulpd rb1, rA2
            addpd rA2, rC21
            #if KB > 2
               MOVAPD 32(pA), rA2
            #endif
            mulpd rA3, rb1
            addpd rb1, rC31
            #if KB > 2
               MOVAPD 48(pA), rA3
               UNPCKHPD rB1, rB1
            #endif
         #endif

         #if KB > 2
            MOVAPD rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            #if KB > 3
               MOVAPD -80(pB), rb1
            #endif
            MOVAPD rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            MOVAPD rA2, m0
            mulpd rB0, m0
            addpd m0, rC20
            mulpd rA3, rB0
            addpd rB0, rC30
            #if KB > 3
               movddup rb1, rB0
            #endif
   
            mulpd rB1, rA0
            addpd rA0, rC01
            #if KB > 3
               UNPCKHPD rb1, rb1
               MOVAPD 64(pA), rA0
            #endif
            mulpd rB1, rA1
            addpd rA1, rC11
            #if KB > 3
               MOVAPD 80(pA), rA1
            #endif
            mulpd rB1, rA2
            addpd rA2, rC21
            #if KB > 3
               MOVAPD 96(pA), rA2
            #endif
            mulpd rA3, rB1
            addpd rB1, rC31
            #if KB > 3
               MOVAPD 112(pA), rA3
              add r256, pA
            #endif
         #endif

         #if KB > 3
            MOVAPD rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            #if KB > 4
               MOVAPD -64(pB), rB1
            #endif
            MOVAPD rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
               prefC 64(pC)
            MOVAPD rA2, m0
            mulpd rB0, m0
            addpd m0, rC20
               prefC 64(pC)
            UNPCKHPD rb1, rb1
            mulpd rA3, rB0
            addpd rB0, rC30
            #if KB > 4
               movddup rB1, rB0
            #endif
   
            mulpd rb1, rA0
            addpd rA0, rC01
            #if KB > 4
               MOVAPD -128(pA), rA0
               UNPCKHPD rB1, rB1
            #endif
            mulpd rb1, rA1
            addpd rA1, rC11
            #if KB > 4
               MOVAPD -112(pA), rA1
            #endif
            mulpd rb1, rA2
            addpd rA2, rC21
            #if KB > 4
               MOVAPD -96(pA), rA2
            #endif
            mulpd rA3, rb1
            addpd rb1, rC31
            #if KB > 4
               MOVAPD -80(pA), rA3
            #endif
         #endif

         ALIGN16
@iexp incT 1280000 0 +
@iexp k 4 0 +
@iexp ao -80 16 +
@iexp bo -64 16 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            MOVAPD rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            #if KB > @(k)
               MOVAPD @(bo)(pB), rb1
   @iexp bo @(bo) 16 +
   @iif @(bo) = @(incT)
               add r256, pB
      @iexp bo -128 0 +
   @endiif
            #endif
            MOVAPD rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            MOVAPD rA2, m0
            mulpd rB0, m0
            addpd m0, rC20
            mulpd rA3, rB0
            addpd rB0, rC30
            #if KB > @(k)
               movddup rb1, rB0
            #endif
   
            mulpd rB1, rA0
            addpd rA0, rC01
            #if KB > @(k)
               UNPCKHPD rb1, rb1
               MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
            mulpd rB1, rA1
            addpd rA1, rC11
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
            mulpd rB1, rA2
            addpd rA2, rC21
            #if KB > @(k) 
               MOVAPD @(ao)(pA), rA2
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
            mulpd rA3, rB1
            addpd rB1, rC31
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA3
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
         #endif

         #if KB > @(k)
   @iexp k @(k) 1 +
            MOVAPD rA0, m0
            mulpd rB0, m0
            addpd m0, rC00
            #if KB > @(k)
               MOVAPD @(bo)(pB), rB1
   @iexp bo @(bo) 16 +
   @iif @(bo) = @(incT)
               add r256, pB
      @iexp bo -128 0 +
   @endiif
            #endif
            MOVAPD rA1, m0
            mulpd rB0, m0
            addpd m0, rC10
            MOVAPD rA2, m0
            mulpd rB0, m0
            addpd m0, rC20
            UNPCKHPD rb1, rb1
            mulpd rA3, rB0
            addpd rB0, rC30
            #if KB > @(k)
               movddup rB1, rB0
            #endif
   
            mulpd rb1, rA0
            addpd rA0, rC01
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
               UNPCKHPD rB1, rB1
            #endif
            mulpd rb1, rA1
            addpd rA1, rC11
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
            mulpd rb1, rA2
            addpd rA2, rC21
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA2
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
            mulpd rA3, rb1
            addpd rb1, rC31
            #if KB > @(k)
               MOVAPD @(ao)(pA), rA3
   @iexp ao @(ao) 16 +
   @iif @(ao) = @(incT)
               add r256, pA
      @iexp ao -128 0 +
   @endiif
            #endif
         #endif
@endiwhile
/*
 *       Write answer out to C
 */
         #ifdef BETCOP
            BETCOP (pC), rC00
         #endif
         MOVAPD rC00, (pC)
         #ifdef BETCOP
            BETCOP 16(pC), rC10
         #endif
         MOVAPD rC10, 16(pC)
         #ifdef BETCOP
            BETCOP 32(pC), rC20
         #endif
         MOVAPD rC20, 32(pC)
         #ifdef BETCOP
            BETCOP 48(pC), rC30
         #endif
         MOVAPD rC30, 48(pC)
         #ifdef BETCOP
            BETCOP 64(pC), rC01
         #endif
         MOVAPD rC01, 64(pC)
         #ifdef BETCOP
            BETCOP 80(pC), rC11
         #endif
         MOVAPD rC11, 80(pC)
         #ifdef BETCOP
            BETCOP 96(pC), rC21
         #endif
         MOVAPD rC21, 96(pC)
         #ifdef BETCOP
            BETCOP 112(pC), rC31
         #endif
         MOVAPD rC31, 112(pC)
         add incPF, pC
         mov pA0, pA
@iif @(incT) = 128
         #if 2*KB*8 >= 256
            sub $((2*KB*8)/256)*256, pB
         #endif
@endiif
         add incB, pB
      dec nnu
      jnz MNLOOP

      mov nnu0, nnu
      add incA, pA0
      mov pA0, pA
      mov pB0, pB
   dec nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
   movq    32(%rsp), %r14
   add $FSIZE, %rsp
   ret
@ROUT !
@ROUT ATL_sammm8x4x256_sdupA.S
   @define mu @8@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014
/*
 * This manually-aligned Corei1 code sucks, and I don't know why
 */
#include "atlas_asm.h"
#define movapd movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
@skip #define pfC     %r13
#define r16     %r13

#define rm0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rB0     %xmm3
#define rB1     %xmm4
#define rB2     %xmm5
#define rB3     %xmm8
#define rC00    %xmm7
#define rC10    %xmm6
#define rC01    %xmm10
#define rC11    %xmm11
#define rC02    %xmm12
#define rC12    %xmm13
#define rC03    %xmm14
#define rC13    %xmm15
#ifndef pref
   #define pref prefetcht2
#endif
#ifndef prefB
   #define prefB prefetcht2
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subps
#else
   #define BETCOP addps
#endif
#define FSIZE 4*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pf      /* pf = pBn */
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
@skip   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
   mov $8*@(mu)*@(nu), incPF
   sub $96, pA    /* make loop offsets all > 1 byte */
   sub $-16, pB   /* get pB to 0 offset in loop by predec by 16 */
   mov $KB*@(mu)*4, incAm           /* incAm = KB*MU*size */
   movq pB, pB0
   mov $16, r16

   ALIGN16
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
         movsldup -16(pB), rC02   /* port2, 6 bytes 6 {b2, b2, b0, b0} */
         movddup rC02, rC00       /* port5, 5 bytes 11 {b0, b0, b0, b0} */
         .byte 0x3e               /* no prt,1 bytes 12 */
         movaps 96(pA), rA0       /* port2, 4 bytes 16 {a3, a2, a1, a0} */

         movaps rC00, rC10        /* port5, 3 bytes 3 */
         mulps rA0, rC00          /* port0, 3 bytes 6 */
         .byte 0x3e               /* no prt,1 bytes 7  */
         movaps 112(pA), rA1      /* port2, 4 bytes 11 */
         .byte 0x3e               /* no prt,1 bytes 12 */
         movhlps rC02, rC02       /* port5, 4 bytes 16 {b2, b2, b2, b2} */

         mulps rA1, rC10          /* port0, 3 bytes 3 */
         movaps rC02, rC12        /* port5, 4 bytes 7  */
         mulps rA0, rC02          /* port0  4 bytes 11 */
         .byte 0x3e               /* no prt,1 bytes 12 */
         prefetcht0 (pC)          /* port2, 4 bytes 16 */

         .byte 0x3e               /* no prt,1 bytes  1 */
         movshdup -16(pB), rC03  /* port2, 6 bytes  7 {b3, b3, b1, b1} */
         movddup rC03, rC01       /* port5, 5 bytes 12 {b1, b1, b1, b1} */
         mulps rA1, rC12          /* port0, 4 bytes 16 */

         #if KB > 1
            movsldup (pB), rB2    /* port2, 4 bytes 4 {b2, b2, b0, b0} */
         #else
            pref (pf)             /* port2, 4 bytes 4 */
         #endif
         movaps rC01, rC11        /* port5, 4 bytes 8 */
         mulps rA0, rC01          /* port0, 4 bytes 12 */
         add $16, pB              /* port1, 4 bytes 16 */

         #if KB > 1
            movshdup -16(pB),rB3/* port2, 6 bytes 6 {b3, b3, b1, b1} */
         #else
            .byte 0x3e           /* noprt, 1 bytes 1 */
            .byte 0x3e           /* noprt, 1 bytes 2 */
            pref 64(pf)          /* port2, 4 bytes 6 */
         #endif
         .byte 0x3e              /* noprt, 1 bytes 7 */
         movhlps rC03, rC03      /* port5, 4 bytes 11 {b3, b3, b3, b3} */
         .byte 0x3e              /* noprt, 1 bytes 12 */
         mulps rA1, rC11         /* port0, 4 bytes 16 */

         movaps rC03, rC13       /* port5, 4 bytes 4 */
         mulps rA0, rC03         /* port0, 4 bytes 8 */
         #if KB > 1
            movddup rB2, rB0     /* port5, 4 bytes 12 */
         #endif
         mulps rA1, rC13         /* port0, 4 bytes 16 */
         #if KB == 1
            add incPF, pf        /* port[0,1,5] */
         #endif

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 128 0 +
@iexp bo -128 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            movaps rA0, rm0             /* port 5, 3 bytes 10 */
            mulps  rB0, rm0             /* port 0, 3 bytes 13 */
            addps  rm0, rC00            /* port 1, 3 bytes 16 */

            movaps @(ao)(pA), rA1         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            mulps  rA1, rB0             /* port 0, 3 bytes 10 */
            addps  rB0, rC10            /* port 1, 4 bytes 14 */
            movaps rA0, rm0             /* port 5, 3 bytes 16 */

            .byte 0x3e                  /* no prt, 1 bytes  1 */
            movhlps rB2, rB2            /* port 5, 3 bytes  4 */
            .byte 0x3e                  /* no prt, 1 bytes  5 */
            mulps  rB2, rm0             /* port 0, 3 bytes  8 */
            addps  rm0, rC02            /* port 1, 4 bytes 12 */
            #if KB == @(k)
               pref (pf)                /* port 2, 4 bytes 16 */
            #else
               prefetcht0 (pC)          /* port 2, 4 bytes 16 */
            #endif
 
            .byte 0x3e                  /* no prt, 1 bytes 1 */
            mulps  rA1, rB2             /* port 0, 3 bytes 4 */
            addps  rB2, rC12            /* port 1, 4 bytes 8 */
            .byte 0x3e                  /* no prt, 1 bytes 9 */
            #if KB > @(k)
               movsldup (pB), rB2       /* port 2, 4 bytes 13 */
            #elif KB == @(k)
               prefB 64(pfB)            /* port 2, 4 bytes 13 */
            #endif
            movaps rA0, rm0             /* port 5, 3 bytes 16 */

            movddup rB3, rB1            /* port 5, 5 bytes 5 */
            mulps  rB1, rm0             /* port 0, 3 bytes 8 */
            addps  rm0, rC01            /* port 1, 4 bytes 12 */
            prefetcht0 -1(pB)           /* port 2, 4 bytes 16 */

            movhlps rB3, rB3            /* port 5, 4 bytes 4 */
            mulps  rA1, rB1             /* port 0, 3 bytes 7 */
            addps  rB1, rC11            /* port 1, 4 bytes 11 */
            .byte 0x3e                  /* no prt, 1 bytes 12 */
            prefetcht0  63(pB)          /* port 2, 4 bytes 16 */

            #if KB > @(k)
               movddup rB2, rB0         /* port 5, 4 bytes 4 {b0, b0, b0, b0} */
            #endif
            mulps rB3, rA0              /* port 0, 4 bytes 8 */
            addps rA0, rC03             /* port 1, 4 bytes 12 */
            prefetcht0 127(pB)          /* port 2, 4 bytes 16 */

            mulps rB3, rA1              /* port 0, 4 bytes 4 */
            addps rA1, rC13             /* port 1, 4 bytes 8 */
            #if KB > @(k)
               movshdup (pB), rB3       /* port 2, 5 bytes 13 */
   @iexp bo @(bo) 16 +
            #elif KB == @(k)
               add incPF, pf            /* port 5, 3 bytes 11 */
               add incPF, pfB           /* port0/1/5, 3 bytes, 14 */
            #endif
            add r16, pB                 /* port0/1/5, 3 bytes 16 */
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movaps rC00, (pC)
            BETCOP 16(pC), rC10
            movaps rC10, 16(pC)
            BETCOP 32(pC), rC01
            movaps rC01, 32(pC)
            BETCOP 48(pC), rC11
            movaps rC11, 48(pC)
            BETCOP 64(pC), rC02
            movaps rC02, 64(pC)
            BETCOP 80(pC), rC12
            movaps rC12, 80(pC)
            BETCOP 96(pC), rC03
            movaps rC03, 96(pC)
            BETCOP 112(pC), rC13
            movaps rC13, 112(pC)
         #else
            movaps rC00, (pC)
            movaps rC10, 16(pC)
            movaps rC01, 32(pC)
            movaps rC11, 48(pC)
            movaps rC02, 64(pC)
            movaps rC12, 80(pC)
            movaps rC03, 96(pC)
            movaps rC13, 112(pC)
         #endif
         sub $-128, pC
@skip         add $KB*4*4, pB
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_sammm8x4x256_sdup.S
   @define mu @8@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014
#include "atlas_asm.h"
#define movapd movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
#define pfC     %r13
#define r256    %r14

#define rm0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rB0     %xmm3
#define rB1     %xmm4
#define rB2     %xmm5
#define rB3     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC01    %xmm9
#define rC11    %xmm10
#define rC02    %xmm11
#define rC12    %xmm12
#define rC03    %xmm13
#define rC13    %xmm14
#ifndef pref
   #define pref prefetcht2
#endif
#ifndef prefB
   #define prefB prefetcht2
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subps
#else
   #define BETCOP addps
#endif
#define FSIZE 4*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pf      /* pf = pBn */
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
   mov $8*@(mu)*@(nu), incPF
/*
 * Extend range of small operands by starting at -128
 */
   sub $-128, pA
   sub $-128, pB
   mov $KB*@(mu)*4, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
         movsldup -128(pB), rC02  /* port2, {b2, b2, b0, b0} */
         movddup rC02, rC00       /* port5, {b0, b0, b0, b0} */

         movaps -128(pA), rA0     /* port2, {a3, a2, a1, a0} */
         movaps rC00, rC10        /* port5 */
         mulps rA0, rC00          /* port0 */

         movaps -112(pA), rA1     /* port2 */
         mulps rA1, rC10          /* port0 */
         movhlps rC02, rC02       /* port5, {b2, b2, b2, b2} */

         movaps rC02, rC12        /* port5 */
         mulps rA0, rC02          /* port0 */
         movshdup -128(pB), rC03  /* port2, {b3, b3, b1, b1} */

         movddup rC03, rC01       /* port5, {b1, b1, b1, b1} */
         mulps rA1, rC12          /* port0 */
         #if KB > 1
            movsldup -112(pB), rB2/* port2, {b2, b2, b0, b0} */
         #else
            pref (pf)
         #endif

         movaps rC01, rC11        /* port5 */
         mulps rA0, rC01          /* port0 */
         #if KB > 1
            movshdup -112(pB), rB3/* port2, {b3, b3, b1, b1} */
         #else
            pref 64(pf)
         #endif

         movhlps rC03, rC03      /* port5, {b3, b3, b3, b3} */
         mulps rA1, rC11         /* port0 */

         movaps rC03, rC13       /* port5 */
         mulps rA0, rC03         /* port0 */

         #if KB > 1
            .byte 0x3e
            movddup rB2, rB0     /* port5 */
         #endif
         mulps rA1, rC13         /* port0 */
         #if KB == 1
            add incPF, pf        /* port[0,1,5] */
         #endif

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -96 0 +
@iexp bo -96 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            movaps rA0, rm0             /* port 5, 3 bytes 10 */
            mulps  rB0, rm0             /* port 0, 3 bytes 13 */
            addps  rm0, rC00            /* port 1, 3 bytes 16 */

            movaps @(ao)(pA), rA1         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            mulps  rA1, rB0             /* port 0, 3 bytes 10 */
            addps  rB0, rC10            /* port 1, 3 bytes 13 */
            movaps rA0, rm0             /* port 5, 3 bytes 16 */

            movhlps rB2, rB2            /* port 5, {b2, b2, b2, b2} */
            mulps  rB2, rm0             /* port 0, 3 bytes  8 */
            addps  rm0, rC02            /* port 1, 4 bytes 12 */
            #if KB == @(k)
               pref (pf)                /* port 2 */
            #endif

            mulps  rA1, rB2             /* port 0 */
            addps  rB2, rC12            /* port 1 */
            #if KB > @(k)
               movsldup @(bo)(pB), rB2    /* port 2 */
            #elif KB == @(k)
               prefB 64(pfB)            /* port 2 */
            #endif
            movaps rA0, rm0             /* port 5 */

            movddup rB3, rB1            /* port 5 */
            mulps  rB1, rm0             /* port 0 */
            addps  rm0, rC01            /* port 1 */

            movhlps rB3, rB3            /* port 5 */
            mulps  rA1, rB1             /* port 0 */
            addps  rB1, rC11            /* port 1 */

            #if KB > @(k)
               movddup rB2, rB0         /* port 5, {b0, b0, b0, b0} */
            #endif
            mulps rB3, rA0              /* port 0 */
            addps rA0, rC03             /* port 1 */

            mulps rB3, rA1              /* port 0 */
            addps rA1, rC13             /* port 1 */
            #if KB > @(k)
               movshdup @(bo)(pB), rB3    /* port 2 */
            #elif KB == @(k)
               add incPF, pf            /* port5 */
               add incPF, pfB           /* port0/1/5 */
            #endif
   @iexp bo @(bo) 16 +
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movaps rC00, (pC)
            BETCOP 16(pC), rC10
            movaps rC10, 16(pC)
            BETCOP 32(pC), rC01
            movaps rC01, 32(pC)
            BETCOP 48(pC), rC11
            movaps rC11, 48(pC)
            BETCOP 64(pC), rC02
            movaps rC02, 64(pC)
            BETCOP 80(pC), rC12
            movaps rC12, 80(pC)
            BETCOP 96(pC), rC03
            movaps rC03, 96(pC)
            BETCOP 112(pC), rC13
            movaps rC13, 112(pC)
         #else
            movaps rC00, (pC)
            movaps rC10, 16(pC)
            movaps rC01, 32(pC)
            movaps rC11, 48(pC)
            movaps rC02, 64(pC)
            movaps rC12, 80(pC)
            movaps rC03, 96(pC)
            movaps rC13, 112(pC)
         #endif
         sub $-128, pC
         add $KB*4*4, pB
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_dammm4x4x256_sse3.S
   @define mu @4@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define movapd movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
#define pfC     %r13
#define r256    %r14

#define rA0     %xmm0
#define rA1     %xmm1
#define rB0     %xmm2
#define rB1     %xmm3
#define rB2     %xmm4
#define rB3     %xmm5
#define rC00    %xmm6
#define rC10    %xmm7
#define rC01    %xmm8
#define rC11    %xmm9
#define rC02    %xmm10
#define rC12    %xmm11
#define rC03    %xmm12
#define rC13    %xmm13
#define rm0     %xmm14
#define unpckhpd movhlps
/* #define movddup pshufd $0x44, */
#ifndef pref
   #define pref prefetcht1
#endif
#ifndef prefB
   #define prefB prefetcht1
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pf      /* pf = pBn */
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
   mov $8*@(mu)*@(nu), incPF
/*
 * Extend range of small operands by starting at -128
 */
   sub $-128, pA
   sub $-128, pB
   mov $KB*@(mu)*8, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
         movapd -128(pB), rB1
         movddup rB1, rB0
         movapd -128(pA), rC00

         movapd rC00, rC01
         mulpd rB0, rC00
         unpckhpd rB1, rB1
         movapd -112(pA), rC10
         movapd rC10, rC11
         mulpd rB0, rC10

         movapd -112(pB), rB3
         movddup rB3, rB2
         movapd rC01, rC02
         mulpd rB1, rC01
         unpckhpd rB3, rB3
         movapd rC11, rC12
         mulpd rB1, rC11
         #if KB > 1
            movapd -96(pB), rB1
         #else
            pref (pf)
         #endif

         #if KB > 1
            movapd -96(pA), rA0
         #else
            pref 64(pf)
         #endif
         movapd rC02, rC03
         mulpd rB2, rC02
         movapd rC12, rC13
         mulpd rB2, rC12
         #if KB > 1
            movddup rB1, rB0
         #else
            add incPF, pf
         #endif

         mulpd rB3, rC03
         prefC (pC)
         mulpd rB3, rC13
         prefC 64(pC)
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -80 0 +
@iexp bo -80 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iif k = 48
            nop
   @endiif
   @iexp k @(k) 1 +
            movapd rB0, rm0
            mulpd rA0, rm0
            addpd rm0, rC00
            movapd @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            mulpd rA1, rB0
            addpd rB0, rC10
            unpckhpd rB1, rB1

            movapd rB1, rm0
            mulpd rA0, rm0
            addpd rm0, rC01
            movapd @(bo)(pB), rB3
   @iexp bo @(bo) 16 +
            mulpd rA1, rB1
            addpd rB1, rC11
            movddup rB3, rB2

            movapd rB2, rm0
            mulpd rA0, rm0
            addpd rm0, rC02
            unpckhpd rB3, rB3
            mulpd rA1, rB2
            addpd rB2, rC12

            #if KB > @(k)
               movapd @(bo)(pB), rB1
   @iexp bo @(bo) 16 +
            #elif KB == @(k)
               pref (pf)
            #endif
            mulpd rB3, rA0
            addpd rA0, rC03
            #if KB > @(k)
               movapd @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #elif KB == @(k)
               pref (pfC)
               add incPF, pfC
            #endif
            mulpd rA1, rB3
            addpd rB3, rC13
            #if KB > @(k)
               movddup rB1, rB0
            #elif KB == @(k)
               prefetcht1 64(pfB)
               add incPF, pfB
               add incPF, pf
            #endif
         #endif   
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movapd rC00, (pC)
            BETCOP 16(pC), rC10
            movapd rC10, 16(pC)
            BETCOP 32(pC), rC01
            movapd rC01, 32(pC)
            BETCOP 48(pC), rC11
            movapd rC11, 48(pC)
            BETCOP 64(pC), rC02
            movapd rC02, 64(pC)
            BETCOP 80(pC), rC12
            movapd rC12, 80(pC)
            BETCOP 96(pC), rC03
            movapd rC03, 96(pC)
            BETCOP 112(pC), rC13
            movapd rC13, 112(pC)
         #else
            movapd rC00, (pC)
            movapd rC10, 16(pC)
            movapd rC01, 32(pC)
            movapd rC11, 48(pC)
            movapd rC02, 64(pC)
            movapd rC12, 80(pC)
            movapd rC03, 96(pC)
            movapd rC13, 112(pC)
         #endif
         sub $-128, pC
         add $KB*4*8, pB
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_sammm8x4x256_sse3.S
   @define mu @4@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define movaps movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pf      %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
#define pfC     %r13
#define r256    %r14

#define rm0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rB0     %xmm3
#define rB1     %xmm4
#define rB2     %xmm5
#define rB3     %xmm6
#define rb3     %xmm7
#define rC00    %xmm8
#define rC10    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC02    %xmm12
#define rC12    %xmm13
#define rC03    %xmm14
#define rC13    %xmm15
#ifndef pref
   #define pref prefetcht1
#endif
#ifndef prefB
   #define prefB prefetcht1
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subps
#else
   #define BETCOP addps
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pf      /* pf = pBn */
   movq FSIZE+8(%rsp), pfB      /* pfB = pAn */
   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
   mov $4*@(mu)*@(nu), incPF
/*
 * Extend range of small operands by starting at -128
 */
   sub $-128, pA
   sub $-128, pB
   mov $KB*8*4, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

   ALIGN8
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
         movaps -128(pB), rB3
         pshufd $0x00, rB3, rB0
         movaps -128(pA), rC00

         movaps rC00, rC01
         mulps rB0, rC00
         pshufd $0x55, rB3, rB1
         movaps -112(pA), rC10
         movaps rC10, rC11
         mulps rB0, rC10

         #if KB > 1
            movaps -112(pB), rb3
         #else
            pref (pf)
         #endif
         movaps rC01, rC02
         mulps rB1, rC01
         #if KB > 1
            movaps -96(pA), rA0
         #else
            pref 64(pf)
         #endif
         movaps rC11, rC12
         mulps rB1, rC11

         #if KB > 1
            movaps -80(pA), rA1
         #endif
         pshufd $0xAA, rB3, rB2
         movaps rC02, rC03
         mulps rB2, rC02
         shufps $0xFF, rB3, rB3
         movaps rC12, rC13
         mulps rB2, rC12
         #if KB > 1
            pshufd $0x00, rb3, rB0
         #else
            add incPF, pf
         #endif

         mulps rB3, rC03
         prefC (pC)
         mulps rB3, rC13
         prefC 64(pC)
/*
 *       2nd peeled K iteration
 */
         #if KB > 1
            #if KB > 2
               movaps -96(pB), rB3
            #endif
            pshufd $0x55, rb3, rB1
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00
            pshufd $0xAA, rb3, rB2
            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10
            shufps $0xFF, rb3, rb3
            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01
            #if KB > 2
               pshufd $0x00, rB3, rB0
            #endif
            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11
            #if KB > 2
               pshufd $0x55, rB3, rB1
            #endif
            movaps rA0, rm0
            mulps rB2, rm0
            addps rm0, rC02
            movaps rA1, rm0
            mulps rB2, rm0
            addps rm0, rC12
            #if KB > 2
               pshufd $0xAA, rB3, rB2
            #endif

            mulps rb3, rA0
            addps rA0, rC03
            #if KB > 2
               movaps -64(pA), rA0
            #endif
            mulps rA1, rb3
            addps rb3, rC13
            #if KB > 2
               movaps -48(pA), rA1
            #endif
         #endif
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -32 0 +
@iexp bo -80 0 +
@iexp k 2 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            #if KB > @(k)
               movaps @(bo)(pB), rb3
   @iexp bo @(bo) 16 +
            #elif KB == @(k)
               pref (pf)
            #endif
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00
            shufps $0xFF, rB3, rB3
            #if KB == @(k)
               add incPF, pf
            #endif
            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10
            #if KB > @(k)
               pshufd $0x00, rb3, rB0
            #elif KB == @(k)
               pref (pfC)
            #endif
            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01
            #if KB == @(k)
               add incPF, pfC
            #endif
            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11
            #if KB > @(k)
               pshufd $0x55, rb3, rB1
            #elif KB == @(k)
               pref (pfB)
            #endif
            movaps rA0, rm0
            mulps rB2, rm0
            addps rm0, rC02
            #if KB == @(k)
               add incPF, pfB
            #endif
            movaps rA1, rm0
            mulps rB2, rm0
            addps rm0, rC12
            #if KB > @(k)
               pshufd $0xAA, rb3, rB2
            #endif

            mulps rB3, rA0
            addps rA0, rC03
            #if KB > @(k)
               movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #endif
            mulps rA1, rB3
            addps rB3, rC13
            #if KB > @(k)
               movaps @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            #endif
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
            #if KB > @(k)
               movaps @(bo)(pB), rB3
   @iexp bo @(bo) 16 +
            #endif
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00
            shufps $0xFF, rb3, rb3
            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10
            #if KB > @(k)
               pshufd $0x00, rB3, rB0
            #endif
            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01
            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11
            #if KB > @(k)
               pshufd $0x55, rB3, rB1
            #endif
            movaps rA0, rm0
            mulps rB2, rm0
            addps rm0, rC02
            movaps rA1, rm0
            mulps rB2, rm0
            addps rm0, rC12
            #if KB > @(k)
               pshufd $0xAA, rB3, rB2
            #endif

            mulps rb3, rA0
            addps rA0, rC03
            #if KB > @(k)
               movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #endif
            mulps rA1, rb3
            addps rb3, rC13
            #if KB > @(k)
               movaps @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            #endif
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movaps rC00, (pC)
            BETCOP 16(pC), rC10
            movaps rC10, 16(pC)
            BETCOP 32(pC), rC01
            movaps rC01, 32(pC)
            BETCOP 48(pC), rC11
            movaps rC11, 48(pC)
            BETCOP 64(pC), rC02
            movaps rC02, 64(pC)
            BETCOP 80(pC), rC12
            movaps rC12, 80(pC)
            BETCOP 96(pC), rC03
            movaps rC03, 96(pC)
            BETCOP 112(pC), rC13
            movaps rC13, 112(pC)
         #else
            movaps rC00, (pC)
            movaps rC10, 16(pC)
            movaps rC01, 32(pC)
            movaps rC11, 48(pC)
            movaps rC02, 64(pC)
            movaps rC12, 80(pC)
            movaps rC03, 96(pC)
            movaps rC13, 112(pC)
         #endif
         sub $-128, pC
         add $KB*4*4, pB
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
@ROUT ATL_sammm8x4x256_sse2.S
   @define mu @4@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define movaps movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pfA     %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
#define pfC     %r13
#define r256    %r14

#define rm0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rB0     %xmm3
#define rB1     %xmm4
#define rB2     %xmm5
#define rC00    %xmm6
#define rC10    %xmm7
#define rC01    %xmm8
#define rC11    %xmm9
#define rC02    %xmm10
#define rC12    %xmm11
#define rC03    %xmm12
#define rC13    %xmm13
#ifndef pref
   #define pref prefetcht1
#endif
#ifndef prefB
   #define prefB prefetcht1
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subps
#else
   #define BETCOP addps
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pfB     /* pf = pBn */
   movq FSIZE+8(%rsp), pfA      /* pfB = pAn */
   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
   mov $4*@(mu)*@(nu), incPF
/*
 * Make it so pA/pB always use 32-bit constants by subtracting 128
 */
   sub $96, pA
   sub $96, pB
   mov $KB*8*4, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

           ALIGN32
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
#if 0
         xorps rC00, rC00
         movaps rC00, rC10
         xorps rC01, rC01
         movaps rC00, rC11
         xorps rC02, rC02
         movaps rC00, rC12
         xorps rC03, rC03
         movaps rC00, rC13
#endif
         .byte 0x3e                     /* no prt, 1 bytes  1 */
         movaps 96(pB), rB2             /* port 2, 4 bytes  5 */
         .byte 0x3e                     /* no prt, 1 bytes  6 */
         pshufd $0x00, rB2, rB0         /* port 5, 5 bytes 11 */

         .byte 0x3e                     /* no prt, 1 bytes 12 */
         movaps 96(pA), rC00            /* port 2, 4 bytes 16 */
         movaps rC00, rC01              /* port 5, 4 bytes 04 */
         mulps  rB0, rC00               /* port 0, 3 bytes 07 */

         movaps 112(pA), rC10           /* port 2, 4 bytes 11 */
         .byte 0x3e                     /* no prt, 1 bytes 12 */
         movaps rC10, rC11              /* port 5, 4 bytes 16 */
         mulps rB0, rC10                /* port 0, 3 bytes 03 */

         pshufd $0x55, rB2, rB1         /* port 5, 5 bytes 08 */
            prefetcht0 (pC)             /* port 2, 4 bytes 12 */

         movaps rC01, rC02              /* port 5, 4 bytes 16 */
         mulps rB1, rC01                /* port 0, 4 bytes 04 */

         movaps rC11, rC12              /* port 5, 4 bytes 08 */
         mulps rB1, rC11                /* port 0, 4 bytes 12 */
            movaps 112(pB), rB1         /* port 2, 4 bytes 16 */

        movaps rC02, rC03               /* port 5, 4 bytes 04 */
         .byte 0x3e                     /* no prt, 1 bytes 05 */
           prefetcht0 64(pC)            /* port 2, 5 bytes 10 */

         .byte 0x3e                     /* no prt, 1 bytes 11 */
        pshufd $0xAA, rB2, rB0          /* port 5, 5 bytes 16 */
        mulps rB0, rC02                 /* port 0, 4 bytes 04 */
           prefetcht2 (pfB)             /* port 2, 3 bytes 07 */

        movaps rC12, rC13               /* port 5, 4 bytes 11 */
         .byte 0x3e                     /* no prt, 1 bytes 12 */
        mulps rB0, rC12                 /* port 0, 4 bytes 16 */
         .byte 0x3e                     /* no prt, 1 bytes 01 */
           prefetcht2 (pfA)             /* port 2, 4 bytes 05 */

         .byte 0x3e                     /* no prt, 1 bytes 06 */
        pshufd $0xFF, rB2, rm0          /* port 5, 5 bytes 11 */
         .byte 0x3e                     /* no prt, 1 bytes 12 */
        mulps rm0, rC03                 /* port 0, 4 bytes 16 */

            pshufd $0x00, rB1, rB0      /* port 5, 5 bytes 05 */
        mulps rm0, rC13                 /* port 0, 4 bytes 09 */
            
            movaps rB1, rB2             /* port 5, 3 bytes 12 */
            .byte 0x66                  /* no prt, 1 bytes 13 */
            .byte 0x66                  /* no prt, 1 bytes 14 */
            .byte 0x66                  /* no prt, 1 bytes 15 */
            nop                         /* port 0, 1 bytes 16 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 128 0 +
@iexp bo 128 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            movaps rA0, rm0             /* port 5, 3 bytes 10 */
            mulps  rB0, rm0             /* port 0, 3 bytes 13 */
            addps  rm0, rC00            /* port 1, 3 bytes 16 */

            movaps @(ao)(pA), rA1         /* port 2, 7 bytes  7 */
   @iexp ao @(ao) 16 +
            mulps  rA1, rB0             /* port 0, 3 bytes 10 */
            addps  rB0, rC10            /* port 1, 3 bytes 13 */
            movaps rA0, rm0             /* port 5, 3 bytes 16 */

            pshufd $0x55, rB2, rB1      /* port 5, 5 bytes  5 */
            mulps  rB1, rm0             /* port 0, 3 bytes  8 */
            addps  rm0, rC01            /* port 1, 4 bytes 12 */

            .byte 0x3e                  /* no prt, 1 bytes 13 */
            mulps  rA1, rB1             /* port 0, 3 bytes 16 */
            addps  rB1, rC11            /* port 1, 4 bytes 04 */
            movaps @(bo)(pB), rB1         /* port 2, 7 bytes 11 */
   @iexp bo @(bo) 16 +
            pshufd $0xAA, rB2, rB0      /* port 5, 5 bytes 16 */

            movaps rA0, rm0             /* port 5, 3 bytes 03 */
            mulps rB0, rm0              /* port 0, 3 bytes 06 */
            addps rm0, rC02             /* port 1, 4 bytes 10 */

            .byte 0x3e                  /* no prt, 1 bytes 11 */
            .byte 0x3e                  /* no prt, 1 bytes 12 */
            .byte 0x3e                  /* no prt, 1 bytes 13 */
            mulps rA1, rB0              /* port 0, 3 bytes 16 */
            addps rB0, rC12             /* port 1, 4 bytes 04 */
            pshufd $0xFF, rB2, rm0      /* port 5, 5 bytes 09 */

            
            mulps rm0, rA0              /* port 0, 3 bytes 12 */
            addps rA0, rC03             /* port 1, 4 bytes 16 */
            pshufd $0x00, rB1, rB0      /* port 5, 5 bytes 05 */

            mulps rm0, rA1              /* port 0, 3 bytes 08 */
            movaps rB1, rB2             /* port 5, 3 bytes 11 */
            .byte 0x3e                  /* no prt, 1 bytes 12 */
            addps rA1, rC13             /* port 1, 4 bytes 16 */
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movaps rC00, (pC)
            BETCOP 16(pC), rC10
            movaps rC10, 16(pC)
            BETCOP 32(pC), rC01
            movaps rC01, 32(pC)
            BETCOP 48(pC), rC11
            movaps rC11, 48(pC)
            BETCOP 64(pC), rC02
            movaps rC02, 64(pC)
            BETCOP 80(pC), rC12
            movaps rC12, 80(pC)
            BETCOP 96(pC), rC03
            movaps rC03, 96(pC)
            BETCOP 112(pC), rC13
            movaps rC13, 112(pC)
         #else
            movaps rC00, (pC)
            movaps rC10, 16(pC)
            movaps rC01, 32(pC)
            movaps rC11, 48(pC)
            movaps rC02, 64(pC)
            movaps rC12, 80(pC)
            movaps rC03, 96(pC)
            movaps rC13, 112(pC)
         #endif
         add incPF, pfA
         add incPF, pfB
         sub $-128, pC
         add $KB*4*4, pB
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm24x1x256_sse3.S
   @define mu @4@
   @define nu @4@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define MOVAPD movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pfA     %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
#define r256    %r8
@skip #define pfC     %r13

#define rA0     %xmm0
#define rB0     %xmm1
#define rC00    %xmm2
#define rC01    %xmm3
#define rC02    %xmm4
#define rC03    %xmm5
#define rC04    %xmm6
#define rC05    %xmm7
#define rC06    %xmm8
#define rC07    %xmm9
#define rC08    %xmm10
#define rC09    %xmm11
#define rC10    %xmm12
#define rC11    %xmm13
/*
 * Small problems on AMD can be prefetched to L1
 */
#if KB < 45
   #ifdef ATL_3DNow
      #ifdef ATL_MOVEB
         #define prefA(m_)
         #define prefB(m_) prefetcht0 m_
      #elif defined(ATL_MOVEA)
         #define prefA(m_) prefetcht0 m_
         #define prefB(m_)
      #endif
   #else
      #define prefA(m_) prefetcht1 m_
      #define prefB(m_) prefetcht1 m_
   #endif
#else
   #if KB > 78  /* very large problems should only pref 1 block */
      #ifdef ATL_MOVEB
         #define prefB(m_) prefetcht2 m_
         #define prefA(m_)
      #else
         #define prefA(m_) prefetcht2 m_
         #define prefB(m_)
      #endif
   #else
      #define prefA(m_) prefetcht2 m_
      #define prefB(m_) prefetcht2 m_
   #endif
#endif
#ifdef ATL_3DNow
   #define prefC(m_) prefetchw m_
#else
   #define prefC(m_) prefetcht0 m_
#endif
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
     prefetcht0 (pA)
   sub $FSIZE, %rsp
     prefetcht0 (%r8)
   movq    %rbp, 0(%rsp)
     prefetcht0 64(pA)
   movq    %rbx, 8(%rsp)
     prefetcht0 128(pA)
   movq    %r12, 16(%rsp)
     prefetcht0 192(pA)
@skip   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
     prefetcht0 256(pA)
   mov nnu, nnu0
     prefetcht0 320(pA)
   movq FSIZE+16(%rsp), pfB     /* pf = pBn */
     prefetcht0 384(pA)
   movq FSIZE+8(%rsp), pfA      /* pfB = pAn */
@skip   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
     prefetcht0 448(pA)
   mov $24*1*8, incPF   /* incPF = MU*NU*sizeof */
/*
 * Maximize are small-op size by adding 128 to ptrs
 */
   sub $-128, pA
     prefetcht0 512(pA)
   sub $-128, pB
     prefetcht0 576(pA)
   sub $-128, pC
     prefetcht0 640(pA)
   sub $-128, pfA
   sub $-128, pfB
@skip   sub $-128, pfC
   mov $KB*24*8, incAm           /* incAm = KB*MU*size */
   movq pB, pB0
   mov $256, r256

   ALIGN16
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
#if 0
         xorps rC00, rC00
         MOVAPD rC00, rC01
         xorps rC02, rC02
         MOVAPD rC00, rC03
         xorps rC04, rC04
         MOVAPD rC00, rC05
         xorps rC06, rC06
         MOVAPD rC00, rC07
         xorps rC08, rC08
         MOVAPD rC00, rC09
         xorps rC10, rC10
         MOVAPD rC00, rC11
#endif
/*
 *       Unroll 1st iteration for zeroing or rCxx & prefetch 
 */
         movddup -128(pB), rC00
         MOVAPD rC00, rC01
         mulpd -128(pA), rC00
         prefC(-128(pC))
         MOVAPD rC01, rC02
         mulpd -112(pA), rC01
         prefC(-64(pC))
         MOVAPD rC02, rC03
         mulpd -96(pA), rC02
         prefC((pC))
         MOVAPD rC03, rC04
         mulpd -80(pA), rC03
         #ifdef ATL_MOVEB
            prefB(-128(pfB))
         #else
            prefC(64(pC))
         #endif
         MOVAPD rC04, rC05
         mulpd -64(pA), rC04
         #ifdef ATL_MOVEB
            prefB(-64(pfB))
         #else
            prefC(-128(pC,r256))
         #endif
         MOVAPD rC05, rC06
         mulpd -48(pA), rC05
         #ifdef ATL_MOVEB
            prefB((pfB))
         #else
            prefC(-64(pC,r256))
         #endif
         MOVAPD rC06, rC07
         mulpd -32(pA), rC06
         prefA(-128(pfA))
         MOVAPD rC07, rC08
         mulpd -16(pA), rC07
         prefA(-64(pfA))
         MOVAPD rC08, rC09
         mulpd (pA), rC08
         prefA((pfA))
         MOVAPD rC09, rC10
         mulpd 16(pA), rC09
         MOVAPD rC10, rC11
         mulpd 32(pA), rC10
         mulpd 48(pA), rC11
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 64 0 +
@iexp bo -120 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 +
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC00
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC01
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC02
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC03
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC04
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC05
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC06
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC07
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC08
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC09
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC10
            mulpd @(ao)(pA), rB0
   @iexp ao @(ao) 16 +
            addpd rB0, rC11
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP -128(pC), rC00
            MOVAPD rC00, -128(pC)
            BETCOP -112(pC), rC01
            MOVAPD rC01, -112(pC)
            BETCOP -96(pC), rC02
            MOVAPD rC02, -96(pC)
            BETCOP -80(pC), rC03
            MOVAPD rC03, -80(pC)
            BETCOP -64(pC), rC04
            MOVAPD rC04, -64(pC)
            BETCOP -48(pC), rC05
            MOVAPD rC05, -48(pC)
            BETCOP -32(pC), rC06
            MOVAPD rC06, -32(pC)
            BETCOP -16(pC), rC07
            MOVAPD rC07, -16(pC)
            BETCOP (pC), rC08
            MOVAPD rC08, (pC)
            BETCOP 16(pC), rC09
            MOVAPD rC09, 16(pC)
            BETCOP 32(pC), rC10
            MOVAPD rC10, 32(pC)
            BETCOP 48(pC), rC11
            MOVAPD rC11, 48(pC)
         #else
            MOVAPD rC00, -128(pC)
            MOVAPD rC01, -112(pC)
            MOVAPD rC02, -96(pC)
            MOVAPD rC03, -80(pC)
            MOVAPD rC04, -64(pC)
            MOVAPD rC05, -48(pC)
            MOVAPD rC06, -32(pC)
            MOVAPD rC07, -16(pC)
            MOVAPD rC08, (pC)
            MOVAPD rC09, 16(pC)
            MOVAPD rC10, 32(pC)
            MOVAPD rC11, 48(pC)
         #endif
         add incPF, pfA
         add incPF, pfB
         add $192, pC
         add $KB*1*8, pB   /* pB += KB*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
@skip   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm24x1x256_sse3b.S
   @define mu @24@
   @define nu @1@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define MOVAPD movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pfA     %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
@skip #define pfC     %r13

#define rA0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rC00    %xmm3
#define rC01    %xmm4
#define rC02    %xmm5
#define rC03    %xmm6
#define rC04    %xmm7
#define rC05    %xmm8
#define rC06    %xmm9
#define rC07    %xmm10
#define rC08    %xmm11
#define rC09    %xmm12
#define rC10    %xmm13
#define rC11    %xmm14
#ifndef prefA
   #define prefA prefetcht1
#endif
#ifndef prefB
   #define prefB prefetcht1
#endif
#ifndef prefC
   #ifdef ATL_3DNow
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subpd
#else
   #define BETCOP addpd
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
     prefetcht0 (pA)
   sub $FSIZE, %rsp
     prefetcht0 (%r8)
   movq    %rbp, 0(%rsp)
     prefetcht0 64(pA)
   movq    %rbx, 8(%rsp)
     prefetcht0 128(pA)
   movq    %r12, 16(%rsp)
     prefetcht0 192(pA)
@skip   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
     prefetcht0 256(pA)
   mov nnu, nnu0
     prefetcht0 320(pA)
   movq FSIZE+16(%rsp), pfB     /* pf = pBn */
     prefetcht0 384(pA)
   movq FSIZE+8(%rsp), pfA      /* pfB = pAn */
@skip   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
     prefetcht0 448(pA)
   mov $24*1*8, incPF   /* incPF = MU*NU*sizeof */
/*
 * Maximize are small-op size by adding 128 to ptrs
 */
   sub $-128, pA
     prefetcht0 512(pA)
   sub $-128, pB
     prefetcht0 576(pA)
   sub $-128, pC
     prefetcht0 640(pA)
   sub $-128, pfA
   sub $-128, pfB
@skip   sub $-128, pfC
   mov $KB*24*8, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

   ALIGN16
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration of K-loop to handle init of C to 0
 */
#if 0
         xorps rC00, rC00
         MOVAPD rC00, rC01
         xorps rC02, rC02
         MOVAPD rC00, rC03
         xorps rC04, rC04
         MOVAPD rC00, rC05
         xorps rC06, rC06
         MOVAPD rC00, rC07
         xorps rC08, rC08
         MOVAPD rC00, rC09
         xorps rC10, rC10
         MOVAPD rC00, rC11
#else
/*
 *       Unroll 1st iteration for zeroing or rCxx & prefetch 
 */
         MOVAPD -128(pB), rB1   /* fpmisc */
         movddup rB1, rC00      /* fpadd */
         movhlps rB1, rB1       /* fmul */
         MOVAPD rC00, rC01
         mulpd -128(pA), rC00
         prefC -128(pC)
         MOVAPD rC01, rC02
         mulpd -112(pA), rC01
         prefC -64(pC)
         MOVAPD rC02, rC03
         mulpd -96(pA), rC02
         prefC (pC)
         MOVAPD rC03, rC04
         mulpd -80(pA), rC03
         prefB -128(pfB)
         MOVAPD rC04, rC05
         mulpd -64(pA), rC04
         prefB -64(pfB)
         MOVAPD rC05, rC06
         mulpd -48(pA), rC05
         prefB (pfB)
         MOVAPD rC06, rC07
         mulpd -32(pA), rC06
         prefA -128(pfA)
         MOVAPD rC07, rC08
         mulpd -16(pA), rC07
         prefA -64(pfA)
         MOVAPD rC08, rC09
         mulpd (pA), rC08
         prefA (pfA)
         MOVAPD rC09, rC10
         mulpd 16(pA), rC09
         MOVAPD rC10, rC11
         mulpd 32(pA), rC10
         mulpd 48(pA), rC11
         #if KB > 1
            MOVAPD 64(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC00
            MOVAPD 80(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC01
            MOVAPD 96(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC02
            MOVAPD 112(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC03
            MOVAPD 128(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC04
            MOVAPD 144(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC05
            MOVAPD 160(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC06
            MOVAPD 176(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC07
            MOVAPD 192(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC08
            MOVAPD 208(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC09
            MOVAPD 224(pA), rA0
            mulpd rB1, rA0
            addpd rA0, rC10
            mulpd 240(pA), rB1
            addpd rB1, rC11
         #endif
#endif
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
   ALIGN16
@iexp ao 256 0 +
@iexp bo -112 0 +
@iexp k 0 2 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            MOVAPD @(bo)(pB), rB1   /* fpmisc */
   @iexp bo @(bo) 16 +
            movddup rB1, rB0  /* fpadd */
            movhlps rB1, rB1  /* fmul */
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC00
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC01
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC02
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC03
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC04
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC05
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC06
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC07
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC08
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC09
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC10
            mulpd @(ao)(pA), rB0
   @iexp ao @(ao) 16 +
            addpd rB0, rC11
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC00
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC01
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC02
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC03
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC04
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC05
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC06
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC07
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC08
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC09
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB1, rA0
            addpd rA0, rC10
            mulpd @(ao)(pA), rB1
   @iexp ao @(ao) 16 +
            addpd rB1, rC11
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP -128(pC), rC00
            MOVAPD rC00, -128(pC)
            BETCOP -112(pC), rC01
            MOVAPD rC01, -112(pC)
            BETCOP -96(pC), rC02
            MOVAPD rC02, -96(pC)
            BETCOP -80(pC), rC03
            MOVAPD rC03, -80(pC)
            BETCOP -64(pC), rC04
            MOVAPD rC04, -64(pC)
            BETCOP -48(pC), rC05
            MOVAPD rC05, -48(pC)
            BETCOP -32(pC), rC06
            MOVAPD rC06, -32(pC)
            BETCOP -16(pC), rC07
            MOVAPD rC07, -16(pC)
            BETCOP (pC), rC08
            MOVAPD rC08, (pC)
            BETCOP 16(pC), rC09
            MOVAPD rC09, 16(pC)
            BETCOP 32(pC), rC10
            MOVAPD rC10, 32(pC)
            BETCOP 48(pC), rC11
            MOVAPD rC11, 48(pC)
         #else
            MOVAPD rC00, -128(pC)
            MOVAPD rC01, -112(pC)
            MOVAPD rC02, -96(pC)
            MOVAPD rC03, -80(pC)
            MOVAPD rC04, -64(pC)
            MOVAPD rC05, -48(pC)
            MOVAPD rC06, -32(pC)
            MOVAPD rC07, -16(pC)
            MOVAPD rC08, (pC)
            MOVAPD rC09, 16(pC)
            MOVAPD rC10, 32(pC)
            MOVAPD rC11, 48(pC)
         #endif
         add incPF, pfA
         add incPF, pfB
         add $192, pC
         add $KB*1*8, pB   /* pB += KB*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
@skip   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_sammm32x1x256_sse.S
   @define mu @48@
   @define nu @1@
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pfA     %rbp
#define pB0     %r12
#define incPF   %rbx
#define pfB     %rdx
#define incAm   %r11 
@skip #define pfC     %r13

#define rA0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rB3     %xmm4
#define rC00    %xmm5
#define rC01    %xmm6
#define rC02    %xmm7
#define rC03    %xmm8
#define rC04    %xmm9
#define rC05    %xmm10
#define rC06    %xmm11
#define rC07    %xmm12
#define rC08    %xmm13
#ifndef prefA
   #if KB <= 72
      #define prefA prefetcht2
   #else
      #define prefA prefetcht2
   #endif
#endif
#ifndef prefB
   #if KB <= 72
      #define prefB prefetcht0
   #else
      #define prefB prefetcht2
   #endif
#endif
#ifndef prefC
   #ifdef ATL_3DNowXXX
      #define prefC prefetchw
   #else
      #define prefC prefetcht0
   #endif
#endif
#ifdef BETAN1
   #define BETCOP subps
#else
   #define BETCOP addps
#endif
#define FSIZE 6*8
/*
                    rdi      rsi    rdx        rcx         r8        r9  
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC, 
                  8(%rsp)    16(%rsp)     24(%rsp)   
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
     prefetcht0 (pA)
   sub $FSIZE, %rsp
     prefetcht0 (%r8)
   movq    %rbp, 0(%rsp)
     prefetcht0 64(pA)
   movq    %rbx, 8(%rsp)
     prefetcht0 128(pA)
   movq    %r12, 16(%rsp)
     prefetcht0 192(pA)
@skip   movq    %r13, 24(%rsp)
@skip   movq    %r14, 32(%rsp)
@skip   movq    %r15, 40(%rsp)
/*
 * Load paramaters
 */
   movq %r8, pB
     prefetcht0 256(pA)
   mov nnu, nnu0
     prefetcht0 320(pA)
   movq FSIZE+16(%rsp), pfB     /* pf = pBn */
     prefetcht0 384(pA)
   movq FSIZE+8(%rsp), pfA      /* pfB = pAn */
@skip   movq FSIZE+24(%rsp), pfC    /* pfC = pCn */
     prefetcht0 448(pA)
   mov $48*1*4, incPF   /* incPF = MU*NU*sizeof */
/*
 * Maximize are small-op size by adding 128 to ptrs
 */
   sub $-128, pA
     prefetcht0 512(pA)
   sub $-128, pB
     prefetcht0 576(pA)
@skip   sub $-128, pfA
@skip   sub $-128, pfB
@skip   sub $-128, pfC
   mov $KB*32*4, incAm           /* incAm = KB*MU*size */
   movq pB, pB0

   ALIGN16
   .local MNLOOP
   MNLOOP:
/*
 *       Peel 1st iteration for zeroing or rCxx & prefetch 
 */
#if 0
         xorps rC00, rC00
         xorps rC01, rC01
         xorps rC02, rC02
         xorps rC03, rC03
         xorps rC04, rC04
         xorps rC05, rC05
         xorps rC06, rC06
         xorps rC07, rC07
#else
         movaps -128(pB), rB3    /* fmisc */
         pshufd $0x00, rB3, rB0  /* fadd */
         movaps -128(pA), rC00   /* fmul */

         mulps rB0, rC00         /* fmul */
         pshufd $0x55, rB3, rB1  /* fadd */
         movaps -112(pA), rC01   /* fmisc */

         mulps rB0, rC01         /* fmul */
         pshufd $0xAA, rB3, rB2  /* fadd */
         movaps -96(pA), rC02     /* fmisc */

         mulps rB0, rC02         /* fmul */
         shufps $0xFF, rB3, rB3  /* fadd */
         movaps -80(pA), rC03     /* fmisc */

         mulps rB0, rC03         /* fmul */
         movaps -64(pA), rC04     /* fadd */
         movaps -48(pA), rC05     /* fmisc */


         mulps rB0, rC04
         movaps -32(pA), rC06     /* fadd  */
         movaps -16(pA), rC07     /* fmisc */

         mulps rB0, rC05
         prefC (pC)
         prefC 64(pC)

         mulps rB0, rC06         /* fmul */
         prefB (pfB)
         prefA (pfA)
//         prefetcht2 64(pfB)

         mulps rB0, rC07         /* fmul */
//         prefetcht2 64(pfA)

         #if KB > 1
            movaps (pA), rA0
            mulps rB1, rA0
            addps rA0, rC00
            movaps 16(pA), rA0
            mulps rB1, rA0
            addps rA0, rC01
            movaps 32(pA), rA0
            mulps rB1, rA0
            addps rA0, rC02
            movaps 48(pA), rA0
            mulps rB1, rA0
            addps rA0, rC03
            movaps 64(pA), rA0
            mulps rB1, rA0
            addps rA0, rC04
            movaps 80(pA), rA0
            mulps rB1, rA0
            addps rA0, rC05
            movaps 96(pA), rA0
            mulps rB1, rA0
            addps rA0, rC06
            mulps 112(pA), rB1
            addps rB1, rC07
         #endif
         #if KB > 2
            movaps 128(pA), rA0
            mulps rB2, rA0
            addps rA0, rC00
            movaps 144(pA), rA0
            mulps rB2, rA0
            addps rA0, rC01
            movaps 160(pA), rA0
            mulps rB2, rA0
            addps rA0, rC02
            movaps 176(pA), rA0
            mulps rB2, rA0
            addps rA0, rC03
            movaps 192(pA), rA0
            mulps rB2, rA0
            addps rA0, rC04
            movaps 208(pA), rA0
            mulps rB2, rA0
            addps rA0, rC05
            movaps 224(pA), rA0
            mulps rB2, rA0
            addps rA0, rC06
            mulps 240(pA), rB2
            addps rB2, rC07
         #endif
         #if KB > 3
            movaps 256(pA), rA0
            mulps rB3, rA0
            addps rA0, rC00
            movaps 272(pA), rA0
            mulps rB3, rA0
            addps rA0, rC01
            movaps 288(pA), rA0
            mulps rB3, rA0
            addps rA0, rC02
            movaps 304(pA), rA0
            mulps rB3, rA0
            addps rA0, rC03
            movaps 320(pA), rA0
            mulps rB3, rA0
            addps rA0, rC04
            movaps 336(pA), rA0
            mulps rB3, rA0
            addps rA0, rC05
            movaps 352(pA), rA0
            mulps rB3, rA0
            addps rA0, rC06
            mulps 368(pA), rB3
            addps rB3, rC07
            #if KB > 4
               movaps -112(pB), rB3   /* fmisc */
            #endif
         #endif
#endif
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 384 0 +
@iexp bo -96 0 +
@iexp k 4 0 +
         ALIGN16
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            pshufd $0x00, rB3, rB0  /* fadd */
            pshufd $0x55, rB3, rB1  /* fmul */
            movaps @(ao)(pA), rA0   /* fpmisc */
   @iexp ao @(ao) 16 +
            pshufd $0xAA, rB3, rB2  /* fadd */
            shufps $0xFF, rB3, rB3  /* fmul */
            nop
            mulps rB0, rA0
            addps rA0, rC00
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC01
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC02
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC03
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC04
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC05
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB0, rA0
            addps rA0, rC06
            mulps @(ao)(pA), rB0
   @iexp ao @(ao) 16 +
            addps rB0, rC07
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC00
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC01
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC02
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC03
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC04
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC05
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB1, rA0
            addps rA0, rC06
            mulps @(ao)(pA), rB1
   @iexp ao @(ao) 16 +
            addps rB1, rC07
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC00
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC01
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC02
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC03
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC04
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC05
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB2, rA0
            addps rA0, rC06
            mulps @(ao)(pA), rB2
   @iexp ao @(ao) 16 +
            addps rB2, rC07
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC00
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC01
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC02
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC03
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC04
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC05
            movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulps rB3, rA0
            addps rA0, rC06
            movaps @(ao)(pA), rA0
            mulps @(ao)(pA), rB3
   @iexp ao @(ao) 16 +
            addps rB3, rC07
            #if KB > @(k)
               movaps @(bo)(pB), rB3   /* fmisc */
            #endif
   @iexp bo @(bo) 16 +
         #endif
@endiwhile
         #if defined(BETA1) || defined(BETAN1)
            BETCOP (pC), rC00
            movaps rC00, (pC)
            BETCOP 16(pC), rC01
            movaps rC01, 16(pC)
            BETCOP 32(pC), rC02
            movaps rC02, 32(pC)
            BETCOP 48(pC), rC03
            movaps rC03, 48(pC)
            BETCOP 64(pC), rC04
            movaps rC04, 64(pC)
            BETCOP 80(pC), rC05
            movaps rC05, 80(pC)
            BETCOP 96(pC), rC06
            movaps rC06, 96(pC)
            BETCOP 112(pC), rC07
            movaps rC07, 112(pC)
         #else
            movaps rC00, (pC)
            movaps rC01, 16(pC)
            movaps rC02, 32(pC)
            movaps rC03, 48(pC)
            movaps rC04, 64(pC)
            movaps rC05, 80(pC)
            movaps rC06, 96(pC)
            movaps rC07, 112(pC)
         #endif
         add incPF, pfA
         add incPF, pfB
         add $128, pC
         add $KB*1*4, pB   /* pB += KB*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
@skip   movq    24(%rsp), %r13
@skip   movq    32(%rsp), %r14
@skip   movq    40(%rsp), %r15
   add $FSIZE, %rsp
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm6x4x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rA2     %xmm3
#define rC00    %xmm4
#define rC10    %xmm5
#define rC20    %xmm6
#define rC01    %xmm7
#define rC11    %xmm8
#define rC21    %xmm9
#define rC02    %xmm10
#define rC12    %xmm11
#define rC22    %xmm12
#define rC03    %xmm13
#define rC13    %xmm14
#define rC23    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r192    %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq 24(%rsp), pfB
   mov KK, KK0
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   mov $192, r192
   mov pA, pA0
   mov pB, pB0
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */
   vxorps rA2, rA2, rA2

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vmovddup      -128(pB), rB0
         vmovaps -128(pA), rA0
         vmulpd rA0, rB0, rC00
         vmovaps -112(pA), rA1
         vmulpd rA1, rB0, rC10
         vmovaps -96(pA), rA2
         vmulpd rA2, rB0, rC20

         vmovddup      -120(pB), rB0
         vmulpd rA0, rB0, rC01
            prefC(-128(pC))
         vmulpd rA1, rB0, rC11
         vmulpd rA2, rB0, rC21

         vmovddup      -112(pB), rB0
         vmulpd rA0, rB0, rC02
         vmulpd rA1, rB0, rC12
            prefC((pC))
         vmulpd rA2, rB0, rC22

         vmovddup      -104(pB), rB0
         vmulpd rA0, rB0, rC03
            vmovapd -80(pA), rA0
         vmulpd rA1, rB0, rC13
            vmovapd -64(pA), rA1
         vmulpd rA2, rB0, rC23


/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -48 0 +
@iexp bo -96 0 +
@iexp k 0 1 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC00
            movapd @(ao)(pA), rA2
   @iexp ao @(ao) 16 + 
            FMAC rA1, rB0, rC10
            FMAC rA2, rB0, rC20

            movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC01
            FMAC rA1, rB0, rC11
            FMAC rA2, rB0, rC21

            movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC02
            FMAC rA1, rB0, rC12
            FMAC rA2, rB0, rC22

            movddup @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC03
               movapd @(ao)(pA), rA0
   @iexp ao @(ao) 16 + 
            FMAC rA1, rB0, rC13
               movapd @(ao)(pA), rA1
   @iexp ao @(ao) 16 + 
            FMAC rA2, rB0, rC23
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         #ifndef BETA0
            VCOP 16(pC), rC03
         #endif
         movapd rC03, 16(pC)
         #ifndef BETA0
            VCOP 32(pC), rC13
         #endif
         movapd rC13, 32(pC)
         #ifndef BETA0
            VCOP 48(pC), rC23
         #endif
         movapd rC23, 48(pC)
         add $KB*4*8, pB     /* pB += KB*NU*sizeof */
         add $192, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm6x3x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %xmm0
#define rB1     %xmm1
#define rB2     %xmm2
#define rA0     %xmm3
#define rA1     %xmm4
#define rA2     %xmm5
#define rC00    %xmm6
#define rC10    %xmm7
#define rC20    %xmm8
#define rC01    %xmm9
#define rC11    %xmm10
#define rC21    %xmm11
#define rC02    %xmm12
#define rC12    %xmm13
#define rC22    %xmm14
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pC
   sub $-128, pfB
   sub $-128, pfA
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */
   vxorps rA1, rA1, rA1
   vxorps rA2, rA2, rA2

   ALIGN16
   MLOOP:
      NLOOP:
@beginskip
/*
 * Using 4 byte offsets:
 * movddup: 1*4 + 3*5 = 4 + 15 = 19 bytes
 * 2 int increments using consts: 2*4 = 8bytes
 * movaps: 3*1 + 2*4 = 12 bytes
 * FMAC : 3*4*5 = 60 bytes
 * total: 19 + 8 + 12 + 60 = 99 bytes
 * -> 7 16 byte words, 13 bytes wasted
 * -> 4 32 byte words, 29 bytes wasted
 *
 */
/*
 * 4 byte @, 4x2 reg blk
 * movddup: 8*2 = 16 bytes
 * movaps : 4*3 = 12 bytes
 * FMAC   : 5*8 = 40 bytes
 * total  : 68 bytes, 6 16byte words, 6 bytes waste
 */
/*
 * 4-byte @, 3x3 reg blk
 * movddup: 8*3 = 24 bytes  -> 4 (add) + 4 + 5 = 13 bytes
 * movaps : 7*3 = 21 bytes  -> 4 (add) + 3 + 8 = 15 bytes
 * FMAC   : 5*9 = 45 bytes
 * total  : 90 bytes, 6 16byte words, 6 bytes waste
 */

         movaps (pA), rA0               /* 3 03 */
         movddup (pB), rB0              /* 4 04 */
         FMAC rA0, rB0, rC00            /* 5 12 */
         add $48, pA                    /* 4 16 */

         movaps -32(pA), rA1            /* 4 04 */
         FMAC rA1, rB0, rC10            /* 5 09 */
         movaps -16(pA), rA1            /* 4 13 */
         add r24, pB                    /* 3 16 */

         FMAC rA2, rB0, rC20            /* 5 05 */
         movddup -16(pB), rB1            /* 5 10 */
         FMAC rA0, rB1, rC01            /* 5 15 */

         FMAC rA1, rB1, rC11            /* 5 05 */
         movddup -8(pB), rB2            /* 5 10 */
         FMAC rA2, rB1, rC21            /* 5 15 */

         FMAC rA0, rB2, rC02            /* 5 05 */
         FMAC rA1, rB2, rC12            /* 5 05 */
         FMAC rA2, rB2, rC22            /* 5 05 */

         movddup @(bo)(pB), rB0         /* 8 08 */
         FMAC rA0, rB0, rC00            /* 5 13 */
         movaps @(ao)(pA), pA1          /* 7 20 */
         FMAC rA1, rB0, rC10            /* 5 25 */
         movaps @(ao)(pA), pA2          /* 7 32 */

         FMAC rA2, rB0, rC20            /* 5 05 */
         movddup @(bo)(pB), rB1         /* 8 13 */
         FMAC rA0, rB1, rC01            /* 5 18 */
         .byte 0x3e                     /* 5 left */
         movddup @(bo)(pB), rB2         /* 8 26 */
         FMAC rA1, rB1, rC11            /* 5 31 */

         FMAC rA2, rB1, rC21            /* 5 05 */
         FMAC rA0, rB2, rC02            /* 5 10 */
         .byte 0x3e                     /* 4 left */
         movaps @(ao)(pA), pA0          /* 7 17 */
         FMAC rA1, rB2, rC12            /* 5 22 */
         FMAC rA2, rB2, rC22            /* 5 27 */
         .byte 0x66                     /* 3 left */
         .byte 0x66                     /* 2 left */
         .byte 0x66                     /* 1 left */
         nop
/*
 * USING 4-BYTE ADDRESSES, REQUIRES 128 BYTES CODESPACE:
 * movddup 8*4 = 32 bytes   -> 3 (inc) + 4*1 + 5*3 = 7 + 15 = 22
 * movaps: 3*7 = 21 bytes   -> 3 (inc) + 3*1 + 4*2 = 6 + 8 = 14 bytes
 * FMAC : 3*4*5 = 60 bytes
 * total = 32 + 21 + 60 = 113 bytes  --> try to do in 128 bytes
 * -> 8 16 byte words, 15 bytes wasted
 * -> 4 32 bytes words, 15 bytes wasted
 */

         .byte 0x3e                 /* 14 */
         movddup @(bo)(pB), rB0     /* 8 08 */
         FMAC rA0, rB0, rC00        /* 5 13 */
         FMAC rA1, rB0, rC10        /* 5 18 */
         .byte 0x3e                 /* 13 */
         movaps @(ao)(pA), rA2      /* 7 25 */
         FMAC rA2, rB0, rC20        /* 5 30 */

         movddup @(bo)(pB), rB0     /* 8 08 */
         FMAC rA0, rB0, rC01        /* 5 13 */
         FMAC rA1, rB0, rC11        /* 5 18 */
         FMAC rA2, rB0, rC21        /* 5 23 */
         .byte 0x3e                 /* 12 */
         movddup @(bo)(pB), rB0     /* 8 31 */

         FMAC rA0, rB0, rC02        /* 5 05 */
         FMAC rA1, rB0, rC12        /* 5 10 */
         FMAC rA2, rB0, rC22        /* 5 15 */
         movddup @(bo)(pB), rB0     /* 8 23 */
         FMAC rA0, rB0, rC03        /* 5 28 */
         .byte 0x66                 /* 11 */
         .byte 0x66                 /* 10 */
         .byte 0x66                 /* 9 */
         nop                        /* 8 */

         movaps @(ao)(pA), rA0      /* 7 07 */
         FMAC rA1, rB0, rC13        /* 5 12 */
         movaps @(ao)(pA), rA1      /* 7 19 */
         FMAC rA2, rB0, rC23        /* 5 24 */

@endskip
/*
 *       First iteration peeled to handle init of rC
 */
         vmovddup (pB), rB0     /* 4  04 */
         vmovaps (pA), rA0      /* 4  08 */
         vmulpd rA0, rB0, rC00  /* 4  12 */
         add $48, pA            /* 4  16 */

         movaps -32(pA), rA1    /* 4  04 */
         vmulpd rA1, rB0, rC10  /* 4  08 */
         movaps -16(pA), rA2    /* 4  12 */
         vmulpd rA2, rB0, rC20  /* 4  16 */

         vmovddup 8(pB), rB1    /* 5  05 */
         vmulpd rA0, rB1, rC01  /* 4  09 */
            prefC(-128(pC))     /* 4  13 */
         add r24, pB            /* 3  16 */

         vmulpd rA1, rB1, rC11  /* 4  04 */
            .byte 0x3e
            prefB(-128(pfB))    /* 4  08 */
         vmulpd rA2, rB1, rC21  /* 4  12 */
            add incPF, pfB      /* 3  15 */

         vmovddup -8(pB), rB2   /* 5  05 */
         vmulpd rA0, rB2, rC02  /* 4  09 */
            prefC((pC))         /* 3  12 */
         vmulpd rA1, rB2, rC12  /* 4  16 */

         vmulpd rA2, rB2, rC22  /* 4  04 */
            prefB(-16(pfB))     /* 4  08 */
            .byte 0x3e
            prefA(-128(pfA))    /* 4  12 */
            add incPF, pfA      /* 3  15 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp k 0 1 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps (pA), rA0               /* 3 03 */
            movddup (pB), rB0              /* 4 04 */
            FMAC rA0, rB0, rC00            /* 5 12 */
            add $48, pA                    /* 4 16 */

            movaps -32(pA), rA1            /* 4 04 */
            FMAC rA1, rB0, rC10            /* 5 09 */
            movaps -16(pA), rA2            /* 4 13 */
            add r24, pB                    /* 3 16 */

            FMAC rA2, rB0, rC20            /* 5 05 */
            .byte 0x3e
            movddup -16(pB), rB1           /* 5 10 */
            FMAC rA0, rB1, rC01            /* 5 15 */

            FMAC rA1, rB1, rC11            /* 5 05 */
            .byte 0x3e
            movddup -8(pB), rB2            /* 5 10 */
            FMAC rA2, rB1, rC21            /* 5 15 */

            FMAC rA0, rB2, rC02            /* 5 05 */
            FMAC rA1, rB2, rC12            /* 5 05 */
            .byte 0x3e
            FMAC rA2, rB2, rC22            /* 5 05 */
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         mov pA0, pA
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm30x1x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %xmm0
#define rC00    %xmm1
#define rC01    %xmm2
#define rC02    %xmm3
#define rC03    %xmm4
#define rC04    %xmm5
#define rC05    %xmm6
#define rC06    %xmm7
#define rC07    %xmm8
#define rC08    %xmm9
#define rC09    %xmm10
#define rC10    %xmm11
#define rC11    %xmm12
#define rC12    %xmm13
#define rC13    %xmm14
#define rC14    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r256    %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   sub $-128, pfB
   sub $-128, pfA
   mov $256, r256
   mov pA, pA0
   mov pB, pB0
   mov $15*1*8, incPF
/*
 * incAm = 30*sizeof*K = 30*8*K = 16*15*K
 */
   lea (KK, KK,4), incAm        /* incAm = 5*K */
   lea (incAm, incAm,2), incAm  /* incAM = 15*K */
   shl $4, incAm                /* incAm = 16*15*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 * pB: 4 (add const) + 4 = 8 bytes
 * FMAC: 6*#
 * pA: 3 (add reg)
 * sz = 11 + 6*14 = 11 + 84 = 95  -> 6.79 bytes/FMAC
 * sz15 = 11 + 6*15 = 101  -> 6.733 bytes/FMAC
 * 
 */
/*
 *       First iteration peeled to handle init of rC
 */
         vmovddup -128(pB), rB0  
         vmulpd -128(pA), rB0, rC00
         vmulpd -112(pA), rB0, rC01
         vmulpd -96(pA), rB0, rC02
         vmulpd -80(pA), rB0, rC03
         vmulpd -64(pA), rB0, rC04
         vmulpd -48(pA), rB0, rC05
         vmulpd -32(pA), rB0, rC06
         vmulpd -16(pA), rB0, rC07
         vmulpd (pA), rB0, rC08
         vmulpd 16(pA), rB0, rC09
         vmulpd 32(pA), rB0, rC10
         vmulpd 48(pA), rB0, rC11
         vmulpd 64(pA), rB0, rC12
         vmulpd 80(pA), rB0, rC13
         vmulpd 96(pA), rB0, rC14

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 112 0 +
@iexp bo -120 0 +
@iexp k 0 1 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            vmovddup @(bo)(pB), rB0 
            FMAC @(ao)(pA), rB0, rC00
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC01
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC02
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC03
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
   @iexp bo @(bo) 8 +
   @iif bo > 120
            add r256, pB
         @iexp bo -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC04
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC05
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC06
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC07
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC08
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC09
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC10
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC11
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC12
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC13
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
            FMAC @(ao)(pA), rB0, rC14
   @iexp ao @(ao) 16 +
   @iif ao > 112
            add r256, pA
         @iexp ao -128 0 +
   @endiif
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC01
         #endif
         movapd rC01, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC02
         #endif
         movapd rC02, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC03
         #endif
         movapd rC03, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC04
         #endif
         movapd rC04, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC05
         #endif
         movapd rC05, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC06
         #endif
         movapd rC06, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC07
         #endif
         movapd rC07, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC08
         #endif
         movapd rC08, (pC)
         #ifndef BETA0
            VCOP 16(pC), rC09
         #endif
         movapd rC09, 16(pC)
         #ifndef BETA0
            VCOP 32(pC), rC10
         #endif
         movapd rC10, 32(pC)
         #ifndef BETA0
            VCOP 48(pC), rC11
         #endif
         movapd rC11, 48(pC)
         #ifndef BETA0
            VCOP 64(pC), rC12
         #endif
         movapd rC12, 64(pC)
         #ifndef BETA0
            VCOP 80(pC), rC13
         #endif
         movapd rC13, 80(pC)
         #ifndef BETA0
            VCOP 96(pC), rC14
         #endif
         movapd rC14, 96(pC)
         add $240, pC
         mov pA0, pA
         add $KB*8-(((KB*8)/256)*256), pB
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm6x4x256_fma3.S_new
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %xmm0
#define rA0     %xmm1
#define rA1     %xmm2
#define rA2     %xmm3
#define rC00    %xmm4
#define rC10    %xmm5
#define rC20    %xmm6
#define rC01    %xmm7
#define rC11    %xmm8
#define rC21    %xmm9
#define rC02    %xmm10
#define rC12    %xmm11
#define rC22    %xmm12
#define rC03    %xmm13
#define rC13    %xmm14
#define rC23    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r32     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq 24(%rsp), pfB
   mov KK, KK0
   sub $-128, pC
   mov $32, r32
   mov pA, pA0
   mov pB, pB0
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */

         ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vmovddup (pB), rB0          /* 4  04 */
         movaps (pA), rA0            /* 3  07 */
         vmulpd rA0, rB0, rC00       /* 4  11 */
         .byte 0x3e                  /* 1  12 */
         movaps 16(pA), rA1          /* 4  16 */

         vmulpd rA1, rB0, rC10       /* 4  04 */
         movaps 32(pA), rA2          /* 4  08 */
         add    $8, pB               /* 4  12 */
         vmulpd rA2, rB0, rC20       /* 4  16 */

         vmovddup (pB), rB0          /* 4  04 */
         vmulpd rA0, rB0, rC01       /* 4  08 */
         add    $8, pB               /* 4  12 */
         vmulpd rA1, rB0, rC11       /* 4  16 */

         vmulpd rA2, rB0, rC21       /* 4  04 */
         vmovddup (pB), rB0          /* 4  08 */
         vmulpd rA0, rB0, rC02       /* 4  12 */
         add $64, pA                 /* 4  16 */

         vmulpd rA1, rB0, rC12       /* 4  04 */
         add    $8, pB               /* 4  08 */
         vmulpd rA2, rB0, rC22       /* 4  12 */
         vmovddup (pB), rB0          /* 4  16 */

         vmulpd rA0, rB0, rC03       /* 4  04 */
            movaps -16(pA), rA0      /* 4  08 */
         add    $8, pB               /* 4  12 */
         prefetchw -128(pC)          /* 4  16 */

         vmulpd rA1, rB0, rC13       /* 4  04 */
         prefetchw -64(pC)           /* 4  08 */
         vmulpd rA2, rB0, rC23       /* 4  12 */
         .byte 0x3e
         prefetchw (pC)              /* 3  16 */

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            vmovddup (pB), rB0          /* 4  04 */
            add $8, pB                  /* 4  08 */
            FMAC rA0, rB0, rC00         /* 5  13 */
            movaps (pA), rA1            /* 3  16 */

            movaps 16(pA), rA2          /* 4  04 */
            FMAC rA1, rB0, rC10         /* 5  09 */
            FMAC rA2, rB0, rC20         /* 5  14 */
            .byte 0x66                  /* 1  15 */
            nop                         /* 1  16 */

            add r32, pA                 /* 3  03 */
            vmovddup (pB), rB0          /* 4  07 */
            FMAC rA0, rB0, rC01         /* 5  12 */
            add $8, pB                  /* 4  16 */

            FMAC rA1, rB0, rC11         /* 5  05 */
            FMAC rA2, rB0, rC21         /* 5  10 */
            vmovddup (pB), rB0          /* 4  14 */
            .byte 0x66                  /* 1  15 */
            nop                         /* 1  16 */

            FMAC rA0, rB0, rC02         /* 5  05 */
            FMAC rA1, rB0, rC12         /* 5  10 */
            nop                         /* 1  11 */
            FMAC rA2, rB0, rC22         /* 5  16 */

            vmovddup 8(pB), rB0         /* 5  05 */
            add $16, pB                 /* 4  09 */
            FMAC rA0, rB0, rC03         /* 5  14 */
            .byte 0x66                  /* 1  15 */
            nop                         /* 1  16 */

               movaps (pA), rA0         /* 3  03 */
               add $16, pA              /* 4  07 */
            prefetcht0 112(pB)          /* 4  11 */
            FMAC rA1, rB0, rC13         /* 5  16 */

            prefetcht0 (pA,KK0)         /* 4  04 */
            FMAC rA2, rB0, rC23         /* 5  09 */
            .byte 0x3e                  /* 1  10 */
            prefetcht0 64(pA)           /* 4  14 */
            .byte 0x66                  /* 1  15 */
            nop                         /* 1  16 */
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         #ifndef BETA0
            VCOP 16(pC), rC03
         #endif
         movapd rC03, 16(pC)
         #ifndef BETA0
            VCOP 32(pC), rC13
         #endif
         movapd rC13, 32(pC)
         #ifndef BETA0
            VCOP 48(pC), rC23
         #endif
         movapd rC23, 48(pC)
         mov pA0, pA
         add $192, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm12x4x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %ymm0
#define rA0     %ymm1
#define rA1     %ymm2
#define rA2     %ymm3
#define rC00    %ymm4
#define rC10    %ymm5
#define rC20    %ymm6
#define rC01    %ymm7
#define rC11    %ymm8
#define rC21    %ymm9
#define rC02    %ymm10
#define rC12    %ymm11
#define rC22    %ymm12
#define rC03    %ymm13
#define rC13    %ymm14
#define rC23    %ymm15
#define rC00x   %xmm4
#define rC10x   %xmm5
#define rC20x   %xmm6
#define rC01x   %xmm7
#define rC11x   %xmm8
#define rC21x   %xmm9
#define rC02x   %xmm10
#define rC12x   %xmm11
#define rC22x   %xmm12
#define rC03x   %xmm13
#define rC13x   %xmm14
#define rC23x   %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r192    %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP vsubpd
#else
   #define VCOP vaddpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq 24(%rsp), pfB
   mov KK, KK0
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   sub $-128, pfA
   sub $-128, pfB
   mov $192, r192
   mov pA, pA0
   mov pB, pB0
   mov $12*3*8, incPF  /* incPF = MU*NU*sizeof = 288 */
/*
 * incAm = 12*sizeof*K = 12*8*K = 3*4*8*K = 3*K*32
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $5, incAm           /* incAm = 32*3*K */
   vxorps rA2, rA2, rA2

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vbroadcastsd      -128(pB), rB0
         vmovaps -128(pA), rA0
         vmulpd rA0, rB0, rC00
         vmovaps -96(pA), rA1
         vmulpd rA1, rB0, rC10
         vmovaps -64(pA), rA2
         vmulpd rA2, rB0, rC20

         vbroadcastsd      -120(pB), rB0
         vmulpd rA0, rB0, rC01
            prefA(-128(pfA))
            prefA((pfA))
         vmulpd rA1, rB0, rC11
         vmulpd rA2, rB0, rC21

         vbroadcastsd      -112(pB), rB0
         vmulpd rA0, rB0, rC02
         vmulpd rA1, rB0, rC12
         vmulpd rA2, rB0, rC22

         vbroadcastsd      -104(pB), rB0
         vmulpd rA0, rB0, rC03
            vmovapd -32(pA), rA0
            prefC(-128(pC))
         vmulpd rA1, rB0, rC13
            vmovapd (pA), rA1
            prefC((pC))
         vmulpd rA2, rB0, rC23

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 32 0 +
@iexp bo -96 0 +
@iexp k 0 1 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC00
            vmovaps @(ao)(pA), rA2
   @iexp ao @(ao) 32 + 
            FMAC rA1, rB0, rC10
            FMAC rA2, rB0, rC20

            vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC01
            FMAC rA1, rB0, rC11
            FMAC rA2, rB0, rC21

            vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC02
            FMAC rA1, rB0, rC12
            FMAC rA2, rB0, rC22

            vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 + 
            FMAC rA0, rB0, rC03
            #if KB > @(k)
               vmovaps @(ao)(pA), rA0
   @iexp ao @(ao) 32 + 
            #endif
            FMAC rA1, rB0, rC13
            #if KB > @(k)
               vmovaps @(ao)(pA), rA1
   @iexp ao @(ao) 32 + 
            #endif
            FMAC rA2, rB0, rC23
         #endif
@endiwhile

         add incPF, pfA
         #ifdef ATL_ARCH_AMDDRIVER
            #define USESTORE16 1
         #else
            #define USESTORE16 0
         #endif
         #ifndef BETA0
            VCOP -128(pC), rC00, rC00
         #endif
         #if USESTORE16
            movaps rC00x, -128(pC)
            vextractf128 $1, rC00, -112(pC)
         #else
            vmovapd rC00, -128(pC)
         #endif
         #ifndef BETA0
            VCOP -96(pC), rC10, rC10
         #endif
         #if USESTORE16
            movaps rC10x, -96(pC)
            vextractf128 $1, rC10, -80(pC)
         #else
            vmovapd rC10, -96(pC)
         #endif
         #ifndef BETA0
            VCOP -64(pC), rC20, rC20
         #endif
         #if USESTORE16
            movaps rC20x, -64(pC)
            vextractf128 $1, rC20, -48(pC)
         #else
            vmovapd rC20, -64(pC)
         #endif
         #ifndef BETA0
            VCOP -32(pC), rC01, rC01
         #endif
         #if USESTORE16
            movaps rC01x, -32(pC)
            vextractf128 $1, rC01, -16(pC)
         #else
            vmovapd rC01, -32(pC)
         #endif
         #ifndef BETA0
            VCOP (pC), rC11, rC11
         #endif
         #if USESTORE16
            movaps rC11x, (pC)
            vextractf128 $1, rC11, 16(pC)
         #else
            vmovapd rC11, (pC)
         #endif
         #ifndef BETA0
            VCOP 32(pC), rC21, rC21
         #endif
         #if USESTORE16
            movaps rC21x, 32(pC)
            vextractf128 $1, rC21, 48(pC)
         #else
            vmovapd rC21, 32(pC)
         #endif
         #ifndef BETA0
            VCOP 64(pC), rC02, rC02
         #endif
         #if USESTORE16
            movaps rC02x, 64(pC)
            vextractf128 $1, rC02, 80(pC)
         #else
            vmovapd rC02, 64(pC)
         #endif
         #ifndef BETA0
            VCOP 96(pC), rC12, rC12
         #endif
         #if USESTORE16
            movaps rC12x, 96(pC)
            vextractf128 $1, rC12, 112(pC)
         #else
            vmovapd rC12, 96(pC)
         #endif
         #ifndef BETA0
            VCOP 128(pC), rC22, rC22
         #endif
         #if USESTORE16
            movaps rC22x, 128(pC)
            vextractf128 $1, rC22, 144(pC)
         #else
            vmovapd rC22, 128(pC)
         #endif
         #ifndef BETA0
            VCOP 160(pC), rC03, rC03
         #endif
         #if USESTORE16
            movaps rC03x, 160(pC)
            vextractf128 $1, rC03, 176(pC)
         #else
            vmovapd rC03, 160(pC)
         #endif
         #ifndef BETA0
            VCOP 192(pC), rC13, rC13
         #endif
         #if USESTORE16
            movaps rC13x, 192(pC)
            vextractf128 $1, rC13, 208(pC)
         #else
            vmovapd rC13, 192(pC)
         #endif
         #ifndef BETA0
            VCOP 224(pC), rC23, rC23
         #endif
         #if USESTORE16
            movaps rC23x, 224(pC)
            vextractf128 $1, rC23, 240(pC)
         #else
            vmovapd rC23, 224(pC)
         #endif
         add $KB*4*8, pB     /* pB += KB*NU*sizeof */
         add $384, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_sammm24x4x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %ymm0
#define rA0     %ymm1
#define rA1     %ymm2
#define rA2     %ymm3
#define rC00    %ymm4
#define rC10    %ymm5
#define rC20    %ymm6
#define rC01    %ymm7
#define rC11    %ymm8
#define rC21    %ymm9
#define rC02    %ymm10
#define rC12    %ymm11
#define rC22    %ymm12
#define rC03    %ymm13
#define rC13    %ymm14
#define rC23    %ymm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r192    %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231ps   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP vsubps
#else
   #define VCOP vaddps
#endif
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfA, pB
   CMOVE pfB, pfA
   CMOVEq 24(%rsp), pfB
   mov KK, KK0
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   mov $192, r192
   mov pA, pA0
   mov pB, pB0
/*
 * incAm = 24*sizeof*K = 24*4*K = 3*4*8*K = 3*K*32
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $5, incAm           /* incAm = 32*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vbroadcastss      -128(pB), rB0
         vmovaps -128(pA), rA0
         vmulps rA0, rB0, rC00
         vmovaps -96(pA), rA1
         vmulps rA1, rB0, rC10
         vmovaps -64(pA), rA2
         vmulps rA2, rB0, rC20

         vbroadcastss      -124(pB), rB0
         vmulps rA0, rB0, rC01
            prefC(-128(pC))
         vmulps rA1, rB0, rC11
         vmulps rA2, rB0, rC21

         vbroadcastss      -120(pB), rB0
         vmulps rA0, rB0, rC02
         vmulps rA1, rB0, rC12
            prefC((pC))
         vmulps rA2, rB0, rC22

         vbroadcastss      -116(pB), rB0
         vmulps rA0, rB0, rC03
            vmovaps -32(pA), rA0
         vmulps rA1, rB0, rC13
            vmovaps (pA), rA1
         vmulps rA2, rB0, rC23

/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao 32 0 +
@iexp bo -112 0 +
@iexp k 0 1 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            vbroadcastss @(bo)(pB), rB0
   @iexp bo @(bo) 4 + 
            FMAC rA0, rB0, rC00
            vmovaps @(ao)(pA), rA2
   @iexp ao @(ao) 32 + 
            FMAC rA1, rB0, rC10
            FMAC rA2, rB0, rC20

            vbroadcastss @(bo)(pB), rB0
   @iexp bo @(bo) 4 + 
            FMAC rA0, rB0, rC01
            FMAC rA1, rB0, rC11
            FMAC rA2, rB0, rC21

            vbroadcastss @(bo)(pB), rB0
   @iexp bo @(bo) 4 + 
            FMAC rA0, rB0, rC02
            FMAC rA1, rB0, rC12
            FMAC rA2, rB0, rC22

            vbroadcastss @(bo)(pB), rB0
   @iexp bo @(bo) 4 + 
            FMAC rA0, rB0, rC03
               vmovaps @(ao)(pA), rA0
   @iexp ao @(ao) 32 + 
            FMAC rA1, rB0, rC13
               vmovaps @(ao)(pA), rA1
   @iexp ao @(ao) 32 + 
            FMAC rA2, rB0, rC23
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00, rC00
         #endif
         vmovaps rC00, -128(pC)
         #ifndef BETA0
            VCOP -96(pC), rC10, rC10
         #endif
         vmovaps rC10, -96(pC)
         #ifndef BETA0
            VCOP -64(pC), rC20, rC20
         #endif
         vmovaps rC20, -64(pC)
         #ifndef BETA0
            VCOP -32(pC), rC01, rC01
         #endif
         vmovaps rC01, -32(pC)
         #ifndef BETA0
            VCOP (pC), rC11, rC11
         #endif
         vmovaps rC11, (pC)
         #ifndef BETA0
            VCOP 32(pC), rC21, rC21
         #endif
         vmovaps rC21, 32(pC)
         #ifndef BETA0
            VCOP 64(pC), rC02, rC02
         #endif
         vmovaps rC02, 64(pC)
         #ifndef BETA0
            VCOP 96(pC), rC12, rC12
         #endif
         vmovaps rC12, 96(pC)
         #ifndef BETA0
            VCOP 128(pC), rC22, rC22
         #endif
         vmovaps rC22, 128(pC)
         #ifndef BETA0
            VCOP 160(pC), rC03, rC03
         #endif
         vmovaps rC03, 160(pC)
         #ifndef BETA0
            VCOP 192(pC), rC13, rC13
         #endif
         vmovaps rC13, 192(pC)
         #ifndef BETA0
            VCOP 224(pC), rC23, rC23
         #endif
         vmovaps rC23, 224(pC)
         add $KB*4*4, pB     /* pB += KB*NU*sizeof */
         add $384, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm4x4x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %xmm0
#define rB1     %xmm1
#define rb0     %xmm2
#define rb1     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define ra0     %xmm6
#define ra1     %xmm7
#define rC00    %xmm8
#define rC10    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC02    %xmm12
#define rC12    %xmm13
#define rC03    %xmm14
#define rC13    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht0 m_
#define prefB(m_) prefetcht0 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pfB
   sub $-128, pfA
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $4*4*8, incPF
/*
 * incAm = 4*sizeof*K = 4*8*K = 32*K
 */
   mov KK, incAm
   shl $5, incAm           /* incAm = 32*K */
   vxorps rA0, rA0, rA0
   vmovaps rA0, ra0
   vxorps rA1, rA1, rA1
   vmovaps rA1, ra1
   vxorps rB0, rB0, rB0
   vmovaps rB0, rb0
   vxorps rB1, rB1, rB1
   vmovaps rB1, rb1

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vxorps rC00, rC00, rC00
         vmovaps rC00, rC10
         vxorps rC01, rC01, rC01
         vmovaps rC00, rC11
         vxorps rC02, rC02, rC02
         vmovaps rC00, rC12
         vxorps rC03, rC03, rC03
         vmovaps rC00, rC13

         movaps -128(pA), rA0
         movaps -128(pB), rb0
         movaps -112(pA), rA1
         movaps -112(pB), rb1
         #if KB > 1
            movaps -96(pA), ra0
            movaps -80(pA), ra1
         #endif
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
@iexp ao -64 0 +
@iexp bo -96 0 +
@iexp k 0 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
   @iexp kn @(k) 1 +
            vunpcklpd rb0, rb0, rB0
            FMAC rA0, rB0, rC00
            vunpckhpd rb0, rb0, rB1
            FMAC rA1, rB0, rC10

            #if KB > @(k)
               movaps @(bo)(pB), rb0
   @iexp bo @(bo) 16 +
            #endif
            FMAC rA0, rB1, rC01
            vunpcklpd rb1, rb1, rB0
            FMAC rA1, rB1, rC11

             vunpckhpd rb1, rb1, rB1
            FMAC rA0, rB0, rC02
            #if KB > @(k)
               movaps @(bo)(pB), rb1
   @iexp bo @(bo) 16 +
            #endif
            FMAC rA1, rB0, rC12

            FMAC rA0, rB1, rC03
            #if KB > @(kn)
               movaps @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #endif
            FMAC rA1, rB1, rC13
            #if KB > @(kn)
               movaps @(ao)(pA), rA1
   @iexp ao @(ao) 16 +
            #endif
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
   @iexp kn @(k) 1 +
            vunpcklpd rb0, rb0, rB0
            FMAC ra0, rB0, rC00
            vunpckhpd rb0, rb0, rB1
            FMAC ra1, rB0, rC10

            #if KB > @(k)
               movaps @(bo)(pB), rb0
   @iexp bo @(bo) 16 +
            #endif
            FMAC ra0, rB1, rC01
            vunpcklpd rb1, rb1, rB0
            FMAC ra1, rB1, rC11

             vunpckhpd rb1, rb1, rB1
            FMAC ra0, rB0, rC02
            #if KB > @(k)
               movaps @(bo)(pB), rb1
   @iexp bo @(bo) 16 +
            #endif
            FMAC ra1, rB0, rC12

            FMAC ra0, rB1, rC03
            #if KB > @(kn)
               movaps @(ao)(pA), ra0
   @iexp ao @(ao) 16 +
            #endif
            FMAC ra1, rB1, rC13
            #if KB > @(kn)
               movaps @(ao)(pA), ra1
   @iexp ao @(ao) 16 +
            #endif
         #endif
@endiwhile

         #ifndef BETA0
            VCOP (pC), rC00
         #endif
         movapd rC00, (pC)
         #ifndef BETA0
            VCOP 16(pC), rC10
         #endif
         movapd rC10, 16(pC)
         #ifndef BETA0
            VCOP 32(pC), rC01
         #endif
         movapd rC01, 32(pC)
         #ifndef BETA0
            VCOP 48(pC), rC11
         #endif
         movapd rC11, 48(pC)
         #ifndef BETA0
            VCOP 64(pC), rC02
         #endif
         movapd rC02, 64(pC)
         #ifndef BETA0
            VCOP 80(pC), rC12
         #endif
         movapd rC12, 80(pC)
         #ifndef BETA0
            VCOP 96(pC), rC03
         #endif
         movapd rC03, 96(pC)
         #ifndef BETA0
            VCOP 112(pC), rC13
         #endif
         movapd rC13, 112(pC)

         add $KB*4*8, pB
         sub $-128, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm8x4x256_fma3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rB0     %ymm0
#define rB1     %ymm1
#define rB2     %ymm2
#define rB3     %ymm3
#define rA0     %ymm4
#define rA1     %ymm5
#define ra0     %ymm6
#define ra1     %ymm7
#define rC00    %ymm8
#define rC10    %ymm9
#define rC01    %ymm10
#define rC11    %ymm11
#define rC02    %ymm12
#define rC12    %ymm13
#define rC03    %ymm14
#define rC13    %ymm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht2 m_
#define prefB(m_) prefetcht2 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP vsubpd
#else
   #define VCOP vaddpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pfB
   sub $-128, pfA
   sub $-128, pA
   sub $-128, pB
   sub $-128, pC
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $8*4*8, incPF
/*
 * incAm = 8*sizeof*K = 8*8*K = 64*K
 */
   mov KK, incAm
   shl $6, incAm           /* incAm = 32*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vxorps rC00, rC00, rC00
         vmovaps rC00, rC10
         vxorps rC01, rC01, rC01
         vmovaps rC00, rC11
         vxorps rC02, rC02, rC02
         vmovaps rC00, rC12
         vxorps rC03, rC03, rC03
         vmovaps rC00, rC13

         vmovaps -128(pA), rA0

         #if 0
         vmovaps -128(pB), rb0          /* rb0 = {b3, b2, b1, b0} */
         vunpcklpd rb0, rb0, rb1        /* rb1 = {b2, b2, b0, b0} */
         vperf2f128 $0, rb1, rb1, rB0   /* rB0 = {b0, b0, b0, b0} */
         vunpckhpd rb0, rb0, rb0        /* rb0 = {b3, b3  b1, b1} */
         vperf2f128 $0, rb0, rb0, rB1   /* rB1 = {b1, b1, b1, b1} */

         vperf2f128 0x11, rb1, rb1, rB0 /* rB0 = {b2, b2, b2, b2} */
         vperf2f128 0x11, rb0, rb0, rB1 /* rB1 = {b3, b3, b3, b3} */
         #else
         vbroadcastsd -128(pB), rB0
         vbroadcastsd -120(pB), rB1
         vbroadcastsd -112(pB), rB2
         #endif
         
         vmovaps -96(pA), rA1
         #if KB > 1
            vmovaps -64(pA), ra0
            vmovaps -32(pA), ra1
         #endif
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         ALIGN16
@iexp ao 0 0 +
@iexp bo -104 0 +
@iexp k 0 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
   @iexp kn @(k) 1 +
            vbroadcastsd @(bo)(pB), rB3
   @iexp bo @(bo) 8 +
            FMAC rA0, rB0, rC00
            FMAC rA1, rB0, rC10

            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 +
            #endif
            FMAC rA0, rB1, rC01
            FMAC rA1, rB1, rC11

            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB1
   @iexp bo @(bo) 8 +
            #endif
            FMAC rA0, rB2, rC02
            FMAC rA1, rB2, rC12
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB2
   @iexp bo @(bo) 8 +
            #endif

            FMAC rA0, rB3, rC03
            #if KB > @(kn)
               vmovaps @(ao)(pA), rA0
   @iexp ao @(ao) 32 +
            #endif
            FMAC rA1, rB3, rC13
            #if KB > @(kn)
               vmovaps @(ao)(pA), rA1
   @iexp ao @(ao) 32 +
            #endif
         #endif
         #if KB > @(k)
   @iexp k @(k) 1 +
   @iexp kn @(k) 1 +
            vbroadcastsd @(bo)(pB), rB3
   @iexp bo @(bo) 8 +
            FMAC ra0, rB0, rC00
            FMAC ra1, rB0, rC10

            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB0
   @iexp bo @(bo) 8 +
            #endif
            FMAC ra0, rB1, rC01
            FMAC ra1, rB1, rC11

            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB1
   @iexp bo @(bo) 8 +
            #endif
            FMAC ra0, rB2, rC02
            FMAC ra1, rB2, rC12
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB2
   @iexp bo @(bo) 8 +
            #endif

            FMAC ra0, rB3, rC03
            #if KB > @(kn)
               vmovaps @(ao)(pA), ra0
   @iexp ao @(ao) 32 +
            #endif
            FMAC ra1, rB3, rC13
            #if KB > @(kn)
               vmovaps @(ao)(pA), ra1
   @iexp ao @(ao) 32 +
            #endif
         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00, rC00
         #endif
         vmovapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -96(pC), rC10, rC10
         #endif
         vmovapd rC10, -96(pC)
         #ifndef BETA0
            VCOP -64(pC), rC01, rC01
         #endif
         vmovapd rC01, -64(pC)
         #ifndef BETA0
            VCOP -32(pC), rC11, rC11
         #endif
         vmovapd rC11, -32(pC)
         #ifndef BETA0
            VCOP (pC), rC02, rC02
         #endif
         vmovapd rC02, (pC)
         #ifndef BETA0
            VCOP 32(pC), rC12, rC12
         #endif
         vmovapd rC12, 32(pC)
         #ifndef BETA0
            VCOP 64(pC), rC03, rC03
         #endif
         vmovapd rC03, 64(pC)
         #ifndef BETA0
            VCOP 96(pC), rC13, rC13
         #endif
         vmovapd rC13, 96(pC)

         add $KB*4*8, pB
         add $256, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_sammm16x2x256_sse2.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rA0     %xmm3
#define rA1     %xmm4
#define rA2     %xmm5
#define rA3     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC20    %xmm9
#define rC30    %xmm10
#define rC01    %xmm11
#define rC11    %xmm12
#define rC21    %xmm13
#define rC31    %xmm14
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht2 m_
#define prefB(m_) prefetcht2 m_
#define prefC(m_) prefetcht0 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subps
#else
   #define VCOP addps
#endif
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfB      /* pfB = pAn */
   movq 16(%rsp), pfA     /* pf = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pfB
   sub $-128, pfA
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $8*4*8, incPF
/*
 * incAm = MU*K*sizeof = 16*K*4 = 64*K
 */
   mov KK, incAm
   shl $6, incAm           /* incAm = 64*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         xorps rC00, rC00
         movaps rC00, rC10
         xorps rC20, rC20
         movaps rC00, rC30
         xorps rC01, rC01
         movaps rC00, rC11
         xorps rC21, rC21
         movaps rC00, rC31

         movddup -128(pB), rB0   /* b1 b0 b1 b0 */
         movaps -128(pA), rA0
         movaps -112(pA), rA1
         movaps -96(pA), rA2
         movaps -80(pA), rA3
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         ALIGN16
@iexp ao -64 0 +
@iexp bo -120 0 +
@iexp k 0 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movshdup rB0, rB1
            movsldup rB0, rB0

            movaps rB0, rm0
            mulps rA0, rm0
            addps rm0, rC00

            movaps rB0, rm0
            mulps rA1, rm0
            addps rm0, rC10

            movaps rB0, rm0
            mulps rA2, rm0
            addps rm0, rC20

            mulps rA3, rB0
            addps rB0, rC30
            #if KB > @(k)
               movddup @(bo)(pB), rB0   /* b1 b0 b1 b0 */
               @iexp bo @(bo) 8 +
            #endif

            mulps rB1, rA0
            addps rA0, rC01
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #endif

            mulps rB1, rA1
            addps rA1, rC11
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #endif

            mulps rB1, rA2
            addps rA2, rC21
            #if KB > @(k)
               movaps @(ao)(pA), rA2
               @iexp ao @(ao) 16 +
            #endif

            mulps rB1, rA3
            addps rA3, rC31
            #if KB > @(k)
               movaps @(ao)(pA), rA3
               @iexp ao @(ao) 16 +
            #endif
         #endif
@endiwhile

         #ifndef BETA0
            VCOP (pC), rC00
         #endif
         movaps rC00, (pC)
         #ifndef BETA0
            VCOP 16(pC), rC10
         #endif
         movaps rC10, 16(pC)
         #ifndef BETA0
            VCOP 32(pC), rC20
         #endif
         movaps rC20, 32(pC)
         #ifndef BETA0
            VCOP 48(pC), rC30
         #endif
         movaps rC30, 48(pC)
         #ifndef BETA0
            VCOP 64(pC), rC01
         #endif
         movaps rC01, 64(pC)
         #ifndef BETA0
            VCOP 80(pC), rC11
         #endif
         movaps rC11, 80(pC)
         #ifndef BETA0
            VCOP 96(pC), rC21
         #endif
         movaps rC21, 96(pC)
         #ifndef BETA0
            VCOP 112(pC), rC31
         #endif
         movaps rC31, 112(pC)

         add $KB*2*4, pB
         sub $-128, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm6x3x256_sse3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define rA2     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) prefetcht2 m_
#define prefB(m_) prefetcht2 m_
//#define prefC(m_) prefetcht0 m_
#define prefC(m_) prefetchw m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfA       /* pfA = pAn */
   movq 16(%rsp), pfB      /* pfB = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pC
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         movddup -128(pB), rC00
         movapd -128(pA), rA0
         movapd rC00, rC10

         mulpd rA0, rC00
         movapd rC10, rC20
         movapd -112(pA), rA1

         mulpd rA1, rC10
         movddup -120(pB), rC01
         movapd -96(pA), rA2

         mulpd rA2, rC20
         movapd rC01, rC11
         movddup -112(pB), rC02

         mulpd rA0, rC01
         movapd rC11, rC21
         movddup -104(pB), rB0

         movapd rC02, rC12
         mulpd rA1, rC11
         movddup -96(pB), rB1

         mulpd rA2, rC21
         movapd rC12, rC22
         prefC(-128(pC))

         mulpd rA0, rC02
         movapd -80(pA), rA0
         prefC((pC))

         mulpd rA1, rC12
         movapd -64(pA), rA1
         prefB(-128(pfB))

         mulpd rA2, rC22
         prefB((pfB))
         add incPF, pfB

@iexp ao -48 0 +
@iexp bo -88 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movapd rA0, rm0
            mulpd rB0, rm0
            addpd rm0, rC00
            movapd @(ao)(pA), rA2
            @iexp ao @(ao) 16 +

            movapd rA1, rm0
            mulpd rB0, rm0
            addpd rm0, rC10

            movddup @(bo)(pB), rB2
            @iexp bo @(bo) 8 +
            mulpd rA2, rB0
            addpd rB0, rC20

            movapd rA0, rm0
            mulpd rB1, rm0
            addpd rm0, rC01

            movapd rA1, rm0
            mulpd rB1, rm0
            addpd rm0, rC11

            mulpd rA2, rB1
            addpd rB1, rC21
            #if KB > @(k)
               movddup @(bo)(pB), rB0
               @iexp bo @(bo) 8 +
            #endif

            mulpd rB2, rA0
            addpd rA0, rC02
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #endif

            mulpd rB2, rA1
            addpd rA1, rC12
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #endif

            mulpd rB2, rA2
            addpd rA2, rC22
            #if KB > @(k)
               movddup @(bo)(pB), rB1
               @iexp bo @(bo) 8 +
            #endif

         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         add $KB*3*8, pB
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_amm6x1x256_x87.S
#include "atlas_asm.h"

#ifdef ATL_GAS_x8632
   #define nmu  %edi
   #define nnu  %esi
   #define K    %ecx
   #define pA   %eax
   #define pB   %ebx
   #define pC   %ebp
   #define r256 %edx
   #define pB0  FSIZE+20(%esp)
   #define nnu0 20(%esp)
   #define K0   24(%esp)
   #define FSIZE 8*4
#elif defined(ATL_GAS_x8664)
   #define nmu %rdi
   #define nnu %rsi
   #define K   %rdx
   #define pA  %rcx
   #define pB  %rax
   #define pC  %r9
   #define nn0 %r8
   #define pB0 %r10
   #define K0  %r11
   #define nnu0 %r12
   #define r256 %r13
#else
   #error "This file requires x86 assembly!"
#endif
#ifdef SREAL
   #define fmull fmuls
   #define fstpl fstps
   #define faddl fadds
   #define fsubl fsubs
   #define fldl flds
   #define SZ 4
#else
   #define SZ 8
#endif
/*
                  4/rdi    8/rsi 12/rdx    16/ rcx     20/ r8    24/ r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                28/8(%rsp) 32/16(%rsp)  36/24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */

.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
#ifdef ATL_GAS_x8632
   sub $FSIZE, %esp
   movl %ebp, (%esp)
   movl %ebx, 4(%esp)
   movl %esi, 8(%esp)
   movl %edi, 16(%esp)

   movl  4+FSIZE(%esp), nmu
   movl  8+FSIZE(%esp), nnu
   movl 12+FSIZE(%esp), K  
   movl 16+FSIZE(%esp), pA
   movl 20+FSIZE(%esp), pB
   movl 24+FSIZE(%esp), pC
   #define movq movl
#else
   mov %r8, pB
   movq %r12, -8(%rsp)
#endif
   mov $256, r256
/*
 * Compute K = -6*K*sizeof = -6*8*K = 3*16*K = 48*K
 */
   lea (K, K, 2), K   /* K = 3*K */
   #ifdef SREAL
      shl $3, K          /* K = 24K */
   #else
      shl $4, K          /* K = 48K */
   #endif
   neg K              /* K = -48*K */
   movq K, K0
   movq nnu, nnu0 
   sub $-128, pA
   sub $-128, pB
   movq pB, pB0

   MLOOP:
      sub K, pA                         /* pA += 6*K*sizeof */
      movq nnu0, nnu
      movq pB0, pB
      NLOOP:
         fldz                           /* ST={0} */
         fldz                           /* ST={0,0} */
         fldz                           /* ST={0,0,0} */
         fldz                           /* ST={0,0,0,0} */
         fldz                           /* ST={0,0,0,0,0} */
         fldz                           /* ST={0,0,0,0,0,0} */
         movq K0, K
/*
 *       Fully unrolled K-loop
 */
@iexp mx 104 0 +
@iexp sz 8 0 +
@iexp ao -128 0 +
@iexp bo -128 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
            @iif bo = 0
            .byte 0x66
            @endiif
            fldl @(bo)(pB)              /* ST={b0,c0,c0,c0,c0,c0,c0}    3  3 */
            @iexp bo @(bo) @(sz)
            @iif bo = 0
            .byte 0x66
            @endiif
            fldl @(ao)(pA)              /* ST={a0,b0,c0,c1,c2,c3,c4,c5} 3  6 */
            fmul  %st(1), %st           /* ST={c0,b0,c0,c1,c2,c3,c4,c5} 2  8 */
            @iexp ao @(ao) @(sz)
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
            faddp  %st, %st(2)          /* ST={b0,c0,c1,c2,c3,c4,c5}    2  2 */
            fldl @(ao)(pA)              /* ST={a1,b0,c0,c1,c2,c3,c4,c5} 3  5 */
            .byte 0x66
            fmul  %st(1), %st           /* ST={c1,b0,c0,c1,c2,c3,c4,c5} 2  7 */
            @iexp ao @(ao) @(sz) +
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
            faddp  %st, %st(3)          /* ST={b0,c0,c1,c2,c3,c4,c5}    2  2 */
            fldl @(ao)(pA)              /* ST={a2,b0,c0,c1,c2,c3,c4,c5} 3  5 */
            .byte 0x66
            fmul  %st(1), %st           /* ST={c2,b0,c0,c1,c2,c3,c4,c5} 2  7 */
            @iexp ao @(ao) @(sz) +
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
            faddp  %st, %st(4)          /* ST={b0,c0,c1,c2,c3,c4,c5}    2  2 */
            fldl @(ao)(pA)              /* ST={a3,b0,c0,c1,c2,c3,c4,c5} 3  5 */
            .byte 0x66
            fmul  %st(1), %st           /* ST={c3,b0,c0,c1,c2,c3,c4,c5} 2  7 */
            @iexp ao @(ao) @(sz) +
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
            faddp  %st, %st(5)          /* ST={b0,c0,c1,c2,c3,c4,c5}    2  2 */
            fldl @(ao)(pA)              /* ST={a4,b0,c0,c1,c2,c3,c4,c5} 3  5 */
            .byte 0x66
            fmul  %st(1), %st           /* ST={c4,b0,c0,c1,c2,c3,c4,c5} 2  7 */
            @iexp ao @(ao) @(sz) +
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
            faddp  %st, %st(6)          /* ST={b0,c0,c1,c2,c3,c4,c5}    2  2 */
            fmull @(ao)(pA)             /* ST={c5,c0,c1,c2,c3,c4,c5}    3  5 */
            faddp  %st, %st(6)          /* ST={c0,c1,c2,c3,c4,c5}       2  7 */
            @iexp ao @(ao) @(sz) +
            @iif ao = @(mx)
               add $240, pA             /* 5  5 */
               add $40, pB              /* 3  8 */
               @iexp ao -128 0 +
               @iexp bo -128 0 + 
            @endiif
         #endif
/*
 *       Write answer back to C
 */
         #ifdef BETA0
            fstpl (pC)                  /* ST={c1,c2,c3,c4,c5} */
            fstpl SZ(pC)                /* ST={c2,c3,c4,c5} */
            fstpl 2*SZ(pC)              /* ST={c3,c4,c5} */
            fstpl 3*SZ(pC)              /* ST={c4,c5} */
            fstpl 4*SZ(pC)              /* ST={c5} */
            fstpl 5*SZ(pC)              /* ST={} */
         #elif defined(BETA1)
            faddl (pC)                  /* ST={c0,c1,c2,c3,c4,c5} */
            fstpl (pC)                  /* ST={c1,c2,c3,c4,c5} */
            faddl SZ(pC)                /* ST={c1,c2,c3,c4,c5} */
            fstpl SZ(pC)                /* ST={c2,c3,c4,c5} */
            faddl 2*SZ(pC)              /* ST={c2,c3,c4,c5} */
            fstpl 2*SZ(pC)              /* ST={c3,c4,c5} */
            faddl 3*SZ(pC)              /* ST={c3,c4,c5} */
            fstpl 3*SZ(pC)              /* ST={c4,c5} */
            faddl 4*SZ(pC)              /* ST={c4,c5} */
            fstpl 4*SZ(pC)              /* ST={c5} */
            faddl 5*SZ(pC)              /* ST={c5} */
            fstpl 5*SZ(pC)              /* ST={} */
         #else
            fsubl (pC)                  /* ST={c0,c1,c2,c3,c4,c5} */
            fstpl (pC)                  /* ST={c1,c2,c3,c4,c5} */
            fsubl SZ(pC)                /* ST={c1,c2,c3,c4,c5} */
            fstpl SZ(pC)                /* ST={c2,c3,c4,c5} */
            fsubl 2*SZ(pC)              /* ST={c2,c3,c4,c5} */
            fstpl 2*SZ(pC)              /* ST={c3,c4,c5} */
            fsubl 3*SZ(pC)              /* ST={c3,c4,c5} */
            fstpl 3*SZ(pC)              /* ST={c4,c5} */
            fsubl 4*SZ(pC)              /* ST={c4,c5} */
            fstpl 4*SZ(pC)              /* ST={c5} */
            fsubl 5*SZ(pC)              /* ST={c5} */
            fstpl 5*SZ(pC)              /* ST={} */
         #endif
         add $6*SZ, pC
         dec nnu
      jnz NLOOP
      movq K0, K
      dec nmu
   jnz MLOOP

DONE:
#ifdef ATL_GAS_x8632
   movl (%esp), %ebp
   movl 4(%esp), %ebx
   movl 8(%esp), %esi
   movl 16(%esp), %edi
   add $FSIZE, %esp
#else
   movq -8(%rsp), %r12
#endif
   ret
@ROUT ATL_sammm12x3d2x256_sse3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define rA2     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
// #define PFADIST 128
#define PFBDIST 1088
#define prefA(m_) prefetcht2 m_
#define prefB(m_) prefetcht2 m_
//#define prefC(m_) prefetcht0 m_
#define prefC(m_) prefetchw m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subps
#else
   #define VCOP addps
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfA       /* pfA = pAn */
   movq 16(%rsp), pfB      /* pfB = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pC
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 12*sizeof*K = 12*4*K = 16*3*K
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 16*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         movddup -128(pB), rC00
         movaps -128(pA), rA0
         movaps rC00, rC10

         mulps rA0, rC00
         movaps rC10, rC20
         movaps -112(pA), rA1

         mulps rA1, rC10
         movddup -120(pB), rC01
         movaps -96(pA), rA2

         mulps rA2, rC20
         movapd rC01, rC11
         movddup -112(pB), rC02

         mulps rA0, rC01
         movaps rC11, rC21
         movddup -104(pB), rB0

         movaps rC02, rC12
         mulps rA1, rC11
         movddup -96(pB), rB1

         mulps rA2, rC21
         movaps rC12, rC22
         prefC(-128(pC))

         mulps rA0, rC02
         movaps -80(pA), rA0
         prefC((pC))

         mulps rA1, rC12
         movaps -64(pA), rA1
         prefB(-128(pfB))

         mulps rA2, rC22
         prefB((pfB))
         add incPF, pfB

@iexp bp  64 0 +
@iexp ao -48 0 +
@iexp bo -88 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp ap 1 0 +
   @iexp k @(k) 1 +
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00
            movaps @(ao)(pA), rA2
            @iexp ao @(ao) 16 +
            @iif bp > 41
               prefetcht0 @(bo)+PFBDIST(pB)
               @iexp bp 0 0 +
               @iexp ap 0 0 +
            @endiif
            @iif ap ! 0
               prefetcht0 @(bo)(pA,incAm)
            @endiif
            @iexp bp @(bp) 24 +

            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10

            movddup @(bo)(pB), rB2
            @iexp bo @(bo) 8 +
            mulps rA2, rB0
            addps rB0, rC20

            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01

            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11

            mulps rA2, rB1
            addps rB1, rC21
            #if KB > @(k)
               movddup @(bo)(pB), rB0
               @iexp bo @(bo) 8 +
            #endif

            mulps rB2, rA0
            addps rA0, rC02
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #endif

            mulps rB2, rA1
            addps rA1, rC12
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #endif

            mulps rB2, rA2
            addps rA2, rC22
            #if KB > @(k)
               movddup @(bo)(pB), rB1
               @iexp bo @(bo) 8 +
            #endif

         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         add $KB*3*8, pB
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_sammm12x3d4x256_sse3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define rA2     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
// #define PFADIST 128
#define PFBDIST 1088
#define prefA(m_) prefetcht2 m_
#define prefB(m_) prefetcht2 m_
//#define prefC(m_) prefetcht0 m_
#define prefC(m_) prefetchw m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subps
#else
   #define VCOP addps
#endif
#define movapd movaps
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfA       /* pfA = pAn */
   movq 16(%rsp), pfB      /* pfB = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pC
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 12*sizeof*K = 12*4*K = 16*3*K
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 16*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         movaps -128(pB), rC00
         movaps -128(pA), rA0
         movaps rC00, rC10

         mulps rA0, rC00
         movaps rC10, rC20
         movaps -112(pA), rA1

         mulps rA1, rC10
         movaps -112(pB), rC01
         movaps -96(pA), rA2

         mulps rA2, rC20
         movapd rC01, rC11
         movaps -96(pB), rC02

         mulps rA0, rC01
         movaps rC11, rC21
         movaps -80(pB), rB0

         movaps rC02, rC12
         mulps rA1, rC11
         movaps -64(pB), rB1

         mulps rA2, rC21
         movaps rC12, rC22
         prefC(-128(pC))

         mulps rA0, rC02
         movaps -80(pA), rA0
         prefC((pC))

         mulps rA1, rC12
         movaps -64(pA), rA1
         prefB(-128(pfB))

         mulps rA2, rC22
         prefB((pfB))
         add incPF, pfB

@iexp bp  64 0 +
@iexp ao -48 0 +
@iexp bo -48 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp ap 1 0 +
   @iexp k @(k) 1 +
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00
            movaps @(ao)(pA), rA2
            @iexp ao @(ao) 16 +
            @iif bp > 41
               prefetcht0 @(bo)+PFBDIST(pB)
               @iexp bp 0 0 +
               @iexp ap 0 0 +
            @endiif
            @iif ap ! 0
               prefetcht0 @(bo)(pA,incAm)
            @endiif
            @iexp bp @(bp) 24 +

            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10

            movaps @(bo)(pB), rB2
            @iexp bo @(bo) 16 +
            mulps rA2, rB0
            addps rB0, rC20

            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01

            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11

            mulps rA2, rB1
            addps rB1, rC21
            #if KB > @(k)
               movaps @(bo)(pB), rB0
               @iexp bo @(bo) 16 +
            #endif

            mulps rB2, rA0
            addps rA0, rC02
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #endif

            mulps rB2, rA1
            addps rA1, rC12
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #endif

            mulps rB2, rA2
            addps rA2, rC22
            #if KB > @(k)
               movaps @(bo)(pB), rB1
               @iexp bo @(bo) 16 +
            #endif

         #endif
@endiwhile

         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         add $KB*3*16, pB
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_dammm6x3r2x256_sse3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define rA2     %xmm8
#define rC00    %xmm7
#define rC10    %xmm6
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#if KB < 45
   #define prefA(m_)
   #define prefB(m_) prefetcht0 m_
#else
   #if KB > 78
      #define prefA(m_)
   #else
      #define prefA(m_) prefetcht2 m_
   #endif
   #define prefB(m_) prefetcht2 m_
#endif
#ifdef ATL_3DNow
   #define prefC(m_) prefetchw m_
#else
   #define prefC(m_) prefetcht0 m_
#endif
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subpd
#else
   #define VCOP addpd
#endif
/* #define movapd movaps */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   #if KB > 78  /* only pfB is prefetched, so put most imp mat in it */
      #if defined(ATL_MOVEB)
         lea KB*6*8(pB), pfB
      #else
         lea KB*12*8(pA), pfB
      #endif
   #else
      movq 8(%rsp), pfA       /* pfA = pAn */
      movq 16(%rsp), pfB      /* pfB = pBn */
      cmp pfB, pB
      CMOVE pfA, pfB
      CMOVE pA, pfA
   #endif
   mov KK, KK0
   sub $-128, pC
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         movddup -128(pB), rC00
         movapd -128(pA), rA0
         movapd rC00, rC10

         mulpd rA0, rC00
         movapd rC10, rC20
         movapd -112(pA), rA1

         mulpd rA1, rC10
         movddup -120(pB), rC01
         movapd -96(pA), rA2

         mulpd rA2, rC20
         movapd rC01, rC11
         movddup -112(pB), rC02

         mulpd rA0, rC01
         movapd rC11, rC21
         movddup -104(pB), rB0

         movapd rC02, rC12
         mulpd rA1, rC11
         movddup -96(pB), rB1

         mulpd rA2, rC21
         movapd rC12, rC22
         prefC(-128(pC))

         mulpd rA0, rC02
         movapd -80(pA), rA0
         prefC(-64(pC))

         mulpd rA1, rC12
         movapd -64(pA), rA1
         movddup -88(pB), rB2

         mulpd rA2, rC22
         prefC((pC))
         prefC(64(pC))

@iexp ao -48 0 +
@iexp bo -80 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movapd rA0, rm0
            mulpd rB0, rm0
            addpd rm0, rC00
   @iif k = 2
            #if defined(ATL_MOVEC)
               prefC(128-192(pC,r24))
            #endif
   @endiif

            movapd rA1, rm0
            mulpd rB0, rm0
            addpd rm0, rC10
   @iif k = 2
            #if defined(ATL_MOVEC)
               prefC((pC,r24))
            #endif
   @endiif

            mulpd @(ao)(pA), rB0
            addpd rB0, rC20
            #if KB > @(k)
               movddup @(bo)(pB), rB0
               @iexp bo @(bo) 8 +
            #elif KB == @(k)
               prefB(-128(pfB))
            #endif

            movapd rA0, rm0
            mulpd rB1, rm0
            addpd rm0, rC01

            movapd rA1, rm0
            mulpd rB1, rm0
            addpd rm0, rC11

            mulpd @(ao)(pA), rB1
            addpd rB1, rC21
            #if KB > @(k)
               movddup @(bo)(pB), rB1
               @iexp bo @(bo) 8 +
            #elif KB == @(k)
               prefB(-64(pfB))
            #endif

            mulpd rB2, rA0
            addpd rA0, rC02
            @iexp ao @(ao) 16 +
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #elif KB == @(k)
               prefB((pfB))
            #endif

            mulpd rB2, rA1
            addpd rA1, rC12
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #elif KB == @(k)
               prefA(-128(pfA))
            #endif

            @iexp aa @(ao) -48 +
            mulpd @(aa)(pA), rB2
            addpd rB2, rC22
            #if KB > @(k)
               movddup @(bo)(pB), rB2
               @iexp bo @(bo) 8 +
            #elif KB == @(k)
               prefA((pfA))
            #endif

         #endif
@endiwhile

         add incPF, pfA
         add incPF, pfB
         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movapd rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movapd rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movapd rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movapd rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movapd rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movapd rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movapd rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movapd rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movapd rC22, (pC)
         add $KB*3*8, pB
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_sammm12x3d4r2x256_sse3.S
@skip @extract -b @(topd)/gen.inc what=crsetup
@skip @extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rB1     %xmm2
#define rB2     %xmm3
#define rA0     %xmm4
#define rA1     %xmm5
#define rA2     %xmm8
#define rC00    %xmm7
#define rC10    %xmm6
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15
/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r24     %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define incPF   %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#if KB <= 128
   #define prefA(m_)
   #define prefB(m_) prefetcht0 m_
#else
   #define prefA(m_)
   #define prefB(m_) prefetcht2 m_
#endif
//#define prefC(m_) prefetcht0 m_
#define prefC(m_) prefetchw m_
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP subps
#else
   #define VCOP addps
#endif
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfA       /* pfA = pAn */
   movq 16(%rsp), pfB      /* pfB = pBn */
   cmp pfB, pB
   CMOVE pfA, pfB
   mov KK, KK0
   sub $-128, pC
   sub $-128, pA
   sub $-128, pB
   mov $24, r24
   mov pA, pA0
   mov pB, pB0
   mov $6*3*8, incPF
/*
 * incAm = 6*sizeof*K = 6*8*K = 3*2*8*K = 3*K*16
 */
   lea (KK, KK,2), incAm   /* incAm = 3*K */
   shl $4, incAm           /* incAm = 32*3*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
/*
 *       ==========================
 *       Completely unrolled K-loop
 *       ==========================
 */
         movaps -128(pB), rC00
         movaps -128(pA), rA0
         movaps rC00, rC10

         mulps rA0, rC00
         movaps rC10, rC20
         movaps -112(pA), rA1

         mulps rA1, rC10
         movaps -112(pB), rC01
         movaps -96(pA), rA2

         mulps rA2, rC20
         movaps rC01, rC11
         movaps -96(pB), rC02

         mulps rA0, rC01
         movaps rC11, rC21
         movaps -80(pB), rB0

         movaps rC02, rC12
         mulps rA1, rC11
         movaps -64(pB), rB1

         mulps rA2, rC21
         movaps rC12, rC22
         prefC(-128(pC))

         mulps rA0, rC02
         movaps -80(pA), rA0
         prefC(-64(pC))

         mulps rA1, rC12
         movaps -64(pA), rA1
         movaps -48(pB), rB2

         mulps rA2, rC22
         prefC((pC))
         prefC(64(pC))

@iexp ao -48 0 +
@iexp bo -32 0 +
@iexp k 1 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 1 +
            movaps rA0, rm0
            mulps rB0, rm0
            addps rm0, rC00

            movaps rA1, rm0
            mulps rB0, rm0
            addps rm0, rC10

            mulps @(ao)(pA), rB0
            addps rB0, rC20
            #if KB > @(k)
               movaps @(bo)(pB), rB0
               @iexp bo @(bo) 16 +
            #elif KB == @(k)
               prefB(-128(pfB))
            #endif

            movaps rA0, rm0
            mulps rB1, rm0
            addps rm0, rC01

            movaps rA1, rm0
            mulps rB1, rm0
            addps rm0, rC11

            mulps @(ao)(pA), rB1
            addps rB1, rC21
            #if KB > @(k)
               movaps @(bo)(pB), rB1
               @iexp bo @(bo) 16 +
            #elif KB == @(k)
               prefB(-64(pfB))
            #endif

            mulps rB2, rA0
            addps rA0, rC02
            @iexp ao @(ao) 16 +
            #if KB > @(k)
               movaps @(ao)(pA), rA0
               @iexp ao @(ao) 16 +
            #elif KB == @(k)
               prefB((pfB))
            #endif

            mulps rB2, rA1
            addps rA1, rC12
            #if KB > @(k)
               movaps @(ao)(pA), rA1
               @iexp ao @(ao) 16 +
            #elif KB == @(k)
               prefA(-128(pfA))
            #endif

            @iexp aa @(ao) -48 +
            mulps @(aa)(pA), rB2
            addps rB2, rC22
            #if KB > @(k)
               movaps @(bo)(pB), rB2
               @iexp bo @(bo) 16 +
            #elif KB == @(k)
               prefA((pfA))
            #endif

         #endif
@endiwhile

         add incPF, pfA
         add incPF, pfB
         #ifndef BETA0
            VCOP -128(pC), rC00
         #endif
         movaps rC00, -128(pC)
         #ifndef BETA0
            VCOP -112(pC), rC10
         #endif
         movaps rC10, -112(pC)
         #ifndef BETA0
            VCOP -96(pC), rC20
         #endif
         movaps rC20, -96(pC)
         #ifndef BETA0
            VCOP -80(pC), rC01
         #endif
         movaps rC01, -80(pC)
         #ifndef BETA0
            VCOP -64(pC), rC11
         #endif
         movaps rC11, -64(pC)
         #ifndef BETA0
            VCOP -48(pC), rC21
         #endif
         movaps rC21, -48(pC)
         #ifndef BETA0
            VCOP -32(pC), rC02
         #endif
         movaps rC02, -32(pC)
         #ifndef BETA0
            VCOP -16(pC), rC12
         #endif
         movaps rC12, -16(pC)
         #ifndef BETA0
            VCOP (pC), rC22
         #endif
         movaps rC22, (pC)
         add $KB*3*16, pB
         add $144, pC
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
#if 0
.global findSize
findSize:
mov $SS1-SS0, %rax
ret
SS0:
SS1:
#endif
@ROUT ATL_damm24x8x1_avxz.S ATL_damm32x4x2rp_avxz.S ATL_damm32x6x1_avxz.S ATL_damm16x8x1_avxz.S ATL_damm24x8x2_avxz.S ATL_damm48x4x1_avxz.S ATL_damm16x8x4_avxz.S ATL_amm8x8x8_avxz.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014
@ROUT ATL_damm24x8x2_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm24x8x2_avxz.S
@ROUT ATL_damm48x4x1_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm48x4x1_avxz.S
@ROUT ATL_damm48x4x2_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm48x4x2_avxz.S
@ROUT ATL_damm24x8x1_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm24x8x1_avxz.S
@ROUT ATL_damm32x4x2rp_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm32x4x2rp_avxz.S
@ROUT ATL_damm32x6x1_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm32x6x1_avxz.S
@ROUT ATL_damm16x8x1_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm16x8x1_avxz.S
@ROUT ATL_amm8x8x8_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_amm8x8x8_avxz.S
@ROUT ATL_damm16x8x4_avxz.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_damm16x8x4_avxz.S
@ROUT ATL_skmmm12x1x4_sse3.S ATL_skmmm12x1x256_sse3.S ATL_dkmmm14x1x2_sse3.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#ifndef ATL_SSE3
   #error "This routine requires SSE3!"
#endif
@ROUT ATL_dkmmm14x1x2_sse3.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_dkmmm14x1x2_sse3.S
@ROUT ATL_skmmm12x1x4_sse3.S
@extract -b @(topd)/kernel/ClintWhaley/ATL_skmmm12x1x4_sse3.S
@ROUT ATL_skmmm12x1x256_sse3.S
#include "atlas_asm.h"

#define rm0     %xmm0
#define rB0     %xmm1
#define rC0     %xmm2
#define rC1     %xmm3
#define rC2     %xmm4
#define rC3     %xmm5
#define rC4     %xmm6
#define rC5     %xmm7
#define rC6     %xmm8
#define rC7     %xmm9
#define rC8     %xmm10
#define rC9     %xmm11
#define rC10    %xmm12
#define rC11    %xmm13

/* #define KK      %rdx */   /* API register */ 
#define pA      %rcx   /* API reg */
#define pB      %rax   /* comes in as r8 */
#define NMU     %rdi   /* API reg */
#define NNU     %rsi   /* API reg */
#define pC      %r9    /* API reg */
#define pfB     %r10
#define pfA     %r8

#define NNU0    %r11
#define incA    %r12
#define pB0     %r13
#define r192    %r14
/* #define K0      %r15 */
#define FSIZE 6*8
#ifdef BETAN1
   #define VOP subps
#elif defined(BETA1)
   #define VOP addps
#elif defined(VOP)
   #undef VOP
#endif
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub  $FSIZE, %rsp
   movq %rbp, (%rsp)
   movq %rbx, 8(%rsp)
   movq %r12, 16(%rsp)
   movq %r13, 24(%rsp)
   movq %r14, 32(%rsp)
   movq %r15, 40(%rsp)

   mov %r8, pB
   sub $-128, pA
   sub $-128, pB
   mov pB, pB0
   movq FSIZE+16(%rsp), pfB
   movq FSIZE+8(%rsp), pfA
   mov $12*KB*4, incA   /* incA = 12*K*sizeof */
   mov $192, r192
   mov NNU, NNU0
   
   ALIGN16
   MLOOP:
/*      NLOOP: */
         movaps -128(pB), rC11
         movaps -128(pA), rC0
         mulps rC11, rC0
         movaps -112(pA), rC1
            prefetchw (pC)
         mulps rC11, rC1
         movaps -96(pA), rC2
            prefetcht0 (pfB)
         mulps rC11, rC2
         movaps -80(pA), rC3
            prefetcht0 (pfA)
         mulps rC11, rC3
         movaps -64(pA), rC4
         mulps rC11, rC4
         movaps -48(pA), rC5
         mulps rC11, rC5
         movaps -32(pA), rC6
         mulps rC11, rC6
         movaps -16(pA), rC7
         mulps rC11, rC7
         movaps (pA), rC8
         mulps rC11, rC8
         movaps 16(pA), rC9
         mulps rC11, rC9
         movaps 32(pA), rC10
         mulps rC11, rC10
         mulps 48(pA), rC11

            add $48, pfB
            add $48, pfA
/*         KLOOP: */
@iexp ao 48 16 +
@iexp bo -128 16 +
@iexp k 4 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 4 +
            movaps @(bo)(pB), rB0
   @iexp bo @(bo) 16 +
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC0
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC1
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC2
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC3
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC4
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC5
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC6
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC7
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC8
            movaps  @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC9
            movaps @(ao)(pA), rm0
   @iexp ao @(ao) 16 +
            mulps rB0, rm0
            addps rm0, rC10
            mulps @(ao)(pA), rB0
   @iexp ao @(ao) 16 +
            addps rB0, rC11
         #endif
@endiwhile
KDONE:
/*
 *       Sum up rCx regs
 */
         haddps rC1, rC0        /* rC0 = {c1cd,c1ab,c0cd,c0ab} */
         haddps rC3, rC2        /* rC2 = {c3cd,c3ab,c2cd,c2ab} */
         haddps rC2, rC0        /* rC0 = {c3abcd,c2abcd,c1abcd,c0abcd} */
         #ifdef VOP
            VOP (pC), rC0
         #endif
         movaps rC0, (pC)
         haddps rC5, rC4        /* rC4 = {c5cd,c5ab,c4cd,c4ab} */
         haddps rC7, rC6        /* rC6 = {c7cd,c7ab,c6cd,c6ab} */
         haddps rC6, rC4        /* rC4 = {c3abcd,c2abcd,c1abcd,c0abcd} */
         #ifdef VOP
            VOP 16(pC), rC4
         #endif
         movaps rC4, 16(pC)
         haddps rC9, rC8        /* rC8 = {c9cd,c9ab,c8cd,c8ab} */
         haddps rC11, rC10      /* rC10 = {c11cd,c11ab,c10cd,c10ab} */
         haddps rC10, rC8       /* rC8 = {c11abcd,c10abcd,c9abcd,c8abcd} */
         #ifdef VOP
            VOP 32(pC), rC8
         #endif
         movaps rC8, 32(pC)
         add $48, pC
         add $KB*4, pB
         sub $1, NNU
      jnz MLOOP
      add incA, pA   /* pA += 12*K*sizeof */
      mov pB0, pB
      sub $1, NMU
      mov NNU0, NNU
   jnz MLOOP

DONE:
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
   movq 16(%rsp), %r12
   movq 24(%rsp), %r13
   movq 32(%rsp), %r14
   movq 40(%rsp), %r15
   add  $FSIZE, %rsp
   ret
@ROUT ATL_skmmm4x2x256_avx.S
#include "atlas_asm.h"

#define rm0     %ymm0
#define rB0     %ymm1
#define rB1     %ymm2
#define rA0     %ymm3
#define rA1     %ymm4
#define rA2     %ymm5
#define rA3     %ymm6
#define rC00    %ymm7
#define rC10    %ymm8
#define rC20    %ymm9
#define rC30    %ymm10
#define rC01    %ymm11
#define rC11    %ymm12
#define rC21    %ymm13
#define rC31    %ymm14

#define KK      %rdx   /* API register */
#define pA      %rcx   /* API reg */
#define pB      %rax   /* comes in as r8 */
#define NMU     %rdi   /* API reg */
#define NNU     %rsi   /* API reg */
#define pC      %r9    /* API reg */
#define pfB     %r10
#define pfA     %r8

#define NNU0    %r11
#define incA    %r12
#define pB0     %r13
#define r192    %r14
#define K0      %r15
#define FSIZE 6*8

#ifdef BETAN1
   #define VCOP vsubps
#elif defined(BETA1)
   #define VCOP vaddps
#elif defined(VCOP)
   #undef VCOP
#endif
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub  $FSIZE, %rsp
   movq %rbp, (%rsp)
   movq %rbx, 8(%rsp)
   movq %r12, 16(%rsp)
   movq %r13, 24(%rsp)
   movq %r14, 32(%rsp)
   movq %r15, 40(%rsp)

   mov %r8, pB
   movq FSIZE+16(%rsp), pfB
   movq FSIZE+8(%rsp), pfA
/*
 * Set incA = 4*K*sizeof = 4*K*4 = 16*K
 */
   mov KK, incA
   shl $4, incA
   shr $3, KK                   /* KK /= VLEN */
   mov KK, K0
   mov $192, r192
   mov NNU, NNU0
   sub $-128, pA
   sub $-128, pB
   mov pB, pB0

   ALIGN16
   MLOOP:
//      NLOOP:
         vmovaps -128(pB), rB0
         vmovaps -128(pA), rA0
         vmulps rB0, rA0, rC00
         vmovaps -96(pA), rA1
         vmovaps -64(pA), rA2
         vmulps rB0, rA1, rC10
         vmovaps -32(pA), rA3
         vmovaps -96(pB), rB1
         vmulps rB0, rA2, rC20
         vmulps rB0, rA3, rC30
            prefetcht0 (pC)
         vmulps rB1, rA0, rC01
         vmovaps -64(pB), rB0
         vmulps rB1, rA1, rC11
         vmulps rB1, rA2, rC21
         vmulps rB1, rA3, rC31
            prefetcht2 (pfB)

            prefetcht2 (pfA)
            add $32, pfB
            add $32, pfA
//         KLOOP:
@iexp ao -32 32 +
@iexp bo -64 32 +
@iexp k 8 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 8 +
            vmovaps @(ao)(pA), rA0
   @iexp ao @(ao) 32 +
            vmulps rB0, rA0, rm0
            vaddps rm0, rC00, rC00
            vmovaps @(ao)(pA), rA1
   @iexp ao @(ao) 32 +
            vmulps rB0, rA1, rm0 
            vaddps rm0, rC10, rC10
            vmovaps @(ao)(pA), rA2
   @iexp ao @(ao) 32 +
            vmulps rB0, rA2, rm0 
            vaddps rm0, rC20, rC20
            vmovaps @(ao)(pA), rA3
   @iexp ao @(ao) 32 +
            vmulps rB0, rA3, rm0 
            vaddps rm0, rC30, rC30
            vmovaps @(bo)(pB), rB1
   @iexp bo @(bo) 32 +
            vmulps rB1, rA0, rm0 
            vaddps rm0, rC01, rC01
            vmulps rB1, rA1, rm0 
            vaddps rm0, rC11, rC11
            vmulps rB1, rA2, rm0 
            vaddps rm0, rC21, rC21
            vmovaps @(bo)(pB), rB0
   @iexp bo @(bo) 32 +
            vmulps rB1, rA3, rm0 
            vaddps rm0, rC31, rC31
         #endif
@endiwhile
            /*rC00 = {c00h,c00g,c00f,c00e,c00d,c00c,c00b,c00a}*/
            /*rC10 = {c10h,c10g,c10f,c10e,c10d,c10c,c10b,c10a}*/
            /*rC20 = {c20h,c20g,c20f,c20e,c20d,c20c,c20b,c20a}*/
            /*rC30 = {c30h,c30g,c30f,c30e,c30d,c30c,c30b,c30a}*/
KDONE:
         vhaddps rC10, rC00, rC00 
            /*rC00 = {c10gh,c10ef,c00gh,c00ef,c10cd,c10ab,c00cd,c00ab}*/
         vhaddps rC30, rC20, rC20
            /*rC20 = {c30gh,c30ef,c20gh,c20ef,c30cd,c30ab,c20cd,c20ab}*/
         vhaddps rC20, rC00, rC00
            /*rC00 = {c30eh,c20eh,c10eh,c00eh,c30ad,c20ad,c10ad,c00ad}*/

         vhaddps rC11, rC01, rC01
            /*rC01 = {c11gh,c11ef,c01gh,c01ef,c11cd,c11ab,c01cd,c01ab}*/
         vhaddps rC31, rC21, rC21
            /*rC21 = {c31gh,c31ef,c21gh,c21ef,c31cd,c31ab,c21cd,c21ab}*/
         vhaddps rC21, rC01, rC01
            /*rC01 = {c31eh,c21eh,c11eh,c01eh,c31ad,c21ad,c11ad,c01ad}*/

         vperm2f128 $0x31, rC01, rC00, rC10  /*0011 0001 */
            /*rC10 = {c31eh,c21eh,c11eh,c01eh,c30eh,c20eh,c10eh,c00eh}*/
         vperm2f128 $0x20, rC01, rC00, rC00  /*0010 0000 */
            /*rC00 = {c31ad,c21ad,c11ad,c01ad,c30ad,c20ad,c10ad,c00ad}*/
         vaddps rC10, rC00, rC00
            /*rC00 = {c31ah,c21ah,c11ah,c01ah,c30ah,c20ah,c10ah,c00ah}*/
         #ifdef VCOP
            VCOP (pC), rC00, rC00
         #endif
         vmovaps rC00, (pC)
         add $KB*4*2, pB
         add $32, pC
         sub $1, NNU
      jnz MLOOP
      add incA, pA   /* pA += 4*K*sizeof */
      mov pB0, pB
      sub $1, NMU
      mov NNU0, NNU
   jnz MLOOP

DONE:
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
   movq 16(%rsp), %r12
   movq 24(%rsp), %r13
   movq 32(%rsp), %r14
   movq 40(%rsp), %r15
   add  $FSIZE, %rsp
   ret
@ROUT ATL_dkmmm14x1x256_sse3.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2013
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#ifndef ATL_SSE3
   #error "This routine requires SSE3!"
#endif
#include "atlas_asm.h"

#define rA0     %xmm0
#define rB0     %xmm1
#define rC0     %xmm2
#define rC1     %xmm3
#define rC2     %xmm4
#define rC3     %xmm5
#define rC4     %xmm6
#define rC5     %xmm7
#define rC6     %xmm8
#define rC7     %xmm9
#define rC8     %xmm10
#define rC9     %xmm11
#define rC10    %xmm12
#define rC11    %xmm13
#define rC12    %xmm14
#define rC13    %xmm15

/* #define KK      %rdx */   /* API register */ 
#define pA      %rcx   /* API reg */
#define pB      %rax   /* comes in as r8 */
#define NMU     %rdi   /* API reg */
#define NNU     %rsi   /* API reg */
#define pC      %r9    /* API reg */
#define pfB     %r10
#define pfA     %r8

#define NNU0    %r11
#define incA    %r12
#define pB0     %r13
#define r112    %r14
/* #define K0      %r15 */
#define FSIZE 6*8
#ifdef BETAN1
   #define VOP subpd
#elif defined(BETA1)
   #define VOP addpd
#elif defined(VOP)
   #undef VOP
#endif
#ifdef ATL_3DNow
   #define prefC prefetchw
#else
   #define prefC prefetcht0
#endif
#if KB <= 40
   #define prefB(m_) prefetcht0 m_
#else
   #define prefB(m_) prefetcht2 m_
#endif
#if KB > 120
   #define prefA(m_)
#elif KB > 40
   #define prefA(m_) prefetcht2 m_
#else
   #define prefA(m_) prefetcht0 m_
#endif
#define MOVAPD movaps
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub  $FSIZE, %rsp
   movq %rbp, (%rsp)
   movq %rbx, 8(%rsp)
   movq %r12, 16(%rsp)
   movq %r13, 24(%rsp)
   movq %r14, 32(%rsp)
   movq %r15, 40(%rsp)

   movq FSIZE+16(%rsp), pfB
   mov %r8, pB
@skip   cmp pfB, pB
@skip   cmovEQ FSIZE+24(%rsp), pfB
   movq FSIZE+8(%rsp), pfA
@skip   cmp pfA, pA
@skip   cmovEQ FSIZE+24(%rsp), pfA
   sub $-128, pA
   sub $-128, pB
   mov pB, pB0
   mov $14*KB*8, incA   /* incA = 14*K*sizeof */
   mov NNU, NNU0
   mov $112, r112
   
   ALIGN16
   MLOOP:
/*      NLOOP: */
         MOVAPD -128(pB), rC13
         MOVAPD -128(pA), rC0
         mulpd rC13, rC0
         MOVAPD -112(pA), rC1
         mulpd rC13, rC1
         MOVAPD -96(pA), rC2
         mulpd rC13, rC2
         MOVAPD -80(pA), rC3
         mulpd rC13, rC3
         MOVAPD -64(pA), rC4
         mulpd rC13, rC4
         MOVAPD -48(pA), rC5
         mulpd rC13, rC5
         MOVAPD -32(pA), rC6
         mulpd rC13, rC6
         MOVAPD -16(pA), rC7
         mulpd rC13, rC7
         MOVAPD (pA), rC8
         mulpd rC13, rC8
         MOVAPD 16(pA), rC9
         mulpd rC13, rC9
         MOVAPD 32(pA), rC10
         mulpd rC13, rC10
         MOVAPD 48(pA), rC11
         mulpd rC13, rC11
         MOVAPD 64(pA), rC12
            prefC (pC)
         mulpd rC13, rC12
            prefC 64(pC)
         mulpd  80(pA), rC13

/*         KLOOP: */
@iexp ao 80 16 +
@iexp bo -128 16 +
@iexp k 2 0 +
@iwhile k < 256
         #if KB > @(k)
   @iexp k @(k) 2 +
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            MOVAPD @(bo)(pB), rB0
   @iexp bo @(bo) 16 +
            mulpd rB0, rA0
            addpd rA0, rC0
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC1
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC2
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC3
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC4
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC5
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC6
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC7
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC8
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC9
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC10
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            mulpd rB0, rA0
            addpd rA0, rC11
            MOVAPD @(ao)(pA), rA0
   @iexp ao @(ao) 16 +
            #if KB == @(k)
               prefB((pfB))
            #elif KB-2 == @(k)
               prefA((pfA))
            #endif
            mulpd rB0, rA0
            addpd rA0, rC12
            #if KB == @(k)
               prefB(64(pfB))
            #elif KB-2 == @(k)
               prefA(64(pfA))
            #endif
            mulpd  @(ao)(pA), rB0
   @iexp ao @(ao) 16 +
            addpd rB0, rC13
         #endif
@endiwhile
KDONE:
/*
 *       Sum up rCx regs
 */
         haddpd rC1, rC0        /* rC0 = {c1ab,c0ab} */
         #ifdef VOP
            VOP (pC), rC0
         #endif
         MOVAPD rC0, (pC)
         haddpd rC3, rC2        /* rC2 = {c3ab,c2ab} */
         #ifdef VOP
            VOP 16(pC), rC2
         #endif
         MOVAPD rC2, 16(pC)
         haddpd rC5, rC4        /* rC4 = {c5ab,c4ab} */
         #ifdef VOP
            VOP 32(pC), rC4
         #endif
         MOVAPD rC4, 32(pC)
         haddpd rC7, rC6        /* rC6 = {c7ab,c6ab} */
         #ifdef VOP
            VOP 48(pC), rC6
         #endif
         MOVAPD rC6, 48(pC)
         haddpd rC9, rC8        /* rC8 = {c9ab,c8ab} */
         #ifdef VOP
            VOP 64(pC), rC8
         #endif
         MOVAPD rC8, 64(pC)
         haddpd rC11, rC10      /* rC10 = {c11ab,c10ab} */
         #ifdef VOP
            VOP 80(pC), rC10
         #endif
         MOVAPD rC10, 80(pC)
         haddpd rC13, rC12      /* rC12 = {c13ab,c12ab} */
         #ifdef VOP
            VOP 96(pC), rC12
         #endif
         MOVAPD rC12, 96(pC)

            add r112, pfB
            add r112, pfA
         add r112, pC
         add $KB*8, pB
         sub $1, NNU
      jnz MLOOP
      add incA, pA   /* pA += 14*K*sizeof */
      mov pB0, pB
      sub $1, NMU
      mov NNU0, NNU
   jnz MLOOP

DONE:
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
   movq 16(%rsp), %r12
   movq 24(%rsp), %r13
   movq 32(%rsp), %r14
   movq 40(%rsp), %r15
   add  $FSIZE, %rsp
   ret
@ROUT ATL_damm32x4x256_avxz.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2014
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#ifndef ATL_AVXZ
   #error "This routine requires AVX-512!"
#endif
#include "atlas_asm.h"

#define rB0     %zmm0
#define rB1     %zmm1
#define rB2     %zmm2
#define rB3     %zmm3
#define rA0     %zmm4
#define rA1     %zmm5
#define rA2     %zmm6
#define rA3     %zmm7
#define rb0     %zmm8
#define rb1     %zmm9
#define rb2     %zmm10
#define rb3     %zmm11
#define ra0     %zmm12
#define ra1     %zmm13
#define ra2     %zmm14
#define ra3     %zmm15
#define rC00    %zmm16
#define rC10    %zmm17
#define rC20    %zmm18
#define rC30    %zmm19
#define rC01    %zmm20
#define rC11    %zmm21
#define rC21    %zmm22
#define rC31    %zmm23
#define rC02    %zmm24
#define rC12    %zmm25
#define rC22    %zmm26
#define rC32    %zmm27
#define rC03    %zmm28
#define rC13    %zmm29
#define rC23    %zmm30
#define rC33    %zmm31

/*
 * Prioritize original registers for inner-loop operations, but inc regs
 * can be anything w/o changing opcode size, so use new regs for those
 */
#define KK      %rdx  /* API reg */
#define pA      %rcx  /* API reg */
#define pB      %rax  /* comes in as r9 */
#define r256    %r9   /* set after mov r9 to pC () */
/*
 * Then N-loop variables much less important, so use any orig regs left
 */
#define pA0     %r8   /* set after mov r8 to pB (rax) */
#define pC      %rsi  /* set after mov rsi to nnu () */
#define nnu     %r10  /* comes in as rsi */
#define pfA     %rbx
#define pfB     %rbp
#define r192    %r12
#define KK0     %rdi
/*
 * We could give a rat's ass about what registers used in outer (M-) loop
 */
#define nmu     %r11  /* comes in as rdi */
#define incAm   %r13
#define nnu0    %r14
#define pB0     %r15
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
#define PFBDIST 128
#define PFADIST 128
#define prefA(m_) vprefetch2 m_
#define prefB(m_) vprefetch2 m_
#define prefC(m_) vprefetch1 m_
#define FMAC vfmadd231pd   /* FMAC m256/r256, rs1, rd */
#if defined(BETAN) || defined(BETAn)
   #define BETAN1
#endif
#ifdef BETAN1
   #define VCOP vsubpd
#else
   #define VCOP vaddpd
#endif
//#define vmovapd vmovaps
.text
.align 16,0x90
.globl ATL_USERMM
ATL_USERMM:
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Load paramaters
 */
   mov %rdi, nmu
   mov %rsi, nnu
   mov %r8, pB
   mov %r9, pC
   mov nnu, nnu0
   movq 8(%rsp), pfA      /* pfA = pAn */
   movq 16(%rsp), pfB     /* pfB = pBn */
/*   cmp pfB, pB */
/*   CMOVE pfA, pfB */
   vprefetch0 (pB)
   mov KK, incAm
   vprefetch0 64(pB)
   shr $1, KK             /* KK = K/2 */
   mov KK, KK0
   vprefetch0 128(pB)
   sub $-128, pC
   sub $-128, pA
   vprefetch0 192(pB)
   sub $-128, pB
   sub $-128, pfA
   vprefetch0 256-128(pB)
   sub $-128, pfB
   mov $256, r256
   mov $192, r192
   vprefetch0 320-128(pB)
   mov pA, pA0
   mov pB, pB0
/*
 * incAm = MU*sizeof*K = 32*8*K = 256*K
 */
   shl $8, incAm           /* incAm = 256*K */

   ALIGN16
   MLOOP:
      NLOOP:
/*
 *       First iteration peeled to handle init of rC
 */
         vbroadcastsd -128(pB), rB0
         vmovapd -128(pA), rA0
         vmovapd -64(pA), rA1
         vmovapd (pA), rA2
         vmovapd 64(pA), rA3
         vbroadcastsd -120(pB), rB1
         vbroadcastsd -112(pB), rB2
         vbroadcastsd -104(pB), rB3
         #if KB > 1
            vmovapd 128(pA), ra0
            vmovapd 192(pA), ra1
            vmovapd 256(pA), ra2
            vmovapd 320(pA), ra3
            vbroadcastsd -96(pB), rb0
            vbroadcastsd -88(pB), rb1
            vbroadcastsd -80(pB), rb2
            vbroadcastsd -72(pB), rb3
         #endif

         vmulpd rA0, rB0, rC00
         vmulpd rA1, rB0, rC10
         vmulpd rA2, rB0, rC20
         vmulpd rA3, rB0, rC30
         #if KB > 2
            vbroadcastsd -64(pB), rB0
         #endif

         vmulpd rA0, rB1, rC01
         vmulpd rA1, rB1, rC11
         vmulpd rA2, rB1, rC21
         vmulpd rA3, rB1, rC31
         #if KB > 2
            vbroadcastsd -56(pB), rB1
         #endif

         vmulpd rA0, rB2, rC02
         vmulpd rA1, rB2, rC12
         vmulpd rA2, rB2, rC22
         vmulpd rA3, rB2, rC32
         #if KB > 2
            vbroadcastsd -48(pB), rB2
         #endif

         vmulpd rA0, rB3, rC03
         #if KB > 2
            vmovapd 384(pA), rA0
         #endif
         vmulpd rA1, rB3, rC13
         #if KB > 2
            vmovapd 448(pA), rA1
         #endif
         vmulpd rA2, rB3, rC23
         #if KB > 2
            vmovapd 512(pA), rA2
         #endif
         vmulpd rA3, rB3, rC33
         #if KB > 2
            vmovapd 576(pA), rA3
         #endif
/*
 *       2nd peeled K iteration
 */
         #if KB > 1
            FMAC ra0, rb0, rC00
            #if KB > 2
               vbroadcastsd -40(pB), rB3
            #endif
            FMAC ra1, rb0, rC10
            FMAC ra2, rb0, rC20
            FMAC ra3, rb0, rC30
            #if KB > 3
               vbroadcastsd -32(pB), rb0
            #endif

            FMAC ra0, rb1, rC01
            FMAC ra1, rb1, rC11
            FMAC ra2, rb1, rC21
            FMAC ra3, rb1, rC31
            #if KB > 3
               vbroadcastsd -24(pB), rb1
            #endif

            FMAC ra0, rb2, rC02
            FMAC ra1, rb2, rC12
            FMAC ra2, rb2, rC22
            FMAC ra3, rb2, rC32
            #if KB > 3
               vbroadcastsd -16(pB), rb2
            #endif

            FMAC ra0, rb3, rC03
            #if KB > 3
               vmovapd 640(pA), ra0
            #endif
            FMAC ra1, rb3, rC13
            #if KB > 3
               vmovapd 704(pA), ra1
            #endif
            FMAC ra2, rb3, rC23
            #if KB > 3
               vmovapd 768(pA), ra2
            #endif
            FMAC ra3, rb3, rC33
            #if KB > 3
               vmovapd 832(pA), ra3
            #endif
         #endif
@iexp ao 832 64 +
@iexp bo -16 8 +
@iexp k 1 1 +
@iexp fo -128 0 +
@multidef pi B A C
@multidef pf pfB pfA pC
@iwhile k < 320
         #if KB > @(k)
   @iexp k @(k) 1 +
            FMAC rA0, rB0, rC00
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rb3
            #endif
   @iexp bo @(bo) 8 +
            FMAC rA1, rB0, rC10
            FMAC rA2, rB0, rC20
            FMAC rA3, rB0, rC30
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB0
            #endif
   @iexp bo @(bo) 8 +

            FMAC rA0, rB1, rC01
            @ifdef pf
               pref@(pi)(@(fo)(@(pf)))
               @iexp fo @(fo) 64 +
               @iif fo > 1408
                  @undef pf 
                  @undef pi
                  @iexp fo -128 0 +
               @endiif
            @endifdef
            FMAC rA1, rB1, rC11
            FMAC rA2, rB1, rC21
            FMAC rA3, rB1, rC31
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB1
            #endif
   @iexp bo @(bo) 8 +

            FMAC rA0, rB2, rC02
            @ifdef pf
               pref@(pi)(@(fo)(@(pf)))
               @iexp fo @(fo) 64 +
               @iif fo > 1408
                  @undef pf 
                  @undef pi
                  @iexp fo -128 0 +
               @endiif
            @endifdef
            FMAC rA1, rB2, rC12
            FMAC rA2, rB2, rC22
            FMAC rA3, rB2, rC32
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rB2
            #endif
   @iexp bo @(bo) 8 +

            FMAC rA0, rB3, rC03
            #if KB > @(k)
               vmovapd @(ao)(pA), rA0
            #endif
   @iexp ao @(ao) 64 +
            FMAC rA1, rB3, rC13
            #if KB > @(k)
               vmovapd @(ao)(pA), rA1
            #endif
   @iexp ao @(ao) 64 +
            FMAC rA2, rB3, rC23
            #if KB > @(k)
               vmovapd @(ao)(pA), rA2
            #endif
   @iexp ao @(ao) 64 +
            FMAC rA3, rB3, rC33
            #if KB > @(k)
               vmovapd @(ao)(pA), rA3
            #endif
   @iexp ao @(ao) 64 +
         #endif

         #if KB > @(k)
   @iexp k @(k) 1 +
            FMAC ra0, rb0, rC00
               vbroadcastsd @(bo)(pB), rB3
   @iexp bo @(bo) 8 +
            FMAC ra1, rb0, rC10
            FMAC ra2, rb0, rC20
            FMAC ra3, rb0, rC30
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rb0
            #endif
   @iexp bo @(bo) 8 +
            FMAC ra0, rb1, rC01
            @ifdef pf
               pref@(pi)(@(fo)(@(pf)))
               @iexp fo @(fo) 64 +
               @iif fo > 1408
                  @undef pf 
                  @undef pi
                  @iexp fo -128 0 +
               @endiif
            @endifdef
            FMAC ra1, rb1, rC11
            FMAC ra2, rb1, rC21
            FMAC ra3, rb1, rC31
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rb1
            #endif
   @iexp bo @(bo) 8 +


            FMAC ra0, rb2, rC02
            @ifdef pf
               pref@(pi)(@(fo)(@(pf)))
               @iexp fo @(fo) 64 +
               @iif fo > 1408
                  @undef pf 
                  @undef pi
                  @iexp fo -128 0 +
               @endiif
            @endifdef
            FMAC ra1, rb2, rC12
            FMAC ra2, rb2, rC22
            FMAC ra3, rb2, rC32
            #if KB > @(k)
               vbroadcastsd @(bo)(pB), rb2
            #endif
   @iexp bo @(bo) 8 +

            FMAC ra0, rb3, rC03
            #if KB > @(k)
               vmovapd @(ao)(pA), ra0
            #endif
   @iexp ao @(ao) 64 +
            FMAC ra1, rb3, rC13
            #if KB > @(k)
               vmovapd @(ao)(pA), ra1
            #endif
   @iexp ao @(ao) 64 +
            FMAC ra2, rb3, rC23
            #if KB > @(k)
               vmovapd @(ao)(pA), ra2
            #endif
   @iexp ao @(ao) 64 +
            FMAC ra3, rb3, rC33
            #if KB > @(k)
               vmovapd @(ao)(pA), ra3
            #endif
   @iexp ao @(ao) 64 +
         #endif
@endiwhile


         #ifndef BETA0
            VCOP -128(pC), rC00, rC00
            VCOP -64(pC), rC10, rC10
            VCOP (pC), rC20, rC20
            VCOP 64(pC), rC30, rC30

            VCOP 128(pC), rC01, rC01
            VCOP 192(pC), rC11, rC11
            VCOP 256(pC), rC21, rC21
            VCOP 320(pC), rC31, rC31

            VCOP 384(pC), rC02, rC02
            VCOP 448(pC), rC12, rC12
            VCOP 512(pC), rC22, rC22
            VCOP 576(pC), rC32, rC32

            VCOP 640(pC), rC03, rC03
            VCOP 704(pC), rC13, rC13
            VCOP 768(pC), rC23, rC23
            VCOP 832(pC), rC33, rC33
         #endif

         vmovapd rC00, -128(pC)
         vmovapd rC10, -64(pC)
         vmovapd rC20, (pC)
         vmovapd rC30, 64(pC)

         vmovapd rC01, 128-256(pC,r256)
         vmovapd rC11, 192-256(pC,r256)
         vmovapd rC21, (pC,r256)
         vmovapd rC31, 320-256(pC,r256)

         vmovapd rC02, 384-512(pC,r256,2)
         vmovapd rC12, 448-512(pC,r256,2)
         vmovapd rC22, (pC,r256,2)
         vmovapd rC32, 576-512(pC,r256,2)

         vmovapd rC03, 640(pC)
         vmovapd rC13, 704(pC)
         vmovapd rC23, 768(pC)
         vmovapd rC33, 832(pC)

         add $4*KB*8, pB /* pB += NU*KB*sizeof */
         add $1024, pC   /* pC += MU*NU*sizeof = 32*4*8 = 1024 */
         sub $1, nnu
      jnz NLOOP
      mov nnu0, nnu
      add incAm, pA0
      mov pA0, pA
      mov pB0, pB
      sub $1, nmu
   jnz MLOOP
 DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
   .align 16,0x90
	.type	ATL_USERMM,@function
	.size	ATL_USERMM,.-ATL_USERMM
@ROUT ATL_kmmm4x2x256_sse3.S
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2015
/*
 * This computational kernel was created using a code fragment demonstrating a
 * Core2Duo-friendly 2-D x86 register block sent to me by Yevgen Voronenko 
 * of the CMU/SPIRAL group as a template.  Here is original the code fragment
 * that Yevgen sent me:
 *      movapd    (%rdi), %xmm9                
 *      movapd    %xmm9, %xmm6                                  
 *      movapd    48(%rdi), %xmm8                               
 *      mulpd     %xmm8, %xmm6                                  
 *      addpd     %xmm6, %xmm5                                  
 *      movapd    16(%rdi), %xmm10                              
 *      movapd    %xmm10, %xmm7                                 
 *      mulpd     %xmm8, %xmm7                                  
 *      addpd     %xmm7, %xmm4                                  
 *      movapd    32(%rdi), %xmm12                              
 *      mulpd     %xmm12, %xmm8                                 
 *      addpd     %xmm8, %xmm3                                  
 *      movapd    64(%rdi), %xmm11                              
 *      mulpd     %xmm11, %xmm9                                 
 *      mulpd     %xmm11, %xmm10                                
 *      mulpd     %xmm11, %xmm12                                
 *      addpd     %xmm9, %xmm2                                  
 *      addpd     %xmm10, %xmm1                                 
 *      addpd     %xmm12, %xmm0                                 
 */


#if !defined(ATL_GAS_x8664)
   #error "This kernel requires x86-64 assembly!"
#endif

#if !defined(KB) || (KB == 0)
   #error "KB must be a compile-time constant!"
#endif
#if KB > 256
   #error "KB can at most be 256!"
#endif
#if (KB/2)*2 != KB
   #error "KB must be a multiple of 2!"
#endif

#include "atlas_asm.h"
#define movapd movaps
#define nmu     %rdi
#define nnu     %rsi
#define nnu0    %r10
#define pA      %rcx
#define pB      %rax
#define pC      %r9
#define pfA     %rbp
#define pB0     %r12
#define pf0     %rbx
#define pfB     %rdx
#define incAm   %r11
#define pfC     %r13
#define incBn   %r14

#define rA0 	%xmm0
#define rA1 	%xmm1
#define rA2 	%xmm2
#define rA3 	%xmm3
#define rB0 	%xmm4
#define ra0 	%xmm5
#define ra1 	%xmm6
#define ra2 	%xmm7
#define rC00 	%xmm8
#define rC10 	%xmm9
#define rC20 	%xmm10
#define rC30 	%xmm11
#define rC01 	%xmm12
#define rC11 	%xmm13
#define rC21 	%xmm14
#define rC31 	%xmm15

/*
 * Save some inst space by using short version of instructions
 */
#if defined(SREAL) || defined(SCPLX)
   #define movapd movaps
   #define subpd subps
   #define addpd addps
   #define mulpd mulps
   #define addpd addps
   #define kmul 2
   #define SZ 4
#else
   #define movapd movaps
   #define kmul 1
   #define SZ 8
#endif
#ifdef BETAN1
   #define VOP subpd
#elif defined(BETA1)
   #define VOP addpd
#elif defined(VOP)
   #undef VOP
#endif
#ifdef ATL_3DNow
   #define prefC prefetchw
#else
   #define prefC prefetcht0
#endif
#define FSIZE 48
/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
.text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 * Save callee-saved iregs
 */
   sub $FSIZE, %rsp
   movq    %rbp, 0(%rsp)
   movq    %rbx, 8(%rsp)
   movq    %r12, 16(%rsp)
   movq    %r13, 24(%rsp)
   movq    %r14, 32(%rsp)
      prefetcht0 (pA)
/*
 * Load paramaters
 */
   movq %r8, pB
      prefetcht0 (pB)
   mov nnu, nnu0
   movq FSIZE+16(%rsp), pfB     /* pfB = pBn */
   movq FSIZE+8(%rsp), pfA      /* pfA = pAn */
   movq FSIZE+24(%rsp), pfC     /* pfC = pCn */
      prefetcht0 64(pA)
/*
 * Extend range of small operands by starting at -128
 */
   sub $-128, pA
      prefetcht0 (pA)
   sub $-128, pB
      prefetcht0 64(pA)
   mov $KB*4*SZ, incAm         /* incAm = KB*MU*size */
      prefetcht0 -64(pB)
   mov $KB*2*SZ, incBn         /* incBn = KB*NU*size */
      prefetcht0 (pB)
   movq pB, pB0
   lea (pA, incAm), pf0

   ALIGN16
   .local MNLOOP
   MNLOOP:
/*
 *       Peel first iteration to zero registers
 */
         movapd -128(pA), rC00
         movapd -128(pB), rB0
         movapd rC00, rC01
         mulpd  rB0, rC00
         movapd -112(pA), rC10
         movapd rC10, rC11
         mulpd  rB0, rC10
         movapd -96(pA), rC20
         movapd rC20, rC21
         mulpd  rB0, rC20
         movapd -80(pA), rC30
         movapd rC30, rC31
         mulpd  rB0, rC30
         movapd -112(pB), ra0
         mulpd  ra0, rC01
         mulpd  ra0, rC11
         #if KB > 2*kmul
            movapd -96(pB), rB0
         #endif
         #if KB < 14*kmul
            prefetcht0 (pC)
         #endif
         mulpd  ra0, rC21
         mulpd  ra0, rC31
            prefetcht0 (pf0)
            add $64, pf0
@iexp k 0 2 +
@iexp bo -96 16 +
@iexp ao -80 16 +
@iwhile k < 256
         #if KB > @(k)*kmul
   @iexp k 2 @(k) +
            movapd @(ao)(pA), rA0 
   @iexp ao @(ao) 16 +
            movapd rA0, ra0 
            mulpd  rB0, rA0 
            addpd  rA0, rC00 
            movapd @(ao)(pA), rA1 
   @iexp ao @(ao) 16 +
            movapd rA1, ra1 
            mulpd  rB0, rA1 
            addpd  rA1, rC10 
            movapd @(ao)(pA), rA2 
   @iexp ao @(ao) 16 +
            movapd rA2, ra2 
            mulpd  rB0, rA2 
            addpd  rA2, rC20 
            movapd @(ao)(pA), rA3 
   @iexp ao @(ao) 16 +
            mulpd  rA3, rB0 
            addpd  rB0, rC30 
            movapd @(bo)(pB), rB0 
   @iexp bo @(bo) 16 +
            mulpd  rB0, ra0 
            mulpd  rB0, ra1 
            mulpd  rB0, ra2 
            mulpd  rB0, rA3 
            #if KB > @(k)*kmul
               movapd @(bo)(pB), rB0 
   @iexp bo @(bo) 16 +
            #endif
            #if kmul*(@(k)+12) == KB
                prefetcht0 (pC)
            #endif
            addpd  ra0, rC01 
            addpd  ra1, rC11 
            addpd  ra2, rC21 
            addpd  rA3, rC31 
         #endif
@endiwhile
            prefetcht2 (pfA)
            add $64, pfA
            prefetcht2 (pfB)
            add $64, pfB

/* KDONE: */
/*
 *       Sum up rCx regs, apply original C, and write C out
 */
         #if defined(SREAL) || defined(SCPLX)
            haddps rC10, rC00  /* rC00 = {c1cd,c1ab,c0cd,c0ab} */
            haddps rC30, rC20  /* rC20 = {c3cd,c3ab,c2cd,c0ab} */
            haddps rC20, rC00  /* rC00 = {c3,c2,c1,c0} */
            #ifdef VOP
               VOP (pC), rC00
            #endif
            movaps rC00, (pC)

            haddps rC11, rC01  /* rC01 = {c1cd,c1ab,c0cd,c0ab} */
            haddps rC31, rC21  /* rC21 = {c3cd,c3ab,c2cd,c0ab} */
            haddps rC21, rC01  /* rC01 = {c3,c2,c1,c0} */
            #ifdef VOP
               VOP 16(pC), rC01
            #endif
            movaps rC01, 16(pC)
            add $32, pC
         #else
            haddpd rC10, rC00  /* rC00 = {c10ab,c0ab} */
            #ifdef VOP
               VOP (pC), rC00
            #endif
            movapd rC00, (pC)
            haddpd rC30, rC20  /* rC20 = {c30ab,c2ab} */
            #ifdef VOP
               VOP 16(pC), rC20
            #endif
            movapd rC20, 16(pC)
   
            haddpd rC11, rC01
            #ifdef VOP
               VOP 32(pC), rC01
            #endif
            movapd rC01, 32(pC)
            haddpd rC31, rC21
            #ifdef VOP
               VOP 48(pC), rC21
            #endif
            movapd rC21, 48(pC)
            add $64, pC
         #endif
         add incBn, pB   /* KB*vlen*NU*sizeof */
      sub $1, nnu
      jnz MNLOOP
      mov nnu0, nnu
      mov pB0, pB
      add incAm, pA
   sub $1, nmu
   jnz MNLOOP

/* DONE: */
   movq    (%rsp), %rbp
   movq    8(%rsp), %rbx
   movq    16(%rsp), %r12
   movq    24(%rsp), %r13
   movq    32(%rsp), %r14
   add $FSIZE, %rsp
   ret
@ROUT ATL_kmmm8x1x256_L1pf.S
@extract -b @(topd)/cw.inc lang=c -define cwdate 2008 -define cwdate 2015
/*
 * This routine hastily adapted to the 3rd gen Opteron from the Core2 kernel,
 * ATL_dmm8x1x120.c.  Main difference is the prefetch strategy, which fetches
 * to the L1 on the Opt3rdGen, and the handling of loads, where MOVUPD
 * is preferred over MOVLPD/MOVHPD pair.  We also turn on unaligned mem 
 * MULPD/ADDPD.  This all makes the kernal about 8% faster, and allows us
 * to use a smaller block factor (for better application performance).
 * This kernel does not do well for complex.
 * Later adapted to access-major in 2015
 */
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#if !defined(MB)
   #define MB 0
#endif
#if !defined(NB)
   #define NB 0
#endif
#if !defined(KB)
   #define KB 0
#endif
#if KB == 0
   #error "KB must be compile time constant!"
#endif
/*
 *Register usage
 */
#define pA0     %rcx
#define NN0     %rbx
#define pB      %rdx
#define pB0     %rax
#define pC0     %rsi
#define pfB     %rbp
#define pfA     %rdi
#define MM      %r10
#define NN      %r11

#define rA0     %xmm0
#define rB0     %xmm1
#define rC00    %xmm2
#define rC01    %xmm3
#define rC02    %xmm4
#define rC03    %xmm5
#define rC04    %xmm6
#define rC05    %xmm7
#define rC06    %xmm8
#define rC07    %xmm9
#define rC0     %xmm10
#define rC2     %xmm11
#define rC4     %xmm12
#define rC6     %xmm13

#if defined(SREAL) || defined(SCPLX)
   #define movapd movaps
   #define addpd addps
   #define subpd subps
   #define mulpd mulps
   #define SZ 4
   #define SZSH 2
   #define IMUL 2
#else
   #define SZ 8
   #define SZSH 3
   #define IMUL 1
#endif
#ifdef BETAN1
   #define VOP subpd
#elif defined(BETA1)
   #define VOP addpd
#elif defined(VOP)
   #undef VOP
#endif
/*
 * Prefetch defines
 */
#ifdef ATL_3DNow
   #if defined(ATL_UAMMID) && ATL_UAMMID == 0
      #define pref2(mem) prefetchw mem
   #else
      #define pref2(mem) prefetch mem
   #endif
   #define prefB(mem) prefetch  mem
   #define prefC(mem) prefetchw mem
#else
   #define pref2(mem) prefetcht0 mem
   #define prefB(mem) prefetcht0 mem
   #define prefC(mem) prefetcht0 mem
#endif
#if KB == 8
   #define KBSH 3
#elif KB == 16
   #define KBSH 4
#elif KB == 32
   #define KBSH 5
#elif KB == 64
   #define KBSH 6
#elif KB == 128
   #define KBSH 7
#elif defined(KBSH)
   #undef KBSH
#endif

#define PFAINC -8*SZ
#define PFBINC -8*SZ

/*
                    rdi      rsi    rdx        rcx         r8        r9
void ATL_USERMM(SZT nmu, SZT nnu, SZT K, CTYPE *pA, CTYPE *pB, TYPE *pC,
                  8(%rsp)    16(%rsp)     24(%rsp)
                CTYPE *pAn, CTYPE *pBn, CTYPE *pCn);
 */
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN64
ATL_asmdecor(ATL_USERMM):
/*
 *  Save callee-saved iregs
 */
   #define FSIZE 16
   sub $FSIZE, %rsp
   movq %rbp, (%rsp)
   movq %rbx, 8(%rsp)
/*
 * Setup input parameters
 */
   mov  %rdi, MM
   mov  %rsi, NN
   mov  %r8, pB0
   mov  %r9, pC0
   mov MM, pfB
   #ifdef KBSH
      shl $KBSH+3+SZSH, pfB
   #else
      imul $KB*8*SZ, pfB
   #endif
   mov NN, pfA
   #ifdef KBSH
      shl $KBSH+SZSH, pfA
   #else
      imul $KB*SZ, pfA
   #endif
   mov FSIZE+16(%rsp), pB
   mov FSIZE+24(%rsp), NN0
   cmp pB, pB0
   cmovE NN0, pB
   add pB, pfA

//   addq FSIZE+16(%rsp), pfA
   addq FSIZE+8(%rsp), pfB
/*
 * pA0 += 128, pB0 += 128
 */
   sub     $-128, pA0
   sub     $-128, pB0
   mov pB0, pB
   mov NN, NN0
   .local MNLOOP
   ALIGN32
   MNLOOP:
            /*KLOOP: */
            movapd -128(pB0), rB0
                                        pref2((pfA))
                                        add     $PFAINC, pfA
            movapd -128(pA0), rC00
            mulpd rB0,rC00
            movapd -112(pA0), rC01
            mulpd rB0,rC01
            movapd -96(pA0), rC02
            mulpd rB0,rC02
            movapd -80(pA0), rC03
            mulpd rB0,rC03
            movapd -64(pA0), rC04
            mulpd rB0,rC04
            movapd -48(pA0), rC05
            mulpd rB0,rC05
            movapd -32(pA0), rC06
            mulpd rB0,rC06
            movapd -16(pA0), rC07
            mulpd rB0,rC07

@iexp ia -16 16 +
@define i @2@
@define j @-112@
@iwhile i < 256
            #if KB > @(i)*IMUL
               movapd @(j)(pB0), rB0 
               movapd @(ia)(pA0), rA0 
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC00
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC01
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC02
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC03
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC04
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC05
               movapd @(ia)(pA0), rA0
      @iexp ia @(ia) 16 +
               mulpd rB0,rA0
               addpd rA0,rC06
               mulpd @(ia)(pA0), rB0
      @iexp ia @(ia) 16 +
               addpd rB0,rC07
            #endif
   @iexp i 2 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j
            /* KLOOP end */
         /* rC00, rC01, rC02 .. rC07 */
         #if defined(SREAL) || defined(SCPLX)
            haddps rC01, rC00    /* rC00 = {c1cd,c1ab,c0cd,c0ab} */
            haddps rC03, rC02    /* rC20 = {c3cd,c3ab,c2cd,c2ab} */
            haddps rC02, rC00    /* rC00 = {c3, c2, c1, c0} */
            #ifdef VOP
               VOP (pC0), rC00
            #endif
            movaps rC00, (pC0)

            haddps rC05, rC04    /* rC04 = {c5cd,c5ab,c4cd,c4ab} */
            haddps rC07, rC06    /* rC26 = {c7cd,c7ab,c6cd,c6ab} */
            haddps rC06, rC04    /* rC04 = {c7, c6, c1, c0} */
            #ifdef VOP
               VOP 16(pC0), rC04
            #endif
            movaps rC04, 16(pC0)
         #elif !defined(ATL_SSE3) && defined(DREAL)
            movapd rC00, rC0     /* rC0  = {c0b, c0a} */
            movapd rC02, rC2     /* rC2  = {c2b, c2a} */
            unpcklpd rC01, rC00  /* rC00 = {c1a, c0a} */
            unpcklpd rC03, rC02  /* rC02 = {c3a, c2a} */
            unpckhpd rC01, rC0   /* rC0  = {c1b, c0b} */
            unpckhpd rC03, rC2   /* rC2  = {c3b, c2b} */
            addpd    rC0, rC00   /* rC00 = {c1ab, c0ab} */
            addpd    rC2, rC02   /* rC02 = {c3ab, c2ab} */

            movapd rC04, rC4     /* rC4  = {c4b, c4a} */
            movapd rC06, rC6     /* rC6  = {c6b, c6a} */
            unpcklpd rC05, rC04  /* rC04 = {c5a, c4a} */
            unpcklpd rC07, rC06  /* rC06 = {c7a, c6a} */
            unpckhpd rC05, rC4   /* rC4  = {c5b, c4b} */
            unpckhpd rC07, rC6   /* rC6  = {c7b, c6b} */
            addpd    rC4, rC04   /* rC04 = {c5ab, c4ab} */
            addpd    rC6, rC06   /* rC06 = {c7ab, c6ab} */
            #ifdef VOP
               VOP (pC0), rC00
               VOP 16(pC0), rC02
               VOP 32(pC0), rC04
               VOP 48(pC0), rC06
            #endif
            movapd rC00, (pC0)
            movapd rC02, 16(pC0)
            movapd rC04, 32(pC0)
            movapd rC06, 48(pC0)
         #else
/*
 *          pC[0-8] = rC[0-8]
 */
            haddpd  rC01,rC00
                  prefB((pfB))
                  add     $PFBINC, pfB
            #ifdef VOP
               VOP (pC0), rC00
            #endif
            movapd rC00, (pC0)
   
            haddpd  rC03,rC02
            #ifdef VOP
               VOP 16(pC0), rC02
            #endif
            movapd  rC02, 16(pC0)
   
            haddpd  rC05,rC04
            #ifdef VOP
               VOP 32(pC0), rC04
            #endif
            movapd  rC04, 32(pC0)
   
            haddpd  rC07,rC06
            #ifdef VOP
               VOP 48(pC0), rC06
            #endif
            movapd  rC06, 48(pC0)
         #endif

         add $KB*SZ, pB0
         add $8*SZ, pC0
         dec NN
      jnz MNLOOP
      add $8*KB*SZ, pA0
      mov pB, pB0
      movq NN0, NN
      dec MM
   jnz     MNLOOP

/* DONE: */
   movq (%rsp), %rbp
   movq 8(%rsp), %rbx
   add $FSIZE, %rsp
   ret
